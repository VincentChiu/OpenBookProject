CHAPTER IV -- PARSERS AND STATE MACHINES
第四章 -- 解析器和状态机
-------------------------------------------------------------------

  All the techniques presented in the prior chapters of this book
  have something in common, but something that is easy to
  overlook.  In a sense, every basic string and regular
  expression operation treats strings as -homogeneous-.  Put another
  way: String and regex techniques operate on -flat- texts.
  While said techniques are largely in keeping with the "Zen of
  Python" maxim that "Flat is better than nested," sometimes the
  maxim (and homogeneous operations) cannot solve a problem.
  Sometimes the data in a text has a deeper -structure- than the
  linear sequence of bytes that make up strings.
  本书前面章节中所述技术都有一些共性, 却又容易被忽略。
  在某种意义上，基本字符串和正则表达式操作对待字符串的方式
  是-同种性质-。换句话说就是，字符串和正则表达式技术都是对
  -平面-文本进行操作。尽管"Python之箴"中有格言"平面胜过嵌套"，
  但是该格言(和同质操作)并不能解决问题。有时候文本中的的数据
  拥有比组成字符串的线性比特序列更深的-结构-。

  It is not entirely true that the prior chapters have eschewed
  data structures.  From time to time, the examples presented
  broke flat texts into lists of lines, or of fields, or of
  segments matched by patterns.  But the structures used have
  been quite simple and quite regular.  Perhaps a text was
  treated as a list of substrings, with each substring
  manipulated in some manner--or maybe even a list of lists of
  such substrings, or a list of tuples of data fields.  But
  overall, the data structures have had limited (and mostly
  fixed) nesting depth and have consisted of sequences of items
  that are themselves treated similarly.  What this chapter
  introduces is the notion of thinking about texts as -trees- of
  nodes, or even still more generally as graphs.
  要说前面的章节完全避开数据结构也是不完全正确的。经常由例子
  把平面文本打散后组成列表，其中组成可以是行，或者是域，
  抑或匹配某个模式的片段。但是用过的结构都是很简单而且常见的。
  也许一个文本会被作为一个子字符串列表处理，其中每个子字符串
  被进行某种处理--或者甚至作为一个子字符串列表的列表处理，再或者
  一个数据域元组的列表。但是总而言之，这些数据结构只有有限的
  (绝大部分时间也是固定的)嵌套深度，他们的组成序列成员都可以
  进行相似的自我处理。本章所要介绍的观点是将文本作为-树-的节点
  处理，甚至更通用一点，作为图处理。

  Before jumping too far into the world of nonflat texts, I
  should repeat a warning this book has issued from time to time.
  If you do not -need- to use the techniques in this chapter, you
  are better off sticking with the simpler and more maintainable
  techniques discussed in the prior chapters.  Solving too
  general a problem too soon is a pitfall for application
  development--it is almost always better to do less than to do
  more.  Full-scale parsers and state machines fall to the "more"
  side of such a choice.  As we have seen already, the class of
  problems you can solve using regular expressions--or even only
  string operations--is quite broad.
  在跳进非平面文本的世界之前，我还要重复一个本书时刻提起的警告，
  那就是如果你没有-必要-使用本章节的技术，你最好还是坚持前面
  章节所涉及的那些简单而又易维护的技术。对于程序开发而言，
  过快解决一个很通用的问题是个缺陷--基本上少做事比多做事好。
  全尺寸的解析器和状态机倾向于"多"的那一面。正如我们所见，
  你使用正则表达式--甚至仅仅字符串操作--所能解决的问题也是相当
  多的。

  There is another warning that can be mentioned at this point.
  This book does not attempt to explain parsing theory or the
  design of parseable languages. There are a lot of intricacies to
  these matters, about which a reader can consult a specialized
  text like the so-called "Dragon Book"--Aho, Sethi, and Ullman's
  _Compilers: Principle, Techniques and Tools_--or Levine, Mason,
  and Brown's _Lex & Yacc_. When Extended Backus-Naur Form (EBNF)
  grammars or other parsing descriptions are discussed below, it is
  in a general fashion that does not delve into algorithmic
  resolution of ambiguities or big-O efficiencies (at least not in
  much detail). In practice, everyday Python programmers who are
  processing texts--but who are not designing new programming
  languages--need not worry about those parsing subtleties omitted
  from this book.
  这儿还要给出另外一个警告，就是本书并不试图解释解析理论，或者
  如何设计一门可解析语言。关于这些错综复杂的内容，读者可以参考那些
  专门的书籍，例如所谓的"龙书"--Aho, Sethi, 和 Ullman的
  _编译器：原理，技术和工具_，或者Levine, Mason，和Brown的
  _Lex和Yacc_。当下面讨论到扩展Backus-Naur式(EBNF)语法或者
  其他解析描述时，只是一种通用方式，而非考究语义不清的算法解决，
  或者大O效率(至少没有太多细节)。从实用角度看，如果Python的编程者
  需要处理文本，但是并非设计新的编程语言，那就不需要担心本书
  忽略的那些微妙之处。
  
SECTION 1 -- An Introduction to Parsers
第1节 -- 解析器介绍
------------------------------------------------------------------------

  TOPIC -- When Data Becomes Deep and Texts Become Stateful
  主题 -- 当数据变复杂，文本拥有状态时
  ### 此处需要三思
  --------------------------------------------------------------------

  Regular expressions can match quite complicated patterns, but
  they fall short when it comes to matching arbitrarily nested
  subpatterns.  Such nested subpatterns occur quite often in
  programming languages and textual markup languages (and other
  places sometimes).  For example, in HTML documents, you can
  find lists or tables nested inside each other.  For that
  matter, character-level markup is also allowed to nest
  arbitrarily--the following defines a valid HTML fragment:
  正则表达式可以匹配相当复杂的模式，但是遇到任意嵌套的子模式时，
  它们就无法达到要求了。此类嵌套子模式在编程语言和文本标示语言
  (有时其他地方)中较常见。例如，在HTML文档中，你可以发现列表
  或者表格彼此互相嵌套。有鉴于此，也允许字符级的标示以任意嵌套
  -- 下例定义了一个有效的HTML片段：

      >>> s = '''<p>Plain text, <i>italicized phrase,
                 <i>italicized subphrase</i>, <b>bold
                 subphrase</b></i>, <i>other italic
                 phrase</i></p>'''

  The problem with this fragment is that most any regular
  expression will match either less or more than a desired '<i>'
  element body.  For example:
  该片段的问题是绝大部分正则表达式匹配到的内容将会比想要的
  '<i>'元素体或多或少。请看例子：

      >>> ital = r'''(?sx)<i>.+</i>'''
      >>> for phrs in re.findall(ital, s):
      ...     print phrs, '\n-----'
      ...
      <i>italicized phrase,
             <i>italicized subphrase</i>, <b>bold
             subphrase</b></i>, <i>other italic
             phrase</i>
      -----
      >>> ital2 = r'''(?sx)<i>.+?</i>'''
      >>> for phrs in re.findall(ital2, s):
      ...     print phrs, '\n-----'
      ...
      <i>italicized phrase,
             <i>italicized subphrase</i>
      -----
      <i>other italic
             phrase</i>
      -----

  What is missing in the proposed regular expressions is a
  concept of -state-.  If you imagine reading through a string
  character-by-character (which a regular expression match must
  do within the underlying regex engine), it would be useful to
  keep track of "How many layers of italics tags am I in?" With
  such a count of nesting depth, it would be possible to figure
  out which opening tag '<i>' a given closing tag '</i>' was
  meant to match.  But regular expressions are not stateful in
  the right way to do this.
  在提出的正则表达式中缺少的是一个-状态-的概念。想象一下，
  如果你一个字符接一个字符地读一个字符串(这正是正则表达式匹配
  在低沉的regex引擎中必须做的)， 能够明了“我现在在斜体标签的
  第几层”将非常有用。拥有这样一个嵌套深度的计数器，才可能
  找出给定的结束标志'</i>'所对应的开始标示'<i>'。但是正则表达式
  无法给出正确的状态。

  You encounter a similar nesting in most programming languages.
  For example, suppose we have a hypothetical (somewhat
  BASIC-like) language with an IF/THEN/END structure.  To
  simplify, suppose that every condition is spelled to match the
  regex 'cond\d+', and every action matches 'act\d+'.  But the
  wrinkle is that IF/THEN/END structures can nest within each
  other also.  So for example, let us define the following three
  top-level structures:
  你在大部分编程语言中都可以遇到类似的嵌套。例如，我们有一个
  假设的语言(在某些程度上与BASIC类似)，它有一个IF/THEN/END
  结构。为了简单起见，每个条件(condition)的命名都能匹配正则表达式
  'cond\d+'，同时每个行动(action)都能匹配'act\d+'。但是关于
   IF/THEN/END结构的一个绝妙之处在于它们可以互相嵌套。作为
   一个例子，我们下面定义三个顶层结构：

      >>> s = '''
      IF cond1 THEN act1 END
      -----
      IF cond2 THEN
        IF cond3 THEN act3 END
      END
      -----
      IF cond4 THEN
        act4
      END
      '''

  As with the markup example, you might first try to identify the
  three structures using a regular expression like:
  对于这些标示例子而言，你一开始可能试图使用以下正则表达式
  来识别这上个结构：

      >>> pat = r'''(?sx)
      IF \s+
      cond\d+ \s+
      THEN \s+
      act\d+ \s+
      END'''
      >>> for stmt in re.findall(pat, s):
      ...     print stmt, '\n-----'
      ...
      IF cond1 THEN act1 END
      -----
      IF cond3 THEN act3 END
      -----
      IF cond4 THEN
        act4
      END
      -----

  This indeed finds three structures, but the wrong three.  The
  second top-level structure should be the compound statement
  that used 'cond2', not its child using 'cond3'.  It is not too
  difficult to allow a nested IF/THEN/END structure to optionally
  substitute for a simple action; for example:
  这个确实找到三个结构，但是是错误的。第二个顶层结构应该是使用
  'cond2'的混合声明，而非使用'cond3'的子声明。允许可选地以
  一个嵌套的IF/THEN/END结构来替换单个行动并非难事，请看例子：

      >>> pat2 = '''(?sx)(
      IF \s+
      cond\d+ \s+
      THEN \s+
      (  (IF \s+ cond\d+ \s+ THEN \s+ act\d+ \s+ END)
       | (act\d+)
      ) \s+
      END
      )'''
      >>> for stmt in re.findall(pat2, s):
      ...     print stmt[0], '\n-----'
      ...
      IF cond1 THEN act1 END
      -----
      IF cond2 THEN
        IF cond3 THEN act3 END
      END
      -----
      IF cond4 THEN
        act4
      END
      -----

  By manually nesting a "first order" IF/THEN/END structure as an
  alternative to a simple action, we can indeed match the
  example in the desired fashion.  But we have assumed that
  nesting of IF/THEN/END structures goes only one level deep.
  What if a "second order" structure is nested inside a "third
  order" structure--and so on, ad infinitum?  What we would
  like is a means of describing arbitrarily nested structures in
  a text, in a manner similar to, but more general than, what
  regular expressions can describe.
  通过手工嵌套一个“一级”的 IF/THEN/END结构以替换单个行动，
  我们确实可以匹配到目标中我们想要的内容。但是我们的假设是
   IF/THEN/END结构的嵌套是仅仅一层。如果一个“二级”的结构
   被嵌入在一个“三级”的结构里，怎么办？以此类推，如果无穷多
   级呢？我们需要的是一个方法，它可以描述文本中的任意嵌套结构，
   使用的方法尽管和正则表达式类似，但是比后者更加通用。

  TOPIC -- What is a Grammar?
  主题 -- 什么是语法？
  --------------------------------------------------------------------
  
  In order to parse nested structures in a text, you usually use
  something called a "grammar." A grammar is a specification of a
  set of "nodes" (also called "productions") arranged into a
  strictly hierarchical "tree" data structure.  A node can have
  a name--and perhaps some other properties--and it can also have
  an ordered collection of child nodes.  When a document is
  parsed under a grammar, no resultant node can ever be a
  descendent of itself; this is another way of saying that a
  grammar produces a tree rather than a graph.
  为了能解析文本中的嵌套结构，你通常需要一个叫"语法"的东西。
  语法就是对一些“节点”(也叫“产品”)集合的规范。一个节点
  可以有名字，也许还有其他的属性，也可以拥有一些排过序的子节点。
  当使用语法来解析文档时，节点不可以是自继承的，换种说法是，
  语法生成树，而非图。

  In many actual implementations, such as the famous C-based
  tools 'lex' and 'yacc', a grammar is expressed at two layers.
  At the first layer, a "lexer" (or "tokenizer") produces a
  stream of "tokens" for a "parser" to operate on.  Such tokens
  are frequently what you might think of as words or fields, but
  in principle they can split the text differently than does our
  normal idea of a "word." In any case tokens are nonoverlapping
  subsequences of the original text.  Depending on the specific
  tool and specification used, some subsequences may be dropped
  from the token stream.  A "zero-case" lexer is one that simply
  treats the actual input bytes as the tokens a parser operates
  on (some modules discussed do this, without losing generality).
  在许多的实际实现中，例如著名的基于C的工具'lex'和'yacc'，语法
  是以两层形式出现的。在第一层中，一个"lexer"(又称"特征标识器", tokenizer)
  产生“特征”流，以供“解析器”操作。此类特征通常是你所认为的
  单词或者域，但是理论上它们可以以不同的方法来切割文本，从而
  生成与通常“单词”不同的结果。在任何情况下，特征都是原始文本中
  非重叠的子序列。根据指定的工具和规格，一些子序列可能会被从
  特征流中抛弃。 一个“零事例”lexer的工作只要把实际的输入比特
  当作特征以供解析器操作。（一些模块不失普适性的讨论了这个问题）。
### token -> 特征 ？ 令牌？ 等等？

  The second layer of a grammar is the actual parser. A parser
  reads a stream or sequence of tokens and generates a "parse tree"
  out of it. Or rather, a tree is generated under the assumption
  that the underlying input text is "well-formed" according to the
  grammar--that is, there is a way to consume the tokens within the
  grammar specification. With most parser tools, a grammar is
  specified using a variant on EBNF.
  语法的第二层才是真正的解析器。解析器会读入一个特征流或者特征序列，
  然后从中生成一个“解析树”。或者说，假设根据语法，输入文本是
  格式良好的，一颗树即被生成。所以，可在语法规范内将特征处理掉。
  对于绝大部分解析工具而言，语法使用EBNF的某些变形来定义的。

  An EBNF grammar consists of a set of rule declarations, where
  each rule allows similar quantification and alternation as that
  in regular expressions. Different tools use slightly different
  syntax for specifying grammars, and different tools also differ
  in expressivity and available quantifiers. But almost all tools
  have a fairly similar feel in their grammar specifications. Even
  the DTDs used in XML dialect specifications (see Chapter 5) have
  a very similar syntax to other grammar languages--which makes
  sense since an XML dialect is a particular grammar. A DTD entry
  looks like:
  EBNF语法包含了一套规则来声明，其中每个规则都允许与正则表达式
  中类似的量词和轮换。对于特定的语法，不同的工具使用的句法略有
  不同，另外表达方式以及可用的量词也有不同。但是几乎所有的工具
  在它们的语法规范中都有相当熟悉的感觉。甚至XML方言规范
  （见第五章）中使用的DTD和其他语法语言都有极其相似的句法
  --这个并不奇怪，因为XML方言是一种特殊的语法。
  DTD条目看上去如下：

      #*----- EBNF-style description in a developerWorks DTD -----#
      <!ELEMENT body  ((example-column | image-column)?, text-column) >

  In brief, under the sample DTD, a '<body>' element may contain
  either one or zero occurrences of a "first thing"--that first
  thing being -either- an '<example-column>' or an
  '<image-column>'.  Following the optional first component,
  exactly one '<text-column>' must occur.  Of course, we would
  need to see the rest of the DTD to see what can go in a
  '<text-column>', or to see what other element(s) a '<body>'
  might be contained in.  But each such rule is similar in form.
  简单说，在该DTD例子中，'<body>'元素可以包含一个或者零个
  '第一件事'，此处第一件事可以是'<example-column>'或者
  '<image-column>'。在可选的第一个成分之后，必须紧跟着一个
  '<text-column>'。当然，我们需要查看该DTD其余的内容才知道
  '<text-column>'中可以拥有的内容，或者'<body>'可以包含的
  其它内容。但是每条规则在形式上都是类似的。

  A familiar EBNF grammar to Python programmers is the grammar
  for Python itself.  On many Python installations, this grammar
  as a single file can be found at a disk location like
  '[...]/Python22/Doc/ref/grammar.txt'.  The online and
  downloadable _Python Language Reference_ excerpts from the
  grammar at various points.  As an example, a floating point
  number in Python is identified by the specification:
  对于Python程序员而言，Python本身的语法就是一个熟悉的EBNF语法。
  对于许多Python安装来说，该语法呈现为单个文件，可在磁盘上
  某处例如'[...]/Python22/Doc/ref/grammar.txt'找着。
  _Python Language Reference_ 是在线文档，亦可下载，摘录了
  该语法的各个方面。例如，Python中的浮点数可以下面的规范来识别：

      #---- EBNF-style description of Python floating point ---#
      #---- 对Python浮点数的EBNF风格描述 ---#
      floatnumber   ::= pointfloat | exponentfloat
      pointfloat    ::= [intpart] fraction | intpart "."
      exponentfloat ::= (intpart | pointfloat) exponent
      intpart       ::= digit+
      fraction      ::= "." digit+
      exponent      ::= ("e" | "E") ["+" | "-"] digit+
      digit         ::= "0"..."9"

  The Python grammar is given in an EBNF variant that allows
  considerable expressivity.  Most of the tools this chapter
  discusses are comparatively limited (but are still ultimately
  capable of expressing just as general grammars, albeit more
  verbosely).  Both literal strings and character ranges may be
  specified as part of a production.  Alternation is expressed
  with "|".  Quantifications with both "+" and "*" are used.
  These features are very similar to those in regular expression
  syntax.  Additionally, optional groups are indicated with
  square brackets ("'['" and "']'"), and mandatory groups with
  parentheses.  Conceptually the former is the same as the regex
  "?" quantifier.
  Python语法以EBNF变量形式清晰地给出。本章中讨论的绝大部分
  工具相对而言都是功能有限的（但仍然胜任表达通用语法的任务，
  就是更加冗长）。作为该产品的一部分，文字串和字符范围都可能
  被制定。轮换是用“|”来表示的，而量词同时使用了“+”和“*”。
  这些特性和正则表达式句法中的非常相似。另外，可选组合以
  方括号“[”和“]”标示。强制组合则使用括号。前者在概念上
  和正则表达式中的“?”量词一致。

  Where an EBNF grammar goes beyond a regular expression pattern
  is in its use of named terms as parts of patterns.  At first
  glance, it might appear possible simply to substitute regular
  expression patterns for named subexpressions.  In fact, in the
  floating point pattern presented, we could simply do this as:
  EBNF语法超出正则表达式模式的地方在于，它使用命名的术语作为部分
  的模式。<+此处不通+> 乍一看，使用命名的子表达式来替换
  正则表达式模式是可能的。实际上，在给出的浮点模式中，我们
  只要如此这般即可：

      #---- Regular expression to identify a floating point ----#
      #---- 识别浮点数的正则表达式 ----#
      pat = r'''(?x)
            (                   # exponentfloat
              (                 # intpart or pointfloat
                (               # pointfloat
                  (\d+)?[.]\d+  # optional intpart with fraction
                  |
                  \d+[.]        # intpart with period
                )               # end pointfloat
                |
                \d+             # intpart
              )                 # end intpart or pointfloat
              [eE][+-]?\d+      # exponent
            )                   # end exponentfloat
            |
            (                   # pointfloat
              (\d+)?[.]\d+      # optional intpart with fraction
              |
              \d+[.]            # intpart with period
            )                   # end pointfloat
            '''

  As a regular expression, the description is harder to read, even
  with the documentation added to a verbose regex. The EBNF grammar
  is more or less self-documenting. Moreover, some care had to be
  taken about the order of the regular expression--the
  'exponentfloat' alternative is required to be listed before the
  'pointfloat' alternative since the latter can form a subsequence
  of the latter. But aside from the need for a little tweaking and
  documentation, the regular expression above is exactly as
  general--and exactly equivalent--to the Python grammar for a
  floating point number.
  作为正则表达式，尽管已经以冗余模式给出文档，以上描述更难理解。
  EBNF语法是或多或少地可以进行自行文档说明。而且，正则表达式的
  顺序也需要花功夫--可选择的'exponentfloat'必须在可选择的'pointfloat'
  之前列出，因为前者可以形成后者的子序列。 除了需要小小的优化以及
  文档说明，对于浮点数而言， 以上的正则表达式和Python语法是
  完全通用而且等价的。
## xxx 原文是否错误？ latter ... latter

  You might wonder, therefore, what the point of a grammar is.
  It turns out that a floating point number is an unusually
  simple structure in one very specific respect.  A 'floatnumber'
  requires no recursion or self-reference in its definition.
  Everything that makes up a 'floatnumber' is something simpler,
  and everything that makes up one of those simpler components is
  itself made up of still simpler ones.  You reach a bottom in
  defining a Python floating point number.
  因此，你也许疑惑为什么要使用语法。事实上，浮点数只是一个
  非常特殊方面的一个不寻常的简单结构。'floatnumber'不允许它的
  定义中有迭代或者自引用。  构建'floatnumber'的每个成分都
  比它更简单，而这些成分本身是由更简单的成分构建而成。
  如此，你就到达定义Python浮点数的一个底线。

  In the general case, structures can recursively contain
  themselves, either directly or by containing other structures
  that in turn contain the first structures.  It is not even
  entirely absurd to imagine floating point numbers with such a
  a grammar (whatever language had them would not be Python,
  however).  For example, the famous number a "googol" was
  defined in 1938 by Edward Kasner as 10 to the 100th power
  (otherwise called "10 dotrigintillion").  As a Python floating
  point, you could write this as '1e100'.  Kasner also defined a
  "googolplex" as 10 to the googol power (a number much larger
  than anyone needs for any practical reason).  While you can
  create a Python expression to name a googolplex--for example,
  '10**1e100'--it is not difficult to conceive a programming
  language that allowed the term '1e1e100' as a name for a
  googolplex.  By the way: If you try to actually -compute- a
  googolplex in Python (or any other programming language), you
  will be in for disappointment; expect a frozen computer and/or
  some sort of crash or overflow.  The numbers you can express in
  most language grammars are quite a bit more numerous than those
  your computer can actually do anything with.
  在通常情况下，结构可以迭代包含自己，或者直接进行，或者通过包含
  其他包含自己的结构。设想浮点数拥有如此语法并非纯属荒谬（当然
  拥有该语法的语言并非Python）。例如，Edward Kasner在1938年
  定义了著名的数字“googol”为10 的100次方
  （又称“10 dotrigintillion”）。作为Python的浮点数，你可以将其
  写作“1e100”。Kasner还定义了“googolplex”，其等于10的
  googol次方。这个数比任何人实际应用中所遇到的数都要大。
  你可以创建一个Python表达式来命名一个googolplex，例如，
  “10**1e100”，不过构思一种能将“1e1e100”作为googolplex的
  名字的编程语言也不是难事。顺便提一下，如果你尝试在Python中
  （或者其他编程语言中）实际 -计算-一下googolplex，你可能会感到
  失望，你的电脑会失去相应，或者/以及某种形式的崩溃或溢出。
  在绝大部分编程语法中可以表达的数字要比电脑实际能处理的数字
  多得多。

  Suppose that you wanted to allow these new "extended" floating
  point terms in a language.  In terms of the grammar, you could
  just change a line of the EBNF description:
  假设你想要在某语言中允许新的“扩展”浮点数术语。按照该语法，
  你只需要EBNF描述中的一行即可：

      #*----- EBNF description of "extended floating point -----#
      exponent ::= ("e" | "E") ["+" | "-"] floatnumber

  In the regular expression, the change is a problem.  A
  portion of the regular expression identifies the (optional)
  exponent:
  在正则表达式中，这样的改变是个问题。正则表达式的一部分可以识别
  （可选的）指数：

      #*-------- Regular expression for option exponent --------#
      [eE][+-]?\d+      # exponent

  In this case, an exponent is just a series of digit characters.
  But for "extended" floating point terms, the regular expression
  would need to substitute the entire 'pat' regular expression in
  place of '\d+'.  Unfortunately, this is impossible, since each
  replacement would still contain the insufficient '\d+'
  description, which would again require substitution.  The
  sequence of substitutions continues ad infinitum, until the
  regular expression is infinitely long.
  在此情况中，指数只是一连串的数字。但是对于“扩展”的浮点数
  术语而言，正则表达式将需要替换整个“pat”正则表达式
  中的“\d+”。不幸的是，这是不可能的。因为每个替换仍然包含
  不充分的“\d+”描述，而它们又需要替换。替换将会无穷尽地
  执行先去，直到正则表达式无限长。


  TOPIC -- An EBNF Grammar for IF/THEN/END Structures
  主题 -- 对于IF/THEN/END结构的EBNF语法
  --------------------------------------------------------------------

  The IF/THEN/END language structure presented above is a more
  typical and realistic example of nestable grammatical structures
  than are our "extended" floating point numbers. In fact,
  Python--along with almost every other programming
  language--allows precisely such 'if' statements inside other 'if'
  statements. It is worthwhile to look at how we might describe our
  hypothetical simplified IF/THEN/END structure in the same EBNF
  variant used for Python's grammar.
  上述IF/THEN/END语言结构是可嵌套的语法结构一个更典型和现实的例子，
  如果和我们“扩展”浮点数相比较的话。事实上，Python--和其他几乎
  所有编程语言一样--允许“if”声明中包含另外的“if”声明。你会发现
  花一些时间来查看如何使用Python语法的EBNF变量来描述
  我们这个假设的简化过的IF/THEN/END结构是值得的。

  Recall first our simplified rules for allowable structures:
  The keywords are 'IF', 'THEN', and 'END', and they always occur
  in that order within a completed structure.  Keywords in this
  language are always in all capitals.  Any whitespace in a
  source text is insignificant, except that each term is
  separated from others by at least some whitespace.  Every
  condition is spelled to match the regular expression 'cond\d+'.
  Every IF "body" either contains an action that matches the
  regular expression 'act\d+', -or- it contains another
  IF/THEN/END structure.  In our example, we created three
  IF/THEN/END structures, one of which contained a nested
  structure:
  首先来回忆一下简化规则中允许的结构：
  关键词是“IF”，“THEN”和“END”，在一个完整的结构中，
  它们总是以上述顺序发生。此语言中的关键词均为大写。在源文件
  中的空白均可忽略，除了术语之间由空白隔开。每个条件的拼写
  需要能被正则表达式“cond\d+”匹配到。
  每个IF“正文”或者包含一个动作，其名需可被“act\d+”匹配，
  -或者-包含另一个IF/THEN/END结构。在我们的例子中，
  我们创建了三个IF/THEN/END结构，其中一个包含了另一个嵌套结构：
##body -》 正文？
  

      #*-------- IF/THEN/END examples --------#
      #*-------- IF/THEN/END 结构 --------#
      IF cond1 THEN act1 END
      -----
      IF cond2 THEN
        IF cond3 THEN act3 END
      END
      -----
      IF cond4 THEN
        act4
      END

  Let us try a grammar:
  让我们尝试一个语法：

      #------- EBNF grammar for IF/THEN/END structures --------#
      #------- 适用于IF/THEN/END结构的语法 --------#
      if_expr   ::= "IF" ws cond ws "THEN" ws action ws "END"
      whitechar ::= " " | "\t" | "\n" | "\r" | "\f" | "\v"
      ws        ::= whitechar+
      digit     ::= "0"..."9"
      number    ::= digit+
      cond      ::= "cond" number
      action    ::= simpleact | if_expr
      simpleact ::= "act" number

  This grammar is fairly easy to follow.  It defines a few
  "convenience" productions like 'ws' and 'number' that consist
  of repetitions of simpler productions.  'whitechar' is defined
  as an explicit alternation of individual characters, as is
  'digit' for a continuous range.  Taken to the extreme, every
  production could actually be included in a much
  more verbose 'if_expr' production--you would just substitute
  all the right-hand sides of nested productions for the names in
  the 'if_expr' production.  But as given, the grammar is much
  easier to read.  The most notable aspect of this grammar is the
  'action' production, since an 'action' can itself recursively
  contain an 'if_expr'.
  此语法理解起来相当容易。它先定义了一些“便利”产品，例如
  “ws”和“number”，它们包含了对一些更简单产品的重复。
  “whitechar”被定义为某些单个字符的显式轮换，而“digit”是
  一个连续的范围。往极端里说，每个产品实际上都可被包含在更冗长
  的“if_expr”产品中--你只需要把所有嵌套产品的右边替换为
  “if_expr”产品的名字。正如已经说明，该语法更易读。而它最
  值得注意的地方是“action”产品，因为“action”本身可以
  迭代包含一个“if_expr”。
## production -》 产品？ 不是特别好
## 上一段翻译不是特别好

  For this problem, the reader is encouraged to develop grammars
  for some more robust variations on the very simple IF/THEN/END
  language we have looked at.  As is evident, it is difficult to
  actually do much with this language by itself, even if its
  actions and conditions are given semantic meaning outside the
  structure.  Readers can invent their own variations, but a few
  are proposed below.
  对于本问题，我们鼓励读者为这些最简单的IF/THEN/END
  开发一些更健壮的语法。 很明显，就算它的行动和条件
  都给定结构外的语义信息，仅靠该语言本身并不能做多少
  实际的事情。


  TOPIC -- Pencil-and-Paper Parsing
  主题 -- 使用纸和笔来解析
  --------------------------------------------------------------------

  To test a grammar at this point, just try to expand each
  successive character into some production that is allowed at
  that point in the parent production, using pencil and paper.
  Think of the text of test cases as a tape:  Each symbol either
  completes a production (if so, write the satisfied production
  down next to the subsequence), or the symbol is added to the
  "unsatisfied register." There is one more rule to follow with
  pencil and paper, however: It is better to satisfy a production
  with a longer subsequence than a shorter one.  If a parent
  production consists of child productions, the children must be
  satisfied in the specified order (and in the quantity
  required).  For now, assume only one character of lookahead in
  trying to follow this rule.  For example, suppose you find the
  following sequence in a test case:
  现在如果想要测试一个语法，只需使用纸笔来尝试将
  每个连续的字符展开为一些产品，但后者需要被父产品所允许。
  将测试例中的文本想象为一个磁带：每个符号要么完成一个产品
  （如此，则将完成产品写于子序列后），要么该符号被加到一个
  “未完成寄存器”。但是使用纸和笔还有一个规则需要遵守：
  使用长子序列来完成一个产品比用短子序列好。如果父产品
  包含若干子产品，则后者必须以制定顺序完成，并满足要求的数量。
  现在，为了遵守此规则，假设每次只向前看一个字符。例如，
  设想你在测试例中找到下列序列：

## satisfied -> 完成？ 满意的？

      #*-------- Pencil-and-Paper Parsing examples --------#
      #*-------- 纸笔解析例子 --------#
      "IF   cond1..."

  Your steps with the pencil would be something like this:
  你使用笔的步骤大概如下：

  1.  Read the "I"--no production is satisfied.
  1、读入“I”-- 没有产品完成。

  2.  Read the "F", unsatisfied becomes "I"'-'"F".  Note that
      "I"'-'"F" matches the literal term in 'if_expr' (a literal is
      considered a production).  Since the literal term contains
      no quantifiers or alternates, write down the "IF"
      production.  Unsatisfied becomes empty.
   2、读入“F”，未完成寄存器中变成“I”-“F”。请注意到
      “I”-“F”匹配“if_expr”中的字面术语（一个字面术语
      被认为一个产品）。因为字面术语不包含量词和轮换，写下
      “IF”产品。未完成寄存器被清空。

  3.  Read the space, Unsatisfied becomes simply a space.  Space
      satisfies the production 'ws', but hold off for a character
      since 'ws' contains a quantifier that allows a longer
      substring to satisfy it.
  3、读入空格。未完成寄存器中有一个空格。空格完成了产品“ws”，
      但是拖延到下一个字符，因为“ws”包含了一个量词，后者允许
      使用一个更长的子字符串来完成。

  4.  Read the second space, unsatisfied becomes space-space.
      Space-space satisfies the production 'ws'.  But again hold
      off for a character.
  4、读入第二个空格。未完成寄存器中变成“空格-空格”。
      “空格-空格”可以完成产品“ws”，但是它被再次拖延到下一
      字符。

  5.  Read the third space, unsatisfied becomes
      space-space-space. This again satisfies the production
      'ws'.  But keep holding off for the next character.
  5、读入第三个空格。未完成寄存器变成了“空格-空格-空格”。
      这个再次完成产品“ws”。但是继续拖延到下一字符。

  6.  Read the "c", unsatisfied becomes space-space-space'-'"c".
      This does not satisfy any production, so revert to the
      production in 5.  Unsatisfied becomes "c".
  6、读入“c”。未完成寄存器变成“空格-空格-空格-c”。
      这个不能完成任何产品，所以退回到5中的产品。未完成寄存器
      变成“c”。

  7.  ...etc...
  7、…等等…

  If you get to the last character, and everything fits into some
  production, the test case is valid under the grammar.
  Otherwise, the test case is nongrammatical.  Try a few
  IF/THEN/END structures that you think are and are not valid
  against the provided grammar.
  如果你到达最后一个字符，并且所有东西都装进相应产品，该
  测试例在此语法下是有效的。否则，该测试例为非语法性的。
  尝试一些新的IF/THEN/END结构，其中有的你认为有效，有的无效，
  来测试提供的语法。
## 此处不是很明白，原文错误？

  EXERCISE: Some variations on the language
  练习：语言的一些变种
  --------------------------------------------------------------------

  1.  Create and test an IF/THEN/END grammar that allows multiple
      actions to occur between the THEN and the END.  For
      example, the following structures are valid under this
      variation:
  1、创建并测试一个IF/THEN/END语法，允许THEN和END之间
      有多个动作。例如，在此变种下，下列结构是有效的。

      #*-------- IF/THEN/END examples --------#
      #*-------- IF/THEN/END 例子 --------#
      IF cond1 THEN act1 act2 act3 END
      -----
      IF cond2 THEN
        IF cond3 THEN act3 END
        IF cond4 THEN act4 END
      END
      -----
      IF cond5 THEN IF cond6 THEN act6 act7 END act8 END

  2.  Create and test an IF/THEN/END grammar that allows for
      arithmetic comparisons of numbers as conditions (as an
      enhancement of variation 1, if you wish).  Specifically, a
      comparison consists of two numbers with one of "'<'",
      "'>'", or "=" between them.  There might or might not be
      any whitespace between a comparison symbol and surrounding
      numbers.  Use your judgment about what a number consists
      of (the Python floating point grammar might provide an
      example, but yours could be simpler).
  2、创建并测试一个IF/THEN/END语法，允许以对数字的算术比较
      来作为条件（如果你希望的话，作为变种1 的增强版）。特别地，
      比较过程包含两个数字，中间有一个“<”、“>”或者“=”。
      在比较符和数字之间以及周围可以有空白。自行判断数字的组成
      （Python浮点语法可以作为一个例子，但是你的可以简单一些）。

  3.  Create and test an IF/THEN/END grammar that includes a loop
      expression as a valid action.  A loop consists of the
      keyword 'LOOP', followed by a positive integer, followed by
      action(s), and terminated by the 'END' keyword.  Loops
      should be considered actions, and therefore ifs and loops
      can be contained inside one another; for example:
  3、创建并测试一个IF/THEN/END语法，有效动作包含一个循环（loop）。
      循环包含关键词“LOOP”，后面跟随一个正整数，接着是一个或者
      几个动作，并以“END”关键词结束。循环被看作动作，因此如果
      结构和循环结构可以彼此包含；例如：

      #*-------- IF/THEN/END/LOOP example --------#
      #*-------- IF/THEN/END/LOOP例子 --------#
     IF cond1 THEN
        LOOP 100
          IF cond2 THEN
            act2
          END
        END
      END

      You can make this LOOP-enhanced grammar an enhancement of
      whichever variant you wish.
      你可以随意将此循环增强语法作为任一变种的增强。

  4.  Create and test an IF/THEN/END grammar that includes an
      optional 'ELSE' keyword.  If an 'ELSE' occurs, it is within
      an 'IF' body, but 'ELSE' might not occur.  An 'ELSE' has its
      own body that can contain action(s).  For example (assuming
      variant 1):
  4、创建并测试一个IF/THEN/END语法，其中“ELSE”关键词是可选的。
      如果存在“ELSE”，它必须在“IF”体内，但是它也可以不存在。
      “ELSE”拥有自己的文本块，其中包含了一个或多个动作。例如（假设变种1）：
## body -> 身体？文本块

      #*-------- IF/THEN/END example --------#
      #*-------- IF/THEN/END 例子 --------#
      IF cond1 THEN
        act1
        act2
      ELSE
        act3
        act4
      END

  5.  Create and test an IF/THEN/END grammar that may include
      -zero- actions inside an 'IF', 'ELSE', or 'LOOP' body.  For
      example, the following structures are valid under this
      variant:
  5、创建并测试一个IF/THEN/END语法，在“IF”、“ELSE”或“LOOP”
      体内可以-没有-任何动作。例如，在此变种下，下列结构是有效的。

      #*-------- IF/THEN/END examples --------#
      #*-------- IF/THEN/END 例子 --------#
      IF cond1 THEN
      ELSE act2
      END
      -*-
      IF cond1 THEN
        LOOP 100 END
      ELSE
      END


SECTION 2 -- An Introduction to State Machines
第二节 -- 关于状态机的介绍
------------------------------------------------------------------------

  State machines, in a theoretical sense, underlay almost
  everything computer- and programming-related.  But a Python
  programmer does not necessarily need to consider highly
  theoretical matters in writing programs.  Nonetheless, there
  is a large class of ordinary programming problems where the
  best and most natural approach is to explicitly code a state
  machine as the solution.  At heart, a state machine is just a
  way of thinking about the flow control in an application.
  状态机，从理论上讲，是几乎一切计算机-以及编程-相关事物的基础。
  尽管Python程序员编程时并不需要考虑过于理论化的东西，
  很多普通编程问题的最好最自然的解决方案就是显式地以状态机编码。
  从心里说，状态机只是一种关于程序中的流控制的思考。

  A parser is a specialized type of state machine that analyzes
  the components and meaning of structured texts.  Generally a
  parser is accompanied by its own high-level description
  language that describes the states and transitions used by the
  implied state machine.  The state machine is in turn applied to
  text obeying a "grammar."
  解析器只是状态机中的一种，它分析结构文本中的组件和意义。通常
  解析器是和它本身的高级描述语言一起的，后者描述了包含的状态机
  所使用的状态和转换。然后该状态机被应用到遵守某语法的文本上。

  In some text processing problems, the processing must be
  -stateful-: How we handle the next bit of text depends upon
  what we have done so far with the prior text.  In some cases,
  statefulness can be naturally expressed using a parser grammar,
  but in other cases the state has more to do with the semantics
  of the prior text than with its syntax.  That is, the issue of
  what grammatical properties a portion of a text has is
  generally orthogonal to the issue of what predicates it
  fulfills.  Concretely, we might calculate some arithmetic
  result on numeric fields, or we might look up a name
  encountered in a text file in a database, before deciding how
  to proceed with the text processing.  Where the parsing of a
  text depends on semantic features, a state machine is often a
  useful approach.
  在某些文本处理问题中，处理过程必须是-有状态的-：如何处理下一
  比特的文本依赖于到目前为止我们是如何处理以前的文本的。在
  某些案例中，状态可以很自然地由解析器语法表达出来，但在另外
  一些案例中，状态和以前文本的语意有关，胜过它的句法。也就是说，
  文本的某部分的语法属性与其陈述的内容是无关的。具体地说，我们
  可能在数字域里面计算一些算术问题，也可能在数据库中查找某文本
  文件中遇到的名字，然后才决定如何处理文本。在文本解析依赖于
  语意特性时，状态机通常是个有效的方法。
  
  Implementing an elementary and generic state machine in Python
  is simple to do, and may be used for a variety of purposes.
  The third-party C-extension module [mx.TextTools], which is
  discussed later in this chapter, can also be used to create far
  faster state machine text processors.
  在Python中实现一个基本而通用的状态机是很简单的，它的用途也颇广泛。
  本章后面会讨论的第三方的C扩展模块[mx.TextTools]，可用来创建
  相当快的状态机文本处理器。

  TOPIC -- Understanding State Machines
  主题 -- 理解状态机
  --------------------------------------------------------------------

  A much too accurate description of a state machine is that it
  is a directed graph, consisting of a set of nodes and a set of
  transition functions.  Such a machine "runs" by responding to a
  series of events; each event is in the domain of the transition
  function of the "current" node, where the range is a subset of
  the nodes.  The function return is a "next" (maybe
  self-identical) node. A subset of the nodes are end-states; if
  an end-state is reached, the machine stops.
  关于状态机的一个更准确的描述是，它是一个有向图，由一套节点以及
  转换功能组成。这样的机器运行时相应一系列的事件；每个事件位于
  转换功能的“当前”节点域内，后者的集合是节点的子集。节点的一个
  子集是结束状态（end-state）；如果达到了结束状态，机器停止运行。

  An abstract mathematical description--like the one above--is of
  little use for most practical programming problems. Equally
  picayune is the observation that every program in an imperative
  programming language like Python is a state machine whose nodes
  are its source lines (but not really in a declarative--functional
  or constraint-based--language such as Haskell, Scheme, or
  Prolog). Furthermore, every regular expression is logically
  equivalent to a state machine, and every parser implements an
  abstract state machine. Most programmers write lots of state
  machines without really thinking about it, but that fact provides
  little guidance to specific programming techniques.
  数学描述摘要--和上面那个一样--对于绝大部分实用编程问题而言，是
  几乎无用的。如同硬币的两面，命令式（imperative）编程语言
  （例如Python）源文件的行就是状态机的节点，而对于
  声明性（declarative）--函数式或者基于约束的--语言
  （例如Haskell，Scheme，或者Prolog），这个不一定成立。而且，
  每个正则表达式逻辑上都有个对应的状态机，每个解析器实现了
  一个抽象的状态机。绝大部分程序员并未意识到业已编写了许多状态机，
  但这个事实并未为此编程技术提供向导。

  An informal, heuristic definition is more useful than an
  abstract one.  Often we encounter a program requirement that
  includes a handful of distinct ways of treating clusters of
  events.  Furthermore, it is sometimes the case that individual
  events need to be put in a context to determine which type of
  treatment is appropriate (as opposed to each event being
  "self-identifying").  The state machines discussed in this
  introduction are high-level machines that are intended to
  express clearly the programming requirements of a class of
  problems.  If it makes sense to talk about your programming
  problem in terms of categories of behavior in response to
  events, it is likely to be a good idea to program the solution
  in terms of explicit state machines.
  一个不正式而又启发性的定义比抽象性的定义要有用。我们经常遇到
  一些程序要求，它们包含了一把方法来处理一簇事件。
## 未完 <+未完+>

  TOPIC -- Text Processing State Machines
  主题 -- 文本处理状态机
  --------------------------------------------------------------------

  One of the programming problems most likely to call for an
  explicit state machine is processing text files. Processing a
  text file very often consists of sequential reading of each chunk
  of a text file (typically either a character or a line), and
  doing something in response to each chunk read. In some cases,
  this processing is "stateless"--that is, each chunk has enough
  information internally to determine exactly what to do in
  response to that chunk of text. And in other cases, even though
  the text file is not 100 percent stateless, there is a very
  limited context to each chunk (for example, the line number might
  matter for the action taken, but not much else besides the line
  number). But in other common text processing problems, the text
  files we deal with are highly "stateful"--the meaning of a chunk
  depends on what types of chunks preceded it (and maybe even on
  what chunks come next). Files like report files, mainframe
  data-feeds, human-readable texts, programming source files, and
  other sorts of text files are stateful. A very simple example of
  a stateful chunk is a line that might occur in a Python source
  file:*
  编程问题中最有可能显式调用状态机的一个问题是文本文件处理。
  文本文件处理通常需要顺序读入该文件中的块（典型的块大小是一个字符
  或者一行），根据读入的每块进行处理。在一些案例中，这样的处理是
  “无状态的”-- 也就是说，每一个块都有足够的内部信息来准确决定如何
  处理该块文本。同时在其他案例中，即使文本文件并非百分百无状态，
  对于每个块而言，仍存在有限的上下文（例如，行号可能影响采取的动作，
  但除此之外并无其他影响）。在其他常见文本处理问题中，我们处理
  的文本文件是高度“有状态的”-- 一个块的意义依赖于它前面块的类型
  （也许还和后面块有关）。很多文件，例如报告，主机数据种子，
  人类可读文本，程序源文件，以及其他类型的文本文件，都是有状态的。
  有状态的块的一个极其简单的例子就是Python源代码中的一行：

      #*----- Python code line -----#
      #*----- Python 代码行 -----#
      myObject = SomeClass(this, that, other)

  That line means something very different if it happens to be
  surrounded by these lines:
  上一行如果被这些行包围，将有相当不同的含义：

      #*----- Python code line quoted -----#
      """How to use SomeClass:
      myObject = SomeClass(this, that, other)
      """

  That is, we needed to know that we were in a "blockquote"
  -state- to determine that the line was a comment rather than an
  action.  Of course, a program that deals with Python programs
  in a more general way will usually use a parser and grammar.
  就是说，我们需要知道我们正在-状态-“块引用（blockquote）”中，
  这样才能确定当前行是注释而非动作。理所当然，如果某程序
  以一种更通用的方法来处理Python程序，它需要使用解析器和语法。


  TOPIC -- When Not to Use a State Machine
  主题 -- 何时不需状态机
  --------------------------------------------------------------------

  When we begin the task of writing a processor for any stateful
  text file, the first question we should ask ourselves is "What
  types of things do we expect to find in the file?" Each type of
  thing is a candidate for a state.  These types should be
  several in number, but if the number is huge or indefinite, a
  state machine is probably not the right approach--maybe some
  sort of database solution is appropriate.  Or maybe the problem
  has not been formulated right if there appear to be that many
  types of things.
  当我们开始为有状态文本文件编写处理器时，我们问自己的第一个问题
  就是：“我们期望在此文件中找到什么东西？”每种类型的东西都是
  状态的候选者。这些类型应该有多个，但是当数量很巨大或者不确定时，
  状态机很有可能并非正确解决方法--也许某些数据库方案才是合适的。
  再或者此问题并未正确陈述，导致出现如此多的类型。

  Moreover, we are not quite ready for a state machine yet; there
  may yet be a simpler approach.  It might turn out that even
  though our text file is stateful there is an easy way to read
  in chunks where each chunk is a single type of thing.  A state
  machine is really only worth implementing if the transitions
  between types of text require some calculation based on the
  content within a single state-block.
  此外，我们也未完全为状态机准备好；也可能有更简单的方法。
  也许尽管我们的文本文件是有状态的，但是有简单方法来读入块，
  此处块是一个简单类型事物。值得实现状态机的唯一原因是，文本类型
  之间的转换需要进行基于单个状态块内容的计算。

  An example of a somewhat stateful text file that is nonetheless
  probably not best handled with a state machine is a Windows-style
  '.ini' file (generally replaced nowadays by use of the
  binary-data-with-API Windows registry). Those files consist of
  some section headers, some comments, and a number of value
  assignments. For example:
  Windows风格的‘.ini’文件（现在通常已被Windows注册表代替，后者是
  二进制，提供API）
  是有状态的文本文件的一个例子，尽管状态机很可能并非最佳解决方案。
  这些文件提供节头信息，注释，以及一些赋值操作。例如：

      #-------------- File: hypothetical.ini ------------------#
      #-------------- 文件: hypothetical.ini ------------------#
      ; set the colorscheme and userlevel
      [colorscheme]
      background=red
      foreground=blue
      title=green

      [userlevel]
      login=2
      ; admin=0
      title=1

  This example has no real-life meaning, but it was constructed to
  indicate some features of the '.ini' format. (1) In one sense,
  the type of each line is determined by its first character
  (either semicolon, left brace, or alphabetic). (2) In another
  sense, the format is "stateful" insofar as the keyword "title"
  presumably means something independent when it occurs in each
  section. You could program a text processor that had a
  COLORSCHEME state and a USERLEVEL state, and processed the value
  assignments of each state. But that does not seem like the
  -right- way to handle this problem.
  上例在实际生活中并无任何意义，只是建立了以显示'.ini'格式的一些特性。
  （1）从某个角度看，每行的类型由它的首字母（分号，左括号或者字母，
  三者之一）决定。（2）从另外一个角度看，此格式属于“有状态的”范畴，
  因为可以推测关键词“title”的含义在不同节是互相独立的。你可以编写一个
  文本处理器，它有一个COLORSCHEME状态和一个USERLEVEL状态，
  并处理每个状态的赋值。但是这个看上去并非处理问题的-正确-方法。

  On the one hand, we could simply create the natural chunks in
  this text file with some Python code like:
  从另一方面，我们可以用Python代码创建自然块，如下：

      #------ Chunking Python code to process .ini file --------#
      txt = open('hypothetical.ini').read()
      from string import strip, split
      nocomm = lambda s: s[0] != ';'        # "no comment" util
      eq2pair = lambda s: split(s,'=')      # assignmet -> pair
      def assignments(sect):
          name, body = split(sect,']')      # identify name, body
          assigns = split(body,'\n')        # find assign lines
          assigns = filter(strip, assigns)  # remove outside space
          assigns = filter(None, assigns)   # remove empty lines
          assigns = filter(nocomm, assigns) # remove comment lines
          assigns = map(eq2pair, assigns)   # make name/val pairs
          assigns = map(tuple, assigns)     # prefer tuple pairs
          return (name, assigns)
      sects = split(txt,'[')                # divide named sects
      sects = map(strip, sects)             # remove outside newlines
      sects = filter(nocomm, sects)         # remove comment sects
      config = map(assignments, sects)      # find assigns by sect
      pprint.pprint(config)

      #------ Python 割块代码，用来处理.ini文件 --------#
      txt = open('hypothetical.ini').read()
      from string import strip, split
      nocomm = lambda s: s[0] != ';'        # “没有注释”功能
      eq2pair = lambda s: split(s,'=')      # 赋值对
      def assignments(sect):
          name, body = split(sect,']')      # 个体名字，躯体
          assigns = split(body,'\n')        # 寻找赋值行
          assigns = filter(strip, assigns)  # 删除外面的空格
          assigns = filter(None, assigns)   # 删除空行
          assigns = filter(nocomm, assigns) # 删除注释行
          assigns = map(eq2pair, assigns)   # 制作“名/值”对
          assigns = map(tuple, assigns)     # 偏好元组对
          return (name, assigns)
      sects = split(txt,'[')                # 分割名字节
      sects = map(strip, sects)             # 删除外面的新行
      sects = filter(nocomm, sects)         # 删除注释节
      config = map(assignments, sects)      # 寻找节中的赋值
      pprint.pprint(config)
      
  Applied to the 'hypothetical.ini' file above, this code produces
  output similar to:
  对上面的“hypothetical.ini”应用以上代码，输出如下：

      #*----- Parsed .ini structure -----#
      #*----- 解析 .ini 结构 -----#
      [('colorscheme',
        [('background', 'red'),
         ('foreground', 'blue'),
         ('title', 'green')]),
       ('userlevel',
        [('login', '2'),
         ('title', '1')])]

  This particular list-oriented data structure may or may not be
  what you want, but it is simple enough to transform this into
  dictionary entries, instance attributes, or whatever is desired.
  Or slightly modified code could generate other data
  representations in the first place.
  该基于列表的数据结构也许不是你想要的，但是你可以很容易就将其
  转换到字典条目、实例特性或者其他任何你想要的格式。你也可以
  稍稍修改代码，直接生成想要的数据表现形式。

  An alternative approach is to use a single 'current_section'
  variable to keep track of relevant state and process lines
  accordingly:
  另一个方法是使用一个“current_section”变量来跟踪相关状态，
  并以此处理各行：

      #*------ Counting Python code to process .ini file ------#
      #*------ Python 计数代码， 用于处理 .ini 文件 ------#
      for line in open('hypothetical.ini').readlines():
          if line[0] == '[':
              current_section = line[1:-2]
          elif line[0] == ';':
              pass    # ignore comments 忽略注释
          else:
              apply_value(current_section, line)


  SIDEBAR: A digression on functional programming
  工具条：关于函数式编程的题外话
  -------------------------------------------------------------------

  Readers will have noticed that the '.ini' chunking code given in
  the example above has more of a functional programming (FP) style
  to it than does most Python code (in this book or elsewhere). I
  wrote the presented code this way for two reasons. The more
  superficial reason is just to emphasize the contrast with a state
  machine approach. Much of the special quality of FP lies in its
  eschewal of state (see the discussion of functional programming
  in Chapter 1); so the example is, in a sense, even farther from a
  state machine technique than would be a coding style that used a
  few nested loops in place of the 'map()' and 'filter()' calls.
  读者会发现上例中给出的‘.ini’代码块更接近与函数式编程（FP）风格，
  而非绝大部分Python代码（不管是在本书中还是其他地方）。我以这种
  风格编写上述代码有两个原因。表面一点的原因是用来强调与状态机
  方法的对比。FP的许多特性存在于它对状态的回避（请见第一章中对函数式
  编程的讨论）；所以从某种角度上来说，本例不像状态机技术，而有点
  类似编程风格，后者在调用‘map()’和‘filter()’的地方使用了一些
  嵌套循环。

  The more substantial reason I adopted a functional programming
  style is because I feel that this type of problem is precisely
  the sort that can often be expressed more compactly and more
  -clearly- using FP constructs. Basically, our source text
  document expresses a data structure that is homogeneous at each
  level. Each section is similar to other sections; and within a
  section, each assignment is similar to others. A clear--and
  stateless--way to manipulate these sorts of implicit structures
  is applying an operation uniformly to each thing at a given
  level. In the example, we do a given set of operations to find
  the assignments contained within a section, so we might as well
  just 'map()' that set of operations to the collection of
  (massaged, noncomment) sections. This approach is more terse than
  a bunch of nested 'for' loops, while simultaneously (in my
  opinion) better expressing the underlying intention of the
  textual analysis.
  而我采用函数式编程风格的另一个更真实的原因是，我觉得该类型问题
  正是那类FP构造可以更简洁更-清晰-表达的问题。基本上，我们的源
  文本文件表达的数据结构在每一层都是相似的。每一节都和其他节
  相似；而在每一节内，每个赋值都和其他节类似。一个清晰-而且无状态的-
  方法，用来操纵上述结构，被用来在给定层统一地操作所有东西。
  在例子中，我们进行给定的一系列操作，以发现某节中包含的赋值语句，
  这样我们可以只要‘map()’这些语句到(massaged, noncomment)节中。
## ？？？
  这个方法和嵌套‘for’循环比较起来更加扼要，同时，（在我眼里）表达
  了文本分析的本质内涵。

  Use of a functional programming style, however, can easily be
  taken too far. Deeply nested calls to 'map()', 'reduce()', and
  'filter()' can quickly become difficult to read, especially if
  whitespace and function/variable names are not chosen carefully.
  Inasmuch as it is possible to write "obfuscated Python" code (a
  popular competition for other languages), it is almost always
  done using FP constructs. Warnings in mind, it is possible to
  create an even terser and more functional variant of the '.ini'
  chunking code (that produces identical results). I believe that
  the following falls considerably short of obfuscated, but will
  still be somewhat more difficult to read for most programmers. On
  the plus side, it is half the length of the prior code and is
  entirely free of accidental side effects:
  但是使用函数式编程风格很容易被带得太远。深度嵌套‘map()’、
  ‘reduce()’和‘filter()’很快会让阅读变得困难，特别是在空白和
  函数/变量名并未仔细的选择的情况下。如果有可能要编写
  “模糊Python（Obfuscated Python）”
  代码（对于其他语言而言，这可能是一种流行竞赛），你总可以
  使用FP构造。你脑中需要知道，你也可能创造一个‘.ini’块文件的变种，
  更加简洁同时又更多功能，但是产生同样的结果。我相信下列代码并
  没有太多模糊不清，但对绝大部分程序员而言，是有难度的。
  从好的一面看，代码长度只有以前的一半，几乎没有附属副效应。

      #---- Strongly functional code to process .ini file -----#
      #---- 用于处理.ini的很函数式的代码 -----#
      from string import strip, split
      eq2tup = lambda s: tuple(split(s,'='))
      splitnames = lambda s: split(s,']')
      parts = lambda s, delim: map(strip, split(s, delim))
      useful = lambda ss: filter(lambda s: s and s[0]!=';', ss)
      config = map(lambda _:(_[0], map(eq2tup, useful(parts(_[1],'\n')))),
                   map(splitnames, useful(parts(txt,'['))) )
      pprint.pprint(config)

  In brief, this functional code says that a configuration consists
  of a list of pairs of (1) names plus (2) a list of key/value
  pairs. Using list comprehensions might make this expression
  clearer, but the example code is compatible back to Python 1.5.
  Moreover, the utility function names 'useful()' and 'parts()' go
  a long way towards keeping the example readable. Utility
  functions of this sort are, furthermore, potentially worth saving
  in a separate module for other use (which, in a sense, makes the
  relevant '.ini' chunking code even shorter).
  简单说，该函数式代码说明配置是由一个列表的配对组成，配对由
  （1）名字加上（2）一个值/键对列表。使用列表综合可能使得该表达式
  更加清楚，但示例代码可以和Python 1.5向下兼容。而且，两个功能函数，
  ‘useful()’和‘parts()’，很努力地将例子保持可读。更远一点看，
  此类功能函数值得保存在单独的模块中，以供他用（这样从某种角度来看，
  使得相关的‘.ini’块代码更加短小）。

  A reader exercise is to consider how the higher-order functions
  proposed in Chapter 1's section on functional programming
  could further improve the sort of "stateless" text processing
  presented in this subsection.
  第一章中关于函数式编程的小节提出了高阶函数，给读者的练习就是，
  思考它如何才能改进本段所述“无状态”文本处理。


  TOPIC -- When to Use a State Machine
  主题 -- 何时需要状态机
  -------------------------------------------------------------------

  Now that we have established not to use a state machine if the
  text file is "too simple," we should look at a case where a
  state machine is worthwhile.  The utility 'Txt2Html' is listed
  in Appendix D.  'Txt2Html' converts "smart ASCII" files to
  HTML.
  我们已经确认当一个文本文件“太简单”的时候就不需要状态机。
  现在我们来看一个状态机是值得的情况。附录D列出了工具‘Txt2Html’，
  它将“智能ASCII”文件转换成HTML文件。

  In very brief recap, smart ASCII format is a text format that
  uses a few spacing conventions to distinguish different types of
  text blocks, such as headers, regular text, quotations, and code
  samples. While it is easy for a human reader or writer to
  visually parse the transitions between these text block types,
  there is no simple way to chunk a whole text file into its text
  blocks. Unlike in the '.ini' file example, text block types can
  occur in any pattern of alternation. There is no single delimiter
  that separates blocks in all cases (a blank line -usually-
  separates blocks, but a blank line within a code sample does not
  necessarily end the code sample, and blocks need not be separated
  by blank lines). But we do need to perform somewhat different
  formatting behavior on each text block type for the correct final
  XML output. A state machine suggests itself as a natural solution
  here.
  简单复述一遍，智能ASCII格式是一种文本格式，它使用了一些空格约定
  来区别不同类型的文字块，例如头，普通文本，引用以及代码例子。
  对于人类读者或者作家而言，很容易就可以视觉解析出文本块类型的转换，
  但是没有简单方法来将整个文件分割成文本块。和‘.ini’文件不同，
  文本块类型可在任何轮换模式中发生。可以分割所有情况中的块的分隔符
  并不存在（空白行-通常-表示分割块，但是示例代码中的空白行并不表示
  代码例子的结束，块也不需要以空白行结束）。但我们确实需要对每个类型的文本
  块进行某种不一样的格式化，以最终输出正确的XML输出。状态机可以证明
  是一个自然的解决方案。

  The general behavior of the 'Txt2Html' reader is as follows:
  (1) Start in a particular state. (2) Read a line of the text
  file and go to current state context. (3) Decide if conditions
  have been met to leave the current state and enter another. (4)
  Failing (3), process the line in a manner appropriate for the
  current state.  This example is about the simplest case you
  would encounter, but it expresses the pattern described:
  ‘Txt2Html’的所作所为如下所示：（1）从某特别状态开始。（2）从
  文件中读入一行，并进入当前的状态上下文。（3）离开当前状态
  的条件是否满足？确定是否进入另一个状态。（4）如果上一步失败，
  在该状态中，以合理的方式处理该行。本例是你将遇上的最简单的例子，
  但它表现了描述的模式：
  

      #----- A simple state machine input loop in Python -----#
      #----- Python中的一个简单状态机的输入循环 -----#
      global state, blocks, newblock
      for line in fpin.readlines():
          if state == "HEADER":         # blank line means new block of ? #空白行表示新的什么块？
              if blankln.match(line):   newblock = 1
              elif textln.match(line):  startText(line)
              elif codeln.match(line):  startCode(line)
              else:
                  if newblock: startHead(line)
                  else: blocks[-1] += line
          elif state == "TEXT":         # blank line means new block of ? #空白行表示新的什么块？
              if blankln.match(line):   newblock = 1
              elif headln.match(line):  startHead(line)
              elif codeln.match(line):  startCode(line)
              else:
                  if newblock: startText(line)
                  else: blocks[-1] += line
          elif state == "CODE":         # blank line does not change state #空白行并不改变状态
              if blankln.match(line):   blocks[-1] += line
              elif headln.match(line):  startHead(line)
              elif textln.match(line):  startText(line)
              else: blocks[-1] += line
          else:
              raise ValueError, "unexpected input block state: "+state

  The only real thing to notice is that the variable 'state' is
  declared 'global', and its value is changed in functions like
  'startText()'.  The transition conditions--such as
  'textln.match()'--are regular expression patterns, but they
  could just as well be custom functions.  The formatting itself
  is actually done later in the program; the state machine just
  parses the text file into labeled blocks in the 'blocks' list.
  In a sense, the state machine here is acting as a tokenizer for
  the later block processor.
  唯一要注意的事情是，变量‘state’被申报为‘全局变量（global）’，
  它的值在函数中可能会被改变，例如‘startText()’。转换条件--例如
  ‘textIn.match()’--是正则表达式模式，但是也可以是自定义函数。
  格式化实际上在程序后面进行；状态机只是解析文本文件，放进‘blocks’
  列表中打好标签的块中。从某种角度看，这里的状态机只是为了
  后面的块处理器充当特征标示器。


  TOPIC -- An Abstract State Machine Class
  主题 -- 抽象的状态机类
  -------------------------------------------------------------------

  It is easy in Python to abstract the form of a state machine.
  Coding in this manner makes the state machine model of the
  program stand out more clearly than does the simple conditional
  block in the previous example (which doesn't right away look all
  that much different from any other conditional). Furthermore, the
  class presented--and the associated handlers--does a very good
  job of isolating in-state behavior. This improves both
  encapsulation and readability in many cases.
  Python可以很轻松抽象状态机的形式。如此编码使得程序的状态机模型
  和前面的例子比较更加清晰（它们和其他条件句相比，并不是完全相同）。
  而且，表述的类--以及相关的句柄--可以很好地隔离状态内的行为。在
  很多情况里，这个可以提高封装性和可读性。

      #-------------- File: statemachine.py ------------------#
      #-------------- 文件: statemachine.py ------------------#
      class InitializationError(Exception): pass

      class StateMachine:
          def __init__(self):
              self.handlers = []
              self.startState = None
              self.endStates = []

          def add_state(self, handler, end_state=0):
              self.handlers.append(handler)
              if end_state:
                  self.endStates.append(name)

          def set_start(self, handler):
              self.startState = handler

          def run(self, cargo=None):
              if not self.startState:
                  raise InitializationError,\
                        "must call .set_start() before .run()"
              if not self.endStates:
                  raise InitializationError, \
                        "at least one state must be an end_state"
              handler = self.startState
              while 1:
                  (newState, cargo) = handler(cargo)
                  if newState in self.endStates:
                      newState(cargo)
                      break
                  elif newState not in self.handlers:
                      raise RuntimeError, "Invalid target %s" % newState
                  else:
                      handler = newState

  The 'StateMachine' class is really all you need for the form of
  a state machine.  It is a whole lot fewer lines than something
  similar would require in most languages--mostly because of the
  ease of passing function objects in Python.  You could even
  save a few lines by removing the target state check and the
  'self.handlers' list, but the extra formality helps enforce and
  document programmer intention.
  ‘StateMachine’类正是你需要的状态机。和其他语言相比，它
  使用的行数更少--这要归功于Python中目前所使用的函数对象。
  如果你移除目标状态检查和‘self.handlers’列表，你还可以省下几行。
  但是这些额外的束缚可以增强和记录程序员的意图。

  To actually -use- the 'StateMachine' class, you need to create
  some handlers for each state you want to use.  A handler must
  follow a particular pattern.  Generally, it should loop
  indefinitely; but in any case it must have some breakout
  condition(s).  Each pass through the state handler's loop
  should process another event of the state's type. But probably
  even before handling events, the handler should check for
  breakout conditions and determine what state is appropriate to
  transition to.  At the end, a handler should pass back a tuple
  consisting of the target state's name and any cargo the new
  state handler will need.
  为了实际-使用-‘StateMachine’类，你需要为每个想要使用的状态
  创建一些句柄。每个句柄必须满足一个特别的模式。通常它需要无限
  循环；但是无论如何它必须至少有一个跳出条件。每次执行状态句柄的
  循环时，都应该处理该状态类型的另一个事件。但是很可能在处理
  句柄事件之前，句柄就需要检查跳出条件以决定转往合适的状态。
  最后，句柄应该传回一个元组，其中有目标状态的名字以及任何新
  状态句柄所需的“货物（cargo）”。

  An encapsulation device is the use of 'cargo' as a variable in
  the 'StateMachine' class (not necessarily called 'cargo' by the
  handlers). This is used to pass around "whatever is needed" by
  one state handler to take over where the last state handler
  left off.  Most typically, 'cargo' will consist of a
  file handle, which would allow the next handler to read some
  more data after the point where the last state handler stopped.
  But a database connection might get passed, or a complex
  class instance, or a tuple with several things in it.
  封装设备就是在‘StateMachine’类中将‘货物’作为变量使用。
  当然在句柄中不一定叫‘货物（cargo）’这个名字。这个方式正是
  离开上一个状态，下一个状态句柄接手时，如何传递“它所需要的东西”的。
  更典型的，‘货物’拥有一个文件句柄，可以使得下一个状态句柄
  在当前句柄结束以后，读入更多数据。不过也可能传递一个数据库连接，
  或者一个复合类示例，再或者一个元组，其中包含了一些信息。


  TOPIC -- Processing a Report with a Concrete State Machine
  主题 -- 使用一个具体的状态机来处理报告
  -------------------------------------------------------------------

  A moderately complicated report format provides a good example of
  some processing amenable to a state machine programming
  style--and specifically, to use of the 'StateMachine' class
  above. The hypothetical report below has a number of
  state-sensitive features. Sometimes lines belong to buyer orders,
  but at other times the identical lines could be part of comments
  or the heading. Blank lines, for example, are processed
  differently from different states. Buyers, who are each processed
  according to different rules, each get their own machine state.
  Moreover, within each order, a degree of stateful processing is
  performed, dependent on locally accumulated calculations:
  一个中等复杂的报告格式可以作为一个很好的例子，为状态机编程风格
  --特别是上述的‘状态机’类--提供一些启发。下面这个假想的报告
  颇有一些对状态敏感的特性。有些行属于买家订单，但有时同样的行
  却可能是注释或者标题。比如说空白行，在不同的状态中，处理的方式
  也不同。每个买家是根据不同的规则处理的，因此也拥有他们自己的状态机。
  而且，每个订单中，根据局部累积计算，会进行状态程度处理：

      #------------ Sample Buyer/Order Report ----------------#
      MONTHLY REPORT -- April 2002
      #--------------------- 买家/订单 报告 例子 ----------------------#
      月度报告 -- 2002年4月
      ===================================================================

      Rules:
       - Each buyer has price schedule for each item (func of quantity).
       - Each buyer has a discount schedule based on dollar totals.
       - Discounts are per-order (i.e.  contiguous block)
       - Buyer listing starts with line containing ">>", then buyer name.
       - Item quantities have name-whitespace-number, one per line.
       - Comment sections begin with line starting with an asterisk,
         and ends with first line that ends with an asterisk.
      规则：
       - 每个买家都有每个物品的价格一览表（数量的函数）。
       - 每个买家都有基于总共花费的折扣一览表。
       - 折扣是针对每个订单的（就是说，连续的块）
       - 买家列表开始的行包含“>>”，后面是买家名字。
       - 物品数量格式是“名字-空白-数字”，每个一行。
       - 注释节的第一行以星号开头，最后一行与第一行相同，但以星号结束。
##此处不懂

      >> Acme Purchasing

        widgets      100
        whatzits    1000
        doodads     5000
        dingdongs   20

      * Note to Donald: The best contact for Acme is Debbie Franlin, at
      * 413-555-0001.  Fallback is Sue Fong (call switchboard). *

      >> Megamart

      doodads   10k
      whatzits  5k

      >> Fly-by-Night Sellers
         widgets        500
         whatzits      4
         flazs         1000

      * Note to Harry: Have Sales contact FbN for negotiations *

      *
      Known buyers:
      >>  Acme
      >>  Megamart
      >>  Standard (default discounts)
      *

      *** LATE ADDITIONS ***

      >> Acme Purchasing
      widgets      500     (rush shipment)**

  The code to processes this report below is a bit simplistic.
  Within each state, almost all the code is devoted merely to
  deciding when to leave the state and where to go next.  In the
  sample, each of the "buyer states" is sufficiently similar that
  they could well be generalized to one parameterized state; but
  in a real-world application, each state is likely to contain
  much more detailed custom programming for both in-state
  behavior and out-from-state transition conditions.  For
  example, a report might allow different formatting and fields
  within different buyer blocks.
  下面处理该报告的代码有些过于简单。在每个状态中，几乎所有的代码
  都用来决定何时离开当前状态，转到哪个状态。在此例中，
  每个“买家状态”都充分相似，所以都可以使用一个参数化的状态
  来产生；但在实际生活中的程序里，每个状态都还会包含更多的详细的
  自定义编程，以说明状态内行为和转出状态的条件。例如，报告可能
  允许不同买家块中有不同的格式以及域。

      #------------------ buyer_invoices.py -------------------#
      from statemachine import StateMachine
      from buyers import STANDARD, ACME, MEGAMART
      from pricing import discount_schedules, item_prices
      import sys, string

      #-- Machine States
      def error(cargo):
          # Don't want to get here! Unidentifiable line
          # 此处伤心地！ 无法识别的行
          sys.stderr.write('Unidentifiable line:\n'+ line)

      def eof(cargo):
          # Normal termination -- Cleanup code might go here.
          # 正常结束 -- 清理代码可能会运行到此处
          sys.stdout.write('Processing Successful\n')

      def read_through(cargo):
          # Skip through headers until buyer records are found
          # 略过文件头，直到找到买家订单
          fp, last = cargo
          while 1:
              line = fp.readline()
              if not line:            return eof, (fp, line)
              elif line[:2] == '>>':  return whichbuyer(line), (fp, line)
              elif line[0] == '*':    return comment, (fp, line)
              else:                   continue

      def comment(cargo):
          # Skip comments
          # 略过注释
          fp, last = cargo
          if len(last) > 2 and string.rstrip(last)[-1:] == '*':
              return read_through, (fp, '')
          while 1:
              # could save or process comments here, if desired
              # 如果希望的话，可以在这里保存或处理注释。
              line = fp.readline()
              lastchar = string.rstrip(line)[-1:]
              if not line:            return eof, (fp, line)
              elif lastchar == '*':   return read_through, (fp, line)

      def STANDARD(cargo, discounts=discount_schedules[STANDARD],
                          prices=item_prices[STANDARD]):
          fp, company = cargo
          invoice = 0
          while 1:
              line = fp.readline()
              nextstate = buyerbranch(line)
              if nextstate == 0: continue         # blank line #空白行
              elif nextstate == 1:                # order item        #买单条目
                  invoice = invoice + calc_price(line, prices)
              else:                               # create invoice          #创建发票
                  pr_invoice(company, 'standard', discount(invoice,discounts))
                  return nextstate, (fp, line)

      def ACME(cargo, discounts=discount_schedules[ACME],
                      prices=item_prices[ACME]):
          fp, company = cargo
          invoice = 0
          while 1:
              line = fp.readline()
              nextstate = buyerbranch(line)
              if nextstate == 0: continue         # blank line   #空白行
              elif nextstate == 1:                # order item          #买单条目
                  invoice = invoice + calc_price(line, prices)
              else:                               # create invoice             #创建发票
                  pr_invoice(company, 'negotiated', discount(invoice,discounts))
                  return nextstate, (fp, line)

      def MEGAMART(cargo, discounts=discount_schedules[MEGAMART],
                          prices=item_prices[MEGAMART]):
          fp, company = cargo
          invoice = 0
          while 1:
              line = fp.readline()
              nextstate = buyerbranch(line)
              if nextstate == 0: continue         # blank line  #空白行
              elif nextstate == 1:                # order item         #买单条目
                  invoice = invoice + calc_price(line, prices)
              else:                               # create invoice            #创建发票
                  pr_invoice(company, 'negotiated', discount(invoice,discounts))
                  return nextstate, (fp, line)

      #-- Support function for buyer/state switch
      #-- 买家/状态转换支持函数
      def whichbuyer(line):
          # What state/buyer does this line identify?
          # 本行确认为哪个状态/买家？
          line = string.upper(string.replace(line, '-', ''))
          find = string.find
          if find(line,'ACME') >= 0:      return ACME
          elif find(line,'MEGAMART')>= 0: return MEGAMART
          else:                           return STANDARD

      def buyerbranch(line):
          if not line:                    return eof
          elif not string.strip(line):    return 0
          elif line[0] == '*':            return comment
          elif line[:2] == '>>':          return whichbuyer(line)
          else:                           return 1

      #-- General support functions
      #-- 一般支持函数
      def calc_price(line, prices):
          product, quant = string.split(line)[:2]
          quant = string.replace(string.upper(quant),'K','000')
          quant = int(quant)
          return quant*prices[product]

      def discount(invoice, discounts):
          multiplier = 1.0
          for threshhold, percent in discounts:
              if invoice >= threshhold: multiplier = 1 - float(percent)/100
          return invoice*multiplier

      def pr_invoice(company, disctype, amount):
          print "Company name:", company[3:-1], "(%s discounts)" % disctype
          print "Invoice total: $", amount, '\n'

      if __name__== "__main__":
          m = StateMachine()
          m.add_state(read_through)
          m.add_state(comment)
          m.add_state(STANDARD)
          m.add_state(ACME)
          m.add_state(MEGAMART)
          m.add_state(error, end_state=1)
          m.add_state(eof, end_state=1)
          m.set_start(read_through)
          m.run((sys.stdin, ''))

  !!!

  The body of each state function consists mostly of a 'while 1:'
  loop that sometimes breaks out by returning a new target state,
  along with a cargo tuple.  In our particular machine, 'cargo'
  consists of a file handle and the last line read.  In some
  cases, the line that signals a state transition is also needed
  for use by the subsequent state.  The cargo could contain
  whatever we wanted.  A flow diagram lets you see the set of
  transitions easily:
  几乎每个状态函数内都有一个‘while 1:’循环，后者有时跳出时
  返回一个新的状态目标，还有一个货物元组。在我们这个机器中，
  ‘货物’包含了一个文件句柄和读入的最后一行。有些情况下，一些行
  显示需要状态装换，后面的状态也需要这些行。货物可以包括任何我们
  需要的东西。下面的流程图可以让你轻松看遍各个转换：

      #----- Buyer state machine diagram -----#
      #----- 买家状态机图 -----#
      <<buyer_statechart.eps>>

  All of the buyer states are "initialized" using default
  argument values that are never changed during calls by a normal
  state machine '.run()' cycle.  You could also perhaps design
  state handlers as classes instead of as functions, but that
  feels like extra conceptual overhead to me.  The specific
  initializer values are contained in a support module that looks
  like:
  所有的买家状态都以默认参数值“初始化”，后者在正常的状态机‘.run()’周期
  呼叫中，从未改变过。你也可以将状态句柄设计为类，而非函数，但
  这个对我来说就是概念上的额外开销了。制定的初始值包含在支持模块中，如下：

      #------------- pricing.py support data ------------------#
      #------------- pricing.py 支持数据 ------------------#
      from buyers import STANDARD, ACME, MEGAMART, BAGOBOLTS

      # Discount consists of dollar requirement and a percentage reduction
      # Each buyer can have an ascending series of discounts, the highest
      # one applicable to a month is used.
      # 折扣包含了需要的金额和减少的百分数
      # 每个买家可以有一系列逐渐增大的折扣，
      # 采用对某月适用的最大折扣
      discount_schedules = {
          STANDARD  : [(5000,10),(10000,20),(15000,30),(20000,40)],
          ACME      : [(1000,10),(5000,15),(10000,30),(20000,40)],
          MEGAMART  : [(2000,10),(5000,20),(10000,25),(30000,50)],
          BAGOBOLTS : [(2500,10),(5000,15),(10000,25),(30000,50)],
        }
      item_prices = {
          STANDARD  : {'widgets':1.0, 'whatzits':0.9, 'doodads':1.1,
                       'dingdongs':1.3, 'flazs':0.7},
          ACME      : {'widgets':0.9, 'whatzits':0.9, 'doodads':1.0,
                       'dingdongs':0.9, 'flazs':0.6},
          MEGAMART  : {'widgets':1.0, 'whatzits':0.8, 'doodads':1.0,
                       'dingdongs':1.2, 'flazs':0.7},
          BAGOBOLTS : {'widgets':0.8, 'whatzits':0.9, 'doodads':1.1,
                       'dingdongs':1.3, 'flazs':0.5},
        }

  In place of reading in such a data structure, a full application
  might calculate some values or read them from a database of some
  sort. Nonetheless, the division of data, state logic, and
  abstract flow into separate modules makes for a good design.
  对于读入这样 一个数据结构，完全版的程序可以计算出某些值，或者
  以从某种数据库中读入代替。尽管如此，数据、状态逻辑以及
  抽象流被分割进几个单独的模块，表明这是个较好的设计。 


  TOPIC -- Subgraphs and State Reuse
  主题 -- 子图和状态重用
  -------------------------------------------------------------------

  Another benefit of the state machine design approach is that
  you can use different start and end states without touching
  the state handlers at all.  Obviously, you do not have complete
  freedom in doing so--if a state branches to another state, the
  branch target needs to be included in the list of "registered"
  states.  You can, however, add homonymic handlers in place of
  target processing states.  For example:
  状态机设计方法的另一个好处是，你根本不需要接触任何状态句柄，
  就可以使用不同状态的开始和结束。显然，你不可能对此拥有完全的
  自由--如果一个状态出现分枝，转到另外一个状态，转向的那个状态
  需要被包括在“注册的”状态列表中。但是，你可以在应该出现目标
  处理状态的地方，添加一个同名句柄。例如：

      #----- Creating end states for subgraphs -----#
      #----- 为子图创建结束状态 -----#
      from statemachine import StateMachine
      from BigGraph import *

      def subgraph_end(cargo): print "Leaving subgraph..."
      foo = subgraph_end
      bar = subgraph_end

      def spam_return(cargo): return spam, None
      baz = spam_return

      if __name__=='__main__':
          m = StateMachine()
          m.add_state(foo, end_state=1)
          m.add_state(bar, end_state=1)
          m.add_state(baz)
          map(m.add_state, [spam, eggs, bacon])
          m.set_start(spam)
          m.run(None)

  In a complex state machine graph, you often encounter relatively
  isolated subgraphs. That is, a particular collection of
  states--i.e., nodes--might have many connections between them, but
  only a few connections out to the rest of the graph. Usually this
  occurs because a subgraph concerns a related set of
  functionality.
  在一个复杂的状态机图中，你经常遇到相对独立的子图。也就是说，
  一些特别的状态--也就是节点--它们之间可能有许多连接，但是从
  它们出去和图的其他地方的连接，却不是很多。通常这个发生的
  原因是子图涉及到相关的一套功能。

  For processing the buyer report discussed earlier, only seven
  states were involved, so no meaningful subgraphs really exist.
  But in the subgraph example above, you can imagine that the
  [BigGraph] module contains hundreds or thousands of state
  handlers, whose targets define a very complex complete graph.
  Supposing that the states 'spam', 'eggs', and 'bacon' define a
  useful subgraph, and all branches out of the subgraph lead to
  'foo', 'bar', or 'baz', the code above could be an entire new
  application.
  为了处理前面讨论的买家报告，只包括了七个状态，所以不存在
  有意义的子图。但是在上面的子图例子中，你可以想象[BigGraph]
  模块中包含了成千上百个状态句柄，它们的目标定义为一个异常
  复杂而完整的图。假设状态‘spam’、‘eggs’和‘bacon’是
  一个有用的子图，而子图外的所有分枝都通向‘foo’、‘bar’和‘baz’，
  上面的代码可以称为一个完全新的程序。

  The example redefined 'foo' and 'bar' as end states, so
  processing (at least in that particular 'StateMachine' object)
  ends when they are reached.  However, 'baz' is redefined to
  transition back into the spam-eggs-bacon subgraph.  A subgraph
  exit need not represent a termination of the state machine.  It
  is actually the 'end_state' flag that controls termination--but
  if 'foo' was not marked as an end state, it would raise a
  'RuntimeError' when it failed to return a valid state target.
  例子将‘foo’和‘bar’重新定义为结束状态，当到达它们时，
  （至少是在‘StateMachine’对象中的）处理就结束了。
  但是，‘baz’被重新定义为转换回spam-eggs-bacon子图。
  子图出口没有必要代表状态机结束--但如果‘foo’没有被标记为
  结束状态，当它无法返回一个有效的状态目标时，
  它会引起‘RuntimeError’。

  If you create large graphs--especially with the intention of
  utilizing subgraphs as state machines--it is often useful to
  create a state diagram.  Pencil and paper are perfectly
  adequate tools for doing this; a variety of flow-chart software
  also exists to do it on a computer.  The goal of a diagram is
  to allow you to identify clustered subgraphs and most
  especially to help identify paths in and out of a functional
  subgraph.  A state diagram from our "Buyer Report" example is
  given as illustration.  A quick look at Figure 4.1, for
  example, allows the discovery that the 'error' end state is
  isolated, which might not have been evident in the code itself.
  This is not a problem, necessarily; a future enhancement to the
  diagram and handlers might utilize this state, and whatever
  logic was written into it.
  如果你创建了大图--特别是你想要使用子图作为状态机--创建一个
  状态图会比较有用。做这个，铅笔和白纸就完全足够了；各种各样的
  流程图软件可以帮助在电脑上完成这件事情。图的目标是允许你
  识别成群的子图，特别是识别进出某功能子图的路径。在我们
  “买家报告”例子中，已经给出一个状态图作为说明。快速回顾一下
  图4.1，可以发现结束状态‘error’被隔离，这个在代码中并不是
  很明显的。其实这不是一个问题；未来对此图和句柄的增强可能用到
  此状态，以及其中编写的逻辑。

  EXERCISE: Finding other solutions
  练习：寻找其他解决方案
  --------------------------------------------------------------------

  1.  On the face of it, a lot of "machinery" went into processing
      what is not really that complicated a report above.  The
      goal of the state machine formality was both to be robust
      and to allow for expansion to larger problems.  Putting
      aside the state machine approach in your mind, how else
      might you go about processing reports of the presented type
      (assume that "reasonable" variations occur between reports
      of the same type).
      表面上，许多“机械”可以处理简单的文档，远没有上面的报告复杂。
      状态机形式的目的在于健壮，而且可以扩展，以解决更大的问题。
      把状态机放到一旁，你还可能如何处理上述类型的报告呢？假设
      同类型的报告之间有一些“合理的”不同。

      Try writing a fresh report processing application that
      produces the same results as the presented application (or
      at least something close to it).  Test your application
      against the sample report and against a few variants you
      create yourself.
      尝试编写一个新的报告处理程序，和给出的程序产生一样的结果
      （或者至少和它相近的结果）。用例子报告和一些你自己创建的变种
      报告来测试你的程序。

      What errors did you encounter running your application?
      Why? Is your application more concise than the presented
      one? Which modules do you count as part of the presented
      application? Is your application's code clearer or less
      clear to follow for another programmer? Which approach
      would be easier to expand to encompass other report
      formats? In what respect is your application better/worse
      than the state machine example?
      当你运行程序时，你遇到什么错误？为什么？你的程序和给出的
      相比，是否更加简洁？你把哪些模块算作给出程序的一部分？
      你的程序对于其他程序员而言，是否更加清晰易读？如要扩展
      以包含其他报告格式，哪种方法更加容易？在哪些方面，你的程序
      比上述状态机例子更好/更差？

  2.  The 'error' state is never actually reached in the
      'buyer_invoices.py' application.  What other transition
      conditions into the 'error' state would be reasonable to
      add to the application? What types of corruption or
      mistakes in reports do you expect most typically to
      encounter? Sometimes reports, or other documents, are
      flawed, but it is still desirable to utilize as much of
      them as possible.  What are good approaches to recover
      from error conditions? How could you express those
      approaches in state machine terms, using the presented
      'StateMachine' class and framework?
      在‘buyer_invoices.py’程序中，状态‘error’从未被实际到达过。
      关于转换到状态‘error’的条件，有没有其他合理的可以加入程序？
      你预期报告中会出现那种类型的破坏或者错误？有时报告或者
      其他文档，是有缺陷的，但是希望能尽可能多地使用它们。
      从错误条件恢复的好方法是什么？在给出的‘StateMachine’类
      和框架中，你用状态机词汇能如何说明这些方法？

  !!!

SECTION 3 -- Parser Libraries for Python
第三节 -- Python的解析器库
------------------------------------------------------------------------

  TOPIC -- Specialized Parsers in the Standard Library
  主题 -- 标准库中的专门的解析器
  --------------------------------------------------------------------

  Python comes standard with a number of modules that perform
  specialized parsing tasks. A variety of custom formats are in
  sufficiently widespread use that it is convenient to have
  standard library support for them. Aside from those listed in
  this chapter, Chapter 5 discusses the [email] and [xml] packages,
  and the modules [mailbox], [HTMLParser], and [urlparse], each of
  which performs parsing of sorts. A number of additional modules
  listed in Chapter 1, which handle and process audio and image
  formats, in a broad sense could be considered parsing tools.
  However, these media formats are better considered as byte
  streams and structures than as token streams of the sort parsers
  handle (the distinction is fine, though).
  标准Python有一些模块，可以执行专门的解析任务。有许多自定义
  格式已经被充分的传播使用，有标准库的支持会非常方便。除了
  本章列出的，第五章讨论了[email]和[xml]包，以及[mailbox]、
  [HTMLParser]和[urlparse]模块，它们每个都进行某种类型的解析。
  第一章中更列出了许多额外的模块，它们可以处理音频和图像格式，
  在广义的角度上，也可以被看作解析工具。但是，这些媒体格式
  最好被看作字节流和结构，而非解析器句柄的特征流（尽管区别还
  可以接受）。

  The specialized tools discussed under this section are
  presented only in summary.  Consult the _Python Library
  Reference_ for detailed documentation of their various APIs and
  features.  It is worth knowing what is available, but for space
  reasons, this book does not document usage specifics of these
  few modules.
  本节讨论的专门的工具只是概述。参考_Python Library Reference_
  以获取各式API和特性的详细文档。知道什么是可利用的是值得的，
  但是鉴于篇幅，本书不能给出这几个模块的使用细节。

  ConfigParser
      Parse and modify Windows-style configuration files.
      解析和修改Windows风格的配置文件。

      >>> import ConfigParser
      >>> config = ConfigParser.ConfigParser()
      >>> config.read(['test.ini','nonesuch.ini'])
      >>> config.sections()
      ['userlevel', 'colorscheme']
      >>> config.get('userlevel','login')
      '2'
      >>> config.set('userlevel','login',5)
      >>> config.write(sys.stdout)
      [userlevel]
      login = 5
      title = 1
      
      [colorscheme]
      background = red
      foreground = blue

  difflib
  .../Tools/scripts/ndiff.py
      The module [difflib], introduced in Python 2.1, contains a
      variety of functions and classes to help you determine the
      difference and similarity of pairs of sequences.  The API
      of [difflib] is flexible enough to work with sequences of
      all kinds, but the typical usage is in comparing sequences
      of lines or sequences of characters.
      Pyton 2.1中介绍了[difflib]，它包含了各种各样的函数和类，
      以帮助你确定序列对之间的差异和相似。[difflib]的API很有弹性，
      足够处理所有类型的序列，但是典型的应用就是比较行序列或者
      字符序列。

      Word similarity is useful for determining likely
      misspellings and typos and/or edit changes required
      between strings.  The function `difflib.get_close_matches()`
      is a useful way to perform "fuzzy matching" of a string
      against patterns.  The required similarity is configurable.
      单词相似性对于确定是否错误拼写和/或字符串之间的编辑修改
      大有用处。函数`difflib.get_close_matches()`可以用来
      根据某些模式对一个字符串进行“模糊匹配（fuzzy matching）”。
      要求的相似性是可以配置的。

      >>> users = ['j.smith', 't.smith', 'p.smyth', 'a.simpson']
      >>> maxhits = 10
      >>> login = 'a.smith'
      >>> difflib.get_close_matches(login, users, maxhits)
      ['t.smith', 'j.smith', 'p.smyth']
      >>> difflib.get_close_matches(login, users, maxhits, cutoff=.75)
      ['t.smith', 'j.smith']
      >>> difflib.get_close_matches(login, users, maxhits, cutoff=.4)
      ['t.smith', 'j.smith', 'p.smyth', 'a.simpson']

      Line matching is similar to the behavior of the Unix 'diff'
      (or 'ndiff') and 'patch' utilities.  The latter utility is
      able to take a source and a difference, and produce the
      second compared line-list (file).  The functions
      `difflib.ndiff()` and `difflib.restore()` implement these
      capabilities.  Much of the time, however, the bundled
      'ndiff.py' tool performs the comparisons you are interested
      in (and the "patches" with an '-r#' option).
      行匹配和Unix上的‘diff’（或者‘ndiff’）和‘patch’工具
      相似。后面的工具可以输入源文件和变化，产生另一个
      作为比较的行列表（文件）。函数`difflib.ndiff()` 和 `difflib.restore()`
      实现了这些功能。但是很多时候，捆绑的'ndiff.py'工具
      （以及带‘-r#’参数的“patches”）会执行你感兴趣的
      比较。

      #*---------------- ndiff.py report -----------------------#
      % ./ndiff.py chap4.txt chap4.txt~ | grep '^[+-]'
      -: chap4.txt
      +: chap4.txt~
      +     against patterns.
      -     against patterns.  The required similarity is configurable.
      -
      -     >>> users = ['j.smith', 't.smith', 'p.smyth', 'a.simpson']
      -     >>> maxhits = 10
      -     >>> login = 'a.smith'

      There are a few more capabilities in the [difflib] module,
      and considerable customization is possible.
      [difflib]模块还有一些其他能力，另外可以进行相当可观的
      自定义。

  formatter
      Transform an abstract sequence of formatting events into a
      sequence of callbacks to "writer" objects.  Writer objects,
      in turn, produce concrete outputs based on these callbacks.
      Several parent formatter and writer classes are contained
      in the module.
      将抽象的格式化事件序列转换到一个对“writer”对象的回调函数
      序列。writer对象，接着根据这些回调函数，产生具体输出。
      本模块中包含了若干个父格式程序以及writer类。

      In a way, [formatter] is an "anti-parser"--that is, while a
      parser transforms a series of tokens into program events,
      [formatter] transforms a series of program events into
      output tokens.
      从某种程度上，[formatter]是一个“反解析器”--就是说，
      解析器将一系列的特征转换成程序事件，[formatter]将
      一系列的程序时间转换成特征输出。

      The purpose of the [formatter] module is to structure
      creation of streams such as word processor file formats.
      The module `htmllib` utilizes the [formatter] module.  The
      particular API details provide calls related to features
      like fonts, margins, and so on.
      [formatter]模块的目标是结构化创建流，例如字处理器的文件格式。
      `htmllib`模块使用了[formatter]模块。一些特别的API提供的
      调用和特性有关，例如字体、页边空白等等。

      For highly structured output of prose-oriented documents,
      the [formatter] module is useful, albeit requiring learning
      a fairly complicated API.  At the minimal level, you may
      use the classes included to create simple tools.  For
      example, the following utility is approximately equivalent
      to 'lynx -dump':
      对于散文类型的文档，为了得到高度结构化的输出，[formatter]
      模块是很有用的，就是需要学习相当复杂的API。最少程度，
      你可以使用内置的类来创建简单的工具。例如，下面的工具
      和‘lynx -dump’差不多：

      #----------------------- urldump.py ----------------------#
      #!/usr/bin/env python
      import sys
      from urllib import urlopen
      from htmllib import HTMLParser
      from formatter import AbstractFormatter, DumbWriter
      if len(sys.argv) > 1:
          fpin = urlopen(sys.argv[1])
          parser = HTMLParser(AbstractFormatter(DumbWriter()))
          parser.feed(fpin.read())
          print '----------------------------------------------------'
          print fpin.geturl()
          print fpin.info()
      else:
          print "No specified URL"

      SEE ALSO, `htmllib`, [urllib]
      参见 `htmllib`、[urllib]

  htmllib
      Parse and process HTML files, using the services of
      [sgmllib].  In contrast to the [HTMLParser] module,
      [htmllib] relies on the user constructing a suitable
      "formatter" object to accept callbacks from HTML events,
      usually utilizing the [formatter] module.  A formatter, in
      turn, uses a "writer" (also usually based on the
      [formatter] module).  In my opinion, there are enough
      layers of indirection in the [htmllib] API to make
      [HTMLParser] preferable for almost all tasks.
      使用[sgmllib]服务来解析和处理HTML文件。和[HTMLParser]
      模块不同，[htmllib]依赖于用户建造一个合适的“formatter”
      对象，以接受从HTML事件的回调。它通常会用到[formatter]
      模块。formatter接着就使用“writer”（这个通常也是基于
      [formatter]模块的）。我的个人看法是，[htmllib]API中
      有太多层的不直接，导致偏爱用[HTMLParser]来处理几乎
      所有事情。

      SEE ALSO, [HTMLParser], `formatter`, `sgmllib`
      参见[HTMLParser]、 `formatter`、`sgmllib`

  multifile
      The class `multifile.MultiFile` allows you to treat a text
      file composed of multiple delimited parts as if it were
      several files, each with their own FILE methods: '.read()',
      '.readline()', '.readlines()', '.seek()', and '.tell()'
      methods.  In iterator fashion, advancing to the next
      virtual file is performed with the method
      `multifile.MultiFile.next()`.
      类`multifile.MultiFile`允许你把由多个分隔开部分组合的
      文本文件，作为多个单独的文件处理，它们都有自己
      单独的FILE方法：'.read()'、
      '.readline()'、'.readlines()'、 '.seek()'以及'.tell()'方法。
      根据流行的迭代子风格，移动到下一个虚拟文件时，
      可以使用方法`multifile.MultiFile.next()`。

      SEE ALSO, [fileinput], [mailbox], [email.Parser],
                `string.split()`, [file]
      参见, [fileinput], [mailbox], [email.Parser],
                `string.split()`, [file]

  parser
  symbol
  token
  tokenize
      Interface to Python's internal parser and tokenizer.
      Although parsing Python source code is arguably a text
      processing task, the complexities of parsing Python are too
      specialized for this book.
      Python内部解析器和特征标示器的接口。尽管解析Python
      源代码可以证明为一个文字处理任务，解析Python的复杂度
      对于本书而言，过于专业。

  robotparser
      Examine a 'robots.txt' access control file.  This file is
      used by Web servers to indicate the desired behavior of
      automatic indexers and Web crawlers--all the popular search
      engines honor these requests.
      检查'robots.txt'存取控制文件。Web服务器使用此文件来表明
      对自动索引器和网络爬虫的行为的要求--所有流行的搜索引擎都
      尊重这些要求。

  sgmllib
      A partial parser for SGML.  Standard Generalized Markup
      Language (SGML) is an enormously complex document standard;
      in its full generality, SGML cannot be considered a
      -format-, but rather a grammar for describing concrete
      formats.  HTML is one particular SGML dialect, and XML is
      (almost) a simplified subset of SGML.
      SGML的部分解析器。标准通用置标语言（SGML）是一个极其
      复杂的文档标志；就其通用性而言，SGML并不能算作一种-格式-，
      而要看作一种描述具体格式的语法。HTML是一种特别的SGML方言，
      而XML（几乎）是SGML的一种简化的子集。

      Although it might be nice to have a Python library that
      handled generic SGML, [sgmllib] is not such a thing.
      Instead, [sgmllib] implements just enough SGML parsing to
      support HTML parsing with [htmllib].  You might be able to
      coax parsing an XML library out of [sgmllib], with some
      work, but Python's standard XML tools are far more refined
      for this purpose.
      尽管能有一个Python库来处理普通SGML是一件美好的事情，
      但是[sgmllib]并不有志于此。相反，[sgmllib]实现只是刚好能
      解析SGML，以支持[htmllib]库的HTML解析。你可以经过
      一些工作之后，从[sgmllib]中解析一个XML库出来，但是
      Python的标准XML工具更加胜任这个工作。

      SEE ALSO, `htmllib`, `xml.sax`
      参见，`htmllib`, `xml.sax`

  shlex
      A lexical analyzer class for simple Unix shell-like
      syntaxes.  This capability is primarily useful to implement
      small command language within Python applications.
      针对简单Unix 类Shell语法的词法分析类。它的主要用途是
      在Python程序中实现缩小版的命令行语言。

  tabnanny
      This module is generally used as a command-line script
      rather than imported into other applications.  The
      module/script [tabnanny] checks Python source code files
      for mixed use of tabs and spaces within the same block.
      Behind the scenes, the Python source is fully tokenized,
      but normal usage consists of something like:
      本模块通常是作为命令行脚本使用，而非导入到其他程序。
      模块/脚本 [tabnanny] 检查Python源代码文件，以发现
      同一个代码块中对制表符（tab）和空格的混合使用。
      Python代码是全部标志化的，但是通常的用法包含一些
      事情如下：

      #*----------- typical tabnanny utility usage -------------#
      #*----------- 典型tabnanny 用法 ---------------#
      % /sw/lib/python2.2/tabnanny.py SCRIPTS/
      SCRIPTS/cmdline.py 165 '\treturn 1\r\n'
      'SCRIPTS/HTMLParser_stack.py': Token Error: ('EOF in
                                      multi-line string', (3, 7))
      SCRIPTS/outputters.py 18 '\tself.writer=writer\r\n'
      SCRIPTS/txt2bookU.py 148 '\ttry:\n'

      The tool is single purpose, but that purpose addresses a
      common pitfall in Python programming.
      本工具只有一个用途，但此用途却说明了Python编程中
      的一个常见陷阱。

      SEE ALSO, `tokenize`
      参见, `tokenize`


  TOPIC -- Low-Level State Machine Parsing
  主题 -- 低级状态机
  --------------------------------------------------------------------

  =================================================================
    MODULE -- mx.TextTools : Fast Text Manipulation Tools
    模块 -- mx.TextTools : 文本快速操作工具
  =================================================================

  Marc-Andre Lemburg's [mx.TextTools] is a remarkable tool that is
  a bit difficult to grasp the gestalt of. [mx.TextTools] can be
  blazingly fast and extremely powerful. But at the same time, as
  difficult as it might be to "get" the mindset of [mx.TextTools],
  it is still more difficult to get an application written with it
  working just right. Once it is working, an application that
  utilizes [mx.TextTools] can process a larger class of text
  structures than can regular expressions, while simultaneously
  operating much faster. But debugging an [mx.TextTools] "tag
  table" can make you wish you were merely debugging a cryptic
  regular expression!
  Marc-Andre Lemburg的[mx.TextTools]是一个值得注意的工具，
  要掌握它的全部形态，有点小小的困难。[mx.TextTools]快得让人眩目，
  而且功能强大。不过与此同时，要“掌握”[mx.TextTools]的思路会比较
  困难，而要用其编写一个能工作的程序，则更加困难。一旦可以工作，
  该程序就可以使用[mx.TextTools]来处理比正则表达式更广泛的文本结构，
  而且速度相当快。只是，调试[mx.TextTools]中的“标签表格（tag table）”
  会让你痛苦得甚至希望调试加密的正则表达式。
  

  In recent versions, [mx.TextTools] has come in a larger package
  with eGenix.com's several other "mx Extensions for Python." Most
  of the other subpackages add highly efficient C implementations
  of datatypes not found in a base Python system.
  在最近的版本中，[mx.TextTools]附带了更大的包，其中有eGenix.com的
  其他几个“针对Python的mx扩展”。绝大部分的子包添加了一些使用
  C语言实现的高效数据类型，在Python基本系统中是没有的。
  

  [mx.TextTools] stands somewhere between a state machine and a
  full-fledged parser. In fact, the module [SimpleParse], discussed
  below, is an EBNF parser library that is built on top of
  [mx.TextTools]. As a state machine, [mx.TextTools] feels like a
  lower-level tool than the [statemachine] module presented in the
  prior section. And yet, [mx.TextTools] is simultaneously very
  close to a high-level parser. This is how Lemburg characterizes
  it in the documentation accompanying [mx.TextTools]:
  [mx.TextTools]的功能位于状态机和全功能解析器之间。事实上，
  下面讨论的[SimpleParse]模块，就是一个建于[mx.TextTools]之上
  的EBNF解析器库。作为一个状态机，[mx.TextTools]感觉就像
  比前述[statemachine]模块更底层的工具。然而[mx.TextTools]
  同时还和高层的解析器比较相近。这是Lemburg在[mx.TextTools]
  的文档中对其的刻画：

    mxTextTools is an extension package for Python that provides
    several useful functions and types that implement
    high-performance text manipulation and searching algorithms
    in addition to a very flexible and extendable state machine,
    the Tagging Engine, that allows scanning and processing text
    based on low-level byte-code "programs" written using Python
    tuples. It gives you access to the speed of C without the
    need to do any compile and link steps every time you change
    the parsing description.
    mxTextTools是一个Python的扩展包，它提供了若干个有用的函数和类型，
    实现了高性能的文本操作和搜索算法，以及一个极灵活的可扩展的状态机，
    也就是“标签引擎”，后者允许使用基于Python元组编写的底层字节码
    来扫描和处理文本。你可以以C的速度进行存取，而不需要在每次
    改变解析描述后，进行编译和连接步骤。

    Applications include parsing structured text, finding and
    extracting text (either exact or using translation tables)
    and recombining strings to form new text.
    程序包括解析结构文本，寻找和提取文本（提取或者使用转换表），
    以及重新结合字符串，以形成新的文本。

  The Python standard library has a good set of text processing
  tools. The basic tools are powerful, flexible, and easy to work
  with. But Python's basic text processing is -not- particularly
  fast. Mind you, for most problems, Python by itself is as fast as
  you need. But for a certain class of problems, being able to
  choose [mx.TextTools] is invaluable.
  Python标准库有一整套文本处理工具。基础工具强大，灵活，很
  容易使用。但是Python的基础文本处理并-不是-特别快。提醒一下，
  对于大部分问题，Python本身是足够快，可以满足你的要求。
  但是对于某些特定类的问题，能够使用[mx.TextTools]将是无法估值的。

  The unusual structure of [mx.TextTools] applications warrants
  some discussion of concrete usage. After a few sample
  applications are presented, a listing of [mx.TextTools]
  constants, commands, modifiers, and functions is given.
  [mx.TextTools]程序的不同寻常结构确保了具体用途的讨论。
  在给出一些例子程序之后，还将给出一个关于[mx.TextTools]中
  常数，命令，修饰语，以及函数的列表。

  BENCHMARKS:
  基准：

  A familiar computer-industry paraphrase of Mark Twain (who
  repeats Benjamin Disraeli) dictates that there are "Lies, Damn
  Lies, and Benchmarks." I will not argue with that and certainly
  do not want readers to put too great an import on the timings
  suggested. Nonetheless, in exploring [mx.TextTools], I wanted to
  get some sense of just how fast it is. So here is a rough idea.
  马克吐温（Mark Twain, 他重复了Benjamin Disraeli）
  关于计算机工业的一个熟悉的解释是“谎言，该死的谎言，以及基准”。
  我不想讨论这个，当然也不希望读者给予建议的时间太高的重要性。
  尽管如此，在浏览[mx.TextTools]时，我希望对它的速度获得一些认识。
  下面是一个粗略的想法：

  The second example below presents part of a reworked version of
  the state machine-based 'Txt2Html' application reproduced in
  Appendix D. The most time-consuming aspect of 'Txt2Html' is the
  regular expression replacements performed in the function
  'Typography()' for smart ASCII inline markup of words and
  phrases.
  下述第二个例子是附录D中基于状态机的‘Txt2Html’程序的重写版本。
  ‘Txt2Html’中最耗时间的是‘Typography’函数中的执行的
  正则表达式替换，它们是针对智能ASCII中单词和短语的行内标记的。

  In order to get a timeable test case, I concatenated 110 copies
  of an article I wrote to get a file a bit over 2MB, and about
  41k lines and 300k words. My test processes an entire input as
  one text block, first using an [mx.TextTools] version of
  'Typography()', then using the [re] version.
  为了获得一个可计时测试案例，我把一个文章反复拼接了110遍，
  获得一个超过2MB的文件，大概有4.1万行，30万单词。我的测试
  将整个输入作为一个文本块，首先使用[mx.TextTools]版本的
  ‘Typography（）’，然后使用[re]版本。

  Processing time of the same test file went from about 34
  seconds to about 12 seconds on one slowish Linux test machine
  (running Python 1.5.2). In other words, [mx.TextTools] gave me
  about a 3x speedup over what I get with the [re] module.
  This speedup is probably typical, but particular applications
  might gain significantly more or less from use of
  [mx.TextTools].  Moreover, 34 seconds is a long time in an
  interactive application, but is not very long at all for a
  batch process done once a day, or once a week.
  在一个低配置的Linux测试机器上（运行Python 1.5.2）
  对同一个测试文件的处理时间范围是从大约34秒到12秒。
  换句话说，[mx.TextTools]的速度是[re]模块的三倍。
  这类加速都是很典型的，但是特定的程序使用[mx.TextTools]获得的加速
  可能会明显地或多或少。而且，34秒对于一个交互式程序来说是一个很长
  的时间，但是对于一个每天甚至每周一次的批处理程序而言，根本不长。
  

  EXAMPLE: Buyer/Order Report Parsing
  例子：买家/订单报告解析
  --------------------------------------------------------------------

  Recall (or refer to) the sample report presented in the previous
  section "An Introduction to State Machines." A report contained a
  mixture of header material, buyer orders, and comments. The state
  machine we used looked at each successive line of the file and
  decided based on context whether the new line indicated a new
  state should start. It would be possible to write almost the same
  algorithm utilizing [mx.TextTools] only to speed up the
  decisions, but that is not what we will do.
  回忆（或者回顾）一下前面章节“关于状态机的介绍”中给出的例子报告，
  它其中混合了文件头材料，买家订单，以及注释。我们使用的状态机
  连续地检查该文件的每一行，根据上下文以决定新行是否表示一个新状态
  的开始。使用[mx.TextTools]来编写与其几乎完全一样的算法完全是可能的，
  它可以加速作出其中的决定，但这个不是我们将要做的。

  A more representative use of [mx.TextTools] is to produce a
  concrete parse tree of the interesting components of the report
  document.  In principle, you should be able to create a
  "grammar" that describes every valid "buyer report" document,
  but in practice using a mixed procedural/grammar approach is
  much easier, and more maintainable--at least for the test
  report.
  [mx.TextTools]的一个更有代表性的用途是为报告文档中感兴趣的成分
  生成具体解析树。原则上，你应该能创建一个“语法”，它描述了
  每个有效的“买家报告”文档，但在实际运用中，使用一个混合程序/语法
  的方法会更加容易，也更容易维护--至少对于测试报告而言。

  An [mx.TextTools] tag table is a miniature state machine that
  either matches or fails to match a portion of a string.
  Matching, in this context, means that a "success" end state is
  reached, while nonmatching means that a "failure" end state is
  reached.  Falling off the end of the tag table is a success
  state.  Each individual state in a tag table tries to match
  some smaller construct by reading from the "read-head," and
  moving the read-head correspondingly.  On either success or
  failure, program flow jumps to an indicated target state (which
  might be a success or failure state for the tag table as a
  whole).  Of course, the jump target for success is often
  different from the jump target for failure--but there are only
  these two possible choices for jump targets, unlike the
  [statemachine] module's indefinite number.
  [mx.TextTools]标签表是一个缩小版的状态机，它尝试匹配字符串的一部分，
  或者成功，或者失败。
  此处所说的成功匹配，表示到达一个“成功”结束状态，而失败匹配则表示
  到达一个“失败”结束状态。离开标签表的尾部就是一个成功状态。
  标签表中的每个单独状态试图通过读取“read-head”以便相应地移动
  read-head，以匹配一些更小的结构。不管成功还是失败，程序流跳转到
  显示的一个目标状态，后者对于整个标签表来说可以是个成功或者失败状态。
  当然，成功的跳转程序通常和失败的跳转程序是不同的--但是对于
  跳转状态而言只有这两种可能选择，而在[statemachine]模块中是无限的。

  Notably, one of the types of states you can include in a tag
  table is another tag table.  That one state can "externally"
  look like a simple match attempt, but internally it might
  involve complex subpatterns and machine flow in order to
  determine if the state is a match or nonmatch.  Much as in an
  EBNF grammar, you can build nested constructs for recognition
  of complex patterns.  States can also have special behavior,
  such as function callbacks--but in general, an [mx.TextTools]
  tag table state is simply a binary match/nonmatch switch.
  特别地，在一个标签表中可以包括另外一个标签表以作为一个状态。
  这个状态“从外部”看可以像一个简单的匹配尝试，但是它内部可能有
  复杂的子模式和机器流，以确定此状态是一个匹配还是非匹配。
  和EBNF语法很相似，你可以创建嵌套的结构以识别复杂模式。
  状态也可以拥有特别的行为，例如回调函数--但是通常来说，
  [mx.TextTools]标签表只是一个二相匹配/非匹配开关。

  Let us look at an [mx.TextTools] parsing application for "buyer
  reports," and then examine how it works:
  让我们看看针对“买家报告”的[mx.TextTools]解析程序，然后解释
  它是如何工作的：

      #------------------- buyer_report.py --------------------#
      from mx.TextTools import *

      word_set = set(alphanumeric+white+'-')
      quant_set = set(number+'kKmM')

      item   = ( (None, AllInSet, newline_set, +1),               # 1
                 (None, AllInSet, white_set, +1),                 # 2
                 ('Prod', AllInSet, a2z_set, Fail),               # 3
                 (None, AllInSet, white_set, Fail),               # 4
                 ('Quant', AllInSet, quant_set, Fail),            # 5
                 (None, WordEnd, '\n', -5) )                      # 6

      buyers = ( ('Order', Table,                                 # 1
                        ( (None, WordEnd, '\n>> ', Fail),         # 1.1
                          ('Buyer', AllInSet, word_set, Fail),    # 1.2
                          ('Item', Table, item, MatchOk, +0) ),   # 1.3
                        Fail, +0), )

      comments = ( ('Comment', Table,                             # 1
                        ( (None, Word, '\n*', Fail),              # 1.1
                          (None, WordEnd, '*\n', Fail),           # 1.2
                          (None, Skip, -1) ),                     # 1.3
                        +1, +2),
                   (None, Skip, +1),                              # 2
                   (None, EOF, Here, -2) )                        # 3

      def unclaimed_ranges(tagtuple):
          starts = [0] + [tup[2] for tup in tagtuple[1]]
          stops = [tup[1] for tup in tagtuple[1]] + [tagtuple[2]]
          return zip(starts, stops)

      def report2data(s):
          comtuple = tag(s, comments)
          taglist = comtuple[1]
          for beg,end in unclaimed_ranges(comtuple):
              taglist.extend(tag(s, buyers, beg, end)[1])
          taglist.sort(cmp)
          return taglist

      if __name__=='__main__':
          import sys, pprint
          pprint.pprint(report2data(sys.stdin.read()))

  Several tag tables are defined in [buyer_report]: 'item',
  'buyers', and 'comments'. State machines such as those in each
  tag table are general matching engines that can be used to
  identify patterns; after working with [mx.TextTools] for a while,
  you might accumulate a library of useful tag tables. As mentioned
  above, states in tag tables can reference other tag tables,
  either by name or inline. For example, 'buyers' contains an
  inline tag table, while this inline tag table utilizes the tag
  table named 'item'.
  在[buyer_report]中定义了若干个标签表：‘item’，‘buyers’以及‘comments’。
  每个标签表中的状态机都是普通匹配引擎，它们被用来识别模式；当你
  用了一阵[mx.TextTools]之后，你可能已经积累了一个有用标签表的库。
  如上面提到的，标签表中的状态可以引用其他标签表，可以使用名字，
  也可以在行内引用。例如，‘buyers’包含了一个行内标签表，这里的行内
  标签表使用了名为‘item’的标签表。

  Let us take a look, step by step, at what the 'buyers' tag
  table does.  In order to -do- anything, a tag table needs to be
  passed as an argument to the 'mx.TextTools.tag()' function,
  along with a string to match against.  That is done in the
  'report2data()' function in the example.  But in general,
  'buyers'--or any tag table--contains a list of states, each
  containing branch offsets.  In the example, all such states are
  numbered in comments.  'buyers' in particular contains just
  one state, which contains a subtable with three states.
  让我们一步一步地看一下‘buyers’标签表做的工作。为了能够-做-任何事，
  标签表以及用于匹配的字符串需要作为参数传递给‘mx.TextTools.tag()’函数。
  在例子中，这个是在函数‘report2data()’中完成的。但是通常来说，‘buyers’
  --或者说任何标签表--包含了一个状态列表，后者每个都包含了分支位移。
  在本例中，所有此类状态都在注释中编了号。特别的是‘buyers’状态，
  它只有一个状态，但后者包含了一个有三个状态的子表。

  +++

  *Tag table state in 'buyers' *
  *‘buyers’中的标签表状态  *

  1.  Try to match the subtable.  If the match succeeds, add the
      name 'Order' to the taglist of matches.  If the match
      fails, do not add anything.  If the match succeeds, jump
      back into the one state (i.e., +0).  In effect, 'buyers'
      loops as long as it succeeds, advancing the read-head on
      each such match.
	  试图匹配子表。如果匹配成功，在匹配的标签列表中添加名字‘Order’。
	  如果失败，不做任何事情。如果匹配成功，跳回一个状态（也就是 +0 ）。
	  本质上，只要‘buyers’成功匹配，就是一直循环下去，并在每一个成功
	  的匹配时，在read-head中推进。

  *Subtable states in 'buyers' *
  *‘buyers’中的子表状态 *

  1.  Try to find the end of the "word" '\n>>' in the string.
      That is, look for two greater-than symbols at the beginning
      of a line.  If successful, move the read-head just past the
      point that first matched.  If this state match fails, jump
      to 'Fail'--that is, the (sub)table as a whole fails to
      match. No jump target is given for a successful match, so
      the default jump of +1 is taken.  Since 'None' is the
      tag object, do not add anything to the taglist upon a
      state match.
	  试图在字符串中发现“word”的结尾‘\n>>’。也就是说，
	  在行首寻找两个大于号。如果成功，将read-head移过第一次匹配的点。
	  如果状态匹配失败了，跳转至‘Fail’--准确地说，
	  整个（子）表无法匹配的时候。对于成功匹配，不会给出跳转目标，
	  所以采用了默认的跳转+1。因为‘None’是标签目标，所以在状态匹配
	  时不需要在标签列表中添加任何东西。

  2.  Try to find some 'word_set' characters.  This set of
      characters is defined in [buyer_report]; various other sets
      are defined in [mx.TextTools] itself.  If the match
      succeeds, add the name 'Buyer' to the taglist of matches.
      As many contiguous characters in the set as possible are
      matched.  The match is considered a failure if there is not
      at least one such character.  If this state match fails,
      jump to 'Fail', as in state (1).
	  试图找到一些‘word_set’字符。[buyer_report]中定义了这套字符；
	  [mx.TextTools]本身还定义了一些不同的集合。如果匹配成功，将把
	  ‘Buyer’名字添加到匹配的标签列表中。尽可能多的匹配邻近的字符。
	  如果没有找到一个字符，此匹配就被认为失败了。如果此状态匹配失败，
	  则跳转至‘Fail’，就和状态（1）中一样。

  3.  Try to match the 'item' tag table.  If the match succeeds,
      add the name 'Item' to the taglist of matches.  What gets
      added, moreover, includes anything added within the 'item'
      tag table.  If the match fails, jump to 'MatchOk'--that
      is, the (sub)table as a whole matches.  If the match
      succeeds, jump +0--that is, keep looking for another
      'Item' to add to the taglist.
	  试图匹配‘item’标签表。如果匹配成功，将名字‘Item’添加到匹配的
	  标签列表中。此外，添加的内容还包括任何已经添加于‘item’标签表
	  中的内容。如果匹配失败，跳转至‘MatchOk’--就是说，此（子）表
	  被整体匹配了。如果匹配成功，跳转+0 -- 就是说，继续寻找下一个
	  ‘Item’以添加至标签表。

  What [buyer_report] actually does is to first identify any
  comments, then to scan what is left in between comments for buyer
  orders. This approach proved easier to understand. Moreover, the
  design of [mx.TextTools] allows us to do this with no real
  inefficiency. Tagging a string does not involve actually pulling
  out the slices that match patterns, but simply identifying
  numerically the offset ranges where they occur. This approach is
  much "cheaper" than performing repeated slices, or otherwise
  creating new strings.
  [buyer_report]实际的工作就是，首先识别所有注释，然后扫描注释间
  剩余的内容以寻找买家订单。这种方式证明为比较容易理解。而且，
  [mx.TextTools]的设计使得我们这样做并没有任何实际的无效率。
  对一个字符串贴标签，并不实际将匹配模式的切片提取出来，只是
  数值上定位匹配位置的位移范围。这个方法比实行重复切片，或者创建
  新字符串要“便宜”得多。

  The following is important to notice: As of version 2.1.0, the
  documentation of the 'mx.TextTools.tag()' function that
  accompanies [mx.TextTools] does not match its behavior! If the
  optional third and fourth arguments are passed to 'tag()' they
  must indicate the start and end offsets within a larger string to
  scan, -not- the starting offset and length. Hopefully, later
  versions will fix the discrepancy (either approach would be fine,
  but could cause some breakage in existing code).
  下面是要注意的重要内容：在版本2.1.0中，函数‘mx.TextTools.tag()’的
  与其行为并不相符！如果可选的第三和第四参数被传递给‘tag()’，它们
  必须表示一个更大字符串中需扫描片段的开始和结尾位移，-而非-开始位移
  和长度。后来的版本估计会修复这个矛盾（随便那个方法都可以，
  但是可能造成现有代码无法工作）。
  
  What [buyer_report] produces is a data structure, not final
  output. This data structure looks something like:
  [buyer_report]产生的是一个数据结构，并非最终输出。此数据结构看上去像：

      #------------ buyer_report.py data structure ------------#
      #------------ buyer_report.py 数据结构 ------------#
      $ python ex_mx.py < recs.tmp
      [('Order', 0,  638,
        [('Buyer', 547, 562, None),
         ('Item', 562, 583,
          [('Prod', 566, 573, None), ('Quant', 579, 582, None)]),
         ('Item', 583, 602,
           [('Prod', 585, 593, None), ('Quant', 597, 601, None)]),
         ('Item', 602, 621,
           [('Prod', 604, 611, None), ('Quant', 616, 620, None)]),
         ('Item', 621, 638,
           [('Prod', 623, 632, None), ('Quant', 635, 637, None)])]),
       ('Comment', 638, 763, []),
       ('Order', 763, 805,
        [('Buyer', 768, 776, None),
         ('Item', 776, 792,
          [('Prod', 778, 785, None), ('Quant', 788, 791, None)]),
         ('Item', 792, 805,
          [('Prod', 792, 800, None), ('Quant', 802, 804, None)])]),
       ('Order', 805, 893,
        [('Buyer', 809, 829, None),
         ('Item', 829, 852,
          [('Prod', 833, 840, None), ('Quant', 848, 851, None)]),
         ('Item', 852, 871,
          [('Prod', 855, 863, None), ('Quant', 869, 870, None)]),
         ('Item', 871, 893,
          [('Prod', 874, 879, None), ('Quant', 888, 892, None)])]),
       ('Comment', 893, 952, []),
       ('Comment', 952, 1025, []),
       ('Comment', 1026, 1049, []),
       ('Order', 1049, 1109,
        [('Buyer', 1054, 1069, None),
         ('Item',1069, 1109,
          [('Prod', 1070, 1077, None), ('Quant', 1083, 1086, None)])])]

  While this is "just" a new data structure, it is quite easy to
  deal with compared to raw textual reports.  For example, here
  is a brief function that will create well-formed XML out of any
  taglist.  You could even arrange for it to be valid XML by
  designing tag tables to match DTDs (see Chapter 5 for details
  about XML, DTDs, etc.):
  尽管这“仅仅”是新的数据结构，与原始文本报告比较，前者更易处理。
  例如，下面是一个简要的程序，用来创建标签列表的良好XML输出。
  你甚至可以使它成为有效的XML，如果你设计标签列表以匹配DTD
  （参见第五章以获取XML，DTD等的细节）：

      #*------------ taglist2xml() function -------------------#
      def taglist2xml(s, taglist, root):
          print '<%s>' % root
          for tt in taglist:
              if tt[3]:
                  taglist2xml(s, tt[3], tt[0])
              else:
                  print '<%s>%s</%s>' % (tt[0], s[tt[1]:tt[2]], tt[0])
          print '</%s>' % root


  EXAMPLE: Marking up smart ASCII
  例子：标记智能ASCII
  --------------------------------------------------------------------

  The "smart ASCII" format uses email-like conventions to lightly
  mark features like word emphasis, source code, and URL links.
  This format--with LaTeX as an intermediate format--was used to
  produce the book you hold (which was written using a variety of
  plaintext editors). By obeying just a few conventions (that are
  almost the same as you would use on Usenet or in email), a writer
  can write without much clutter, but still convert to
  production-ready markup.
  “智能ASCII”格式使用了类电子邮件惯例，可以轻松标识某些特性，例如
  单词强调，源代码，以及URL链接。这种格式--使用LaTeX作为中间格式--
  被用来产生你手中的这本书（它是用多个普通文本编辑器写就）。
  只要遵守一些约定（这些约定和你在Usenet或者电子邮件中使用的几乎一样），
  作家就可以毫无障碍地写作，但仍可转换成产品准备完毕的标识。
## production-ready

  The 'Txt2Html' utility uses a block-level state machine, combined
  with a collection of inline-level regular expressions, to
  identify and modify markup patterns in smart ASCII texts. Even
  though Python's regular expression engine is moderately slow,
  converting a five-page article takes only a couple seconds. In
  practice, 'Txt2Html' is more than adequate for my own 20 kilobyte
  documents. However, it is easy to imagine a not-so-different
  situation where you were converting multimegabyte documents
  and/or delivering such dynamically converted content on a
  high-volume Web site. In such a case, Python's string operations,
  and especially regular expressions, would simply be too slow.
  ‘Txt2Html’功能使用一个块级的状态机，与一些行内级的正则表达式组合起来，
  以识别和修改智能ASCII文本中的表示模式。尽管Python的正则表达式引擎
  有点慢，转换一个五页的文章只需要几秒钟。在实际运用中，‘Txt2Html’
  对于我自己的20kB文档而言，是绝对足够了。但是，我们可以很容易设想
  这样一种并非太困难的处境，你正转换几兆的文档，并且/或者将这些
  动态转换的内容投递到一个高容量的网站。在此情形下，Python的
  字符串处理，特别是正则表达式，将会显得太慢了。

  [mx.TextTools] can do everything regular expressions can, plus
  some things regular expressions cannot. In particular, a taglist
  can contain recursive references to matched patterns, which
  regular expressions cannot. The utility 'mxTypography.py'
  utilizes several [mx.TextTools] capabilities the prior example
  did not use. Rather than create a nested data structure,
  'mxTypography.py' utilizes a number of callback functions, each
  responding to a particular match event. As well,
  'mxTypography.py' adds some important debugging techniques.
  Something similar to these techniques is almost required for tag
  tables that are likely to be updated over time (or simply to aid
  the initial development). Overall, this looks like a robust
  application should.
  [mx.TextTools]可以做任何正则表达式能做的事情，以及一些正则表达式
  不能做的事情。特别来说，标签列表能包含对匹配到的模式的递归引用，
  这可是正则表达式无法做到的。‘mxTypography.py’工具使用了若干个
  [mx.TextTools]的能力，而前面的例子尚未使用到它们。
  ‘mxTypography.py’并没有创建一个嵌套的数据结构，而是使用了许多
  回调函数，每个相应于某个特定的匹配事件。‘mxTypography.py’还
  添加了一些重要的调试技术。标签表也需要这样类似的技术，以随着
  时间的推移而更新标签表。总体而言，这个看上去就像一个健壮的程序
  应该看上去的样子。

      #------- mx.TextTools version of Typography() -----------#
      #------- mx.TextTools 版本的 Typography() -----------#
      from mx.TextTools import *
      import string, sys

      #-- List of all words with  markup, head position, loop count
      #-- 关于带有标记的所有单词，头位置，循环计数的列表
      ws, head_pos, loops = [], None, 0

      #-- Define "emitter" callbacks for each output format
      #-- 为每个输出格式定义“emitter”回调函数
      def emit_misc(tl,txt,l,r,s):
          ws.append(txt[l:r])
      def emit_func(tl,txt,l,r,s):
          ws.append('<code>'+txt[l+1:r-1]+'</code>')
      def emit_modl(tl,txt,l,r,s):
          ws.append('<em><code>'+txt[l+1:r-1]+'</code></em>')
      def emit_emph(tl,txt,l,r,s):
          ws.append('<em>'+txt[l+1:r-1]+'</em>')
      def emit_strg(tl,txt,l,r,s):
          ws.append('<strong>'+txt[l+1:r-1]+'</strong>')
      def emit_titl(tl,txt,l,r,s):
          ws.append('<cite>'+txt[l+1:r-1]+'</cite>')
      def jump_count(tl,txt,l,r,s):
          global head_pos, loops
          loops = loops+1
          if head_pos is None: head_pos = r
          elif head_pos == r:
              raise "InfiniteLoopError", \
                    txt[l-20:l]+'{'+txt[l]+'}'+txt[l+1:r+15]
          else: head_pos = r

      #-- What can appear inside, and what can be, markups?
      #-- 什么可以出现在内部，以及什么可以是标识？
      punct_set = set("`!@#$%^&*()_-+=|\{}[]:;'<>,.?/"+'"')
      markable = alphanumeric+whitespace+"`!@#$%^&()+=|\{}:;<>,.?/"+'"'
      markable_func = set(markable+"*-_[]")
      markable_modl = set(markable+"*-_'")
      markable_emph = set(markable+"*_'[]")
      markable_strg = set(markable+"-_'[]")
      markable_titl = set(markable+"*-'[]")
      markup_set    = set("-*'[]_")

      #-- What can precede and follow markup phrases?
      #-- 什么可以出现在标识短语的前后？
      darkins = '(/"'
      leadins = whitespace+darkins      # might add from "-*'[]_"
      darkouts = '/.),:;?!"'
      darkout_set = set(darkouts)
      leadouts = whitespace+darkouts    # for non-conflicting markup
      leadout_set = set(leadouts)

      #-- What can appear inside plain words?
      #-- 什么可以出现在普通单词中？
      word_set = set(alphanumeric+'{}/@#$%^&-_+=|\><'+darkouts)
      wordinit_set = set(alphanumeric+"$#+\<.&{"+darkins)

      #-- Define the word patterns (global so as to do it only at import)
      # Special markup
      #-- 定义单词模式 (全局变量，这样只需在import时运行)
      # 特别的标识
      def markup_struct(lmark, rmark, callback, markables, x_post="-"):
          struct = \
            ( callback, Table+CallTag,
              ( (None, Is, lmark),                 # Starts with left marker     #以左标记开始
                (None, AllInSet, markables),       # Stuff marked                #内容被标识
                (None, Is, rmark),                 # Ends with right marker      #以右标识结束
                (None, IsInSet, leadout_set,+2,+1),# EITHR: postfix w/ leadout   #或者: 后缀 w/ 引导 #TYPO
                (None, Skip, -1,+1, MatchOk),      # ..give back trailng ldout   # .. 恢复后面的ldout
                (None, IsIn, x_post, MatchFail),   # OR: special case postfix    #或者：特殊情况后缀
                (None, Skip, -1,+1, MatchOk)       # ..give back trailing char   # .. 恢复后面的字符
              )
            )
          return struct
      funcs   = markup_struct("'", "'", emit_func, markable_func)
      modules = markup_struct("[", "]", emit_modl, markable_modl)
      emphs   = markup_struct("-", "-", emit_emph, markable_emph, x_post="")
      strongs = markup_struct("*", "*", emit_strg, markable_strg)
      titles  = markup_struct("_", "_", emit_titl, markable_titl)

      # All the stuff not specially marked
      # 所有没有被特别标出的东西
      plain_words = \
       ( ws, Table+AppendMatch,           # AppendMatch only -slightly-      # AppendMatch只是-小小地-
         ( (None, IsInSet,                #  faster than emit_misc callback  # 比emit_misc回调函数快一点
              wordinit_set, MatchFail),   # Must start with word-initial     # 必须以word-initial开始
           (None, Is, "'",+1),            # May have apostrophe next         # 下面可以是省略号
           (None, AllInSet, word_set,+1), # May have more word-internal      # 可以有更多的word-internal
           (None, Is, "'", +2),           # May have trailing apostrophe     # 后面可以有省略号
           (None, IsIn, "st",+1),         # May have [ts] after apostrophe   # 省略号后可以有[ts]
           (None, IsInSet,
              darkout_set,+1, MatchOk),   # Postfixed with dark lead-out     # 后缀是黑色的开始 ???不懂
           (None, IsInSet,
              whitespace_set, MatchFail), # Give back trailing whitespace    # 恢复后面的空白
           (None, Skip, -1)
         ) )
      # Catch some special cases
      # 抓获一些特别的情形
      bullet_point = \
       ( ws, Table+AppendMatch,
         ( (None, Word+CallTag, "* "),       # Asterisk bullet is a word      # 星号是个单词 ???
         ) )
      horiz_rule = \
       ( None, Table,
         ( (None, Word, "-"*50),             # 50 dashes in a row             # 一行50个破折号
           (None, AllIn, "-"),               # More dashes                    # 更多破折号
         ) )
      into_mark = \
       ( ws, Table+AppendMatch,             # Special case where dark leadin  # 特殊情况，此处dark leadin ???
         ( (None, IsInSet, set(darkins)),   #   is followed by markup char    # 后面跟的是标识字符
           (None, IsInSet, markup_set),
           (None, Skip, -1)                 # Give back the markup char       # 恢复标识字符
         ) )
      stray_punct = \
       ( ws, Table+AppendMatch,              # Pickup any cases where multiple # 获取任何案例，如果多个
         ( (None, IsInSet, punct_set),       # punctuation character occur     # 标点字符单独出现
           (None, AllInSet, punct_set),      # alone (followed by whitespace)  # （后面跟着空白）
           (None, IsInSet, whitespace_set),
           (None, Skip, -1)                  # Give back the whitespace        # 恢复空白
         ) )
      leadout_eater = (ws, AllInSet+AppendMatch, leadout_set)

      #-- Tag all the (possibly marked-up) words
      tag_words = \
       ( bullet_point+(+1,),
         horiz_rule + (+1,),
         into_mark  + (+1,),
         stray_punct+ (+1,),
         emphs   + (+1,),
         funcs   + (+1,),
         strongs + (+1,),
         modules + (+1,),
         titles  + (+1,),
         into_mark+(+1,),
         plain_words +(+1,),             # Since file is mstly plain wrds, can  # 因为文件几乎都是普通单词，
         leadout_eater+(+1,-1),          # shortcut by tight looping (w/ esc)   # 可用紧循环（w/ esc）来缩短
         (jump_count, Skip+CallTag, 0),  # Check for infinite loop              # 检查无限循环
         (None, EOF, Here, -13)          # Check for EOF                        # 检查 EOF
       )
      def Typography(txt):
          global ws
          ws = []    # clear the list before we proceed  # 在我们继续之前，清空此列表
          tag(txt, tag_words, 0, len(txt), ws)
          return string.join(ws, '')

      if __name__ == '__main__':
          print Typography(open(sys.argv[1]).read())

  +++

  'mxTypographify.py' reads through a string and determines if the
  next bit of text matches one of the markup patterns in
  'tag_words'. Or rather, it better match some pattern or the
  application just will not know what action to take for the next
  bit of text. Whenever a named subtable matches, a callback
  function is called, which leads to a properly annotated string
  being appended to the global list 'ws'. In the end, all such
  appended strings are concatenated.
  ‘mxTypographify.py’读入一个字符串，并测定文本的下一比特是否匹配
  ‘tag_word’中的某个标识模式。或者确切一点说，最好能匹配某个模式，
  否则程序会不知道对下一个比特采取何种措施。任何时候一个命名子表
  被匹配到，将呼叫一个回调函数，这将导致一个正确评注过的字符串
  添加到全局列表‘ws’后。最后，所有这些附加的字符串被连接起来。

  Several of the patterns given are mostly fallback conditions. For
  example, the 'stray_punct' tag table detects the condition where
  the next bit of text is some punctuation symbols standing alone
  without abutting any words. In most cases, you don't -want- smart
  ASCII to contain such a pattern, but [mxTypographify] has to do
  -something- with them if they are encountered.
  给出的模式中有几个是退却条件。例如，‘stray_punct’标签表用来探测
  下一比特的文本是某些孤立的标点符号，旁边没有任何单词。
  在绝大部分情况下，你不会-希望-智能ASCII会包含这样一个模式，
  但是如果遇到它们，[mxTypographify]必须对它们做些-什么-。

  Making sure that every subsequence is matched by some subtable or
  another is tricky. Here are a few examples of matches and
  failures for the 'stray_punct' subtable. Everything that does not
  match this subtable needs to match some other subtable instead:
  要确保每个子序列都能被某个子表匹配到是比较难搞的事情。
  这儿有一些‘stray_punct’子表匹配成功和失败的例子。
  每个没有匹配到此子表的内容需要匹配到其他子表：

      #*--------- stray_punct successes and failures ----------#
      #*--------- stray_punct 的成功和失败 ----------#
      -- spam      # matches "--"   # 匹配“--”
      & spam       # fails at "AllInSet" since '&' advanced head # 在“AllInSet”处失败， 因为前面有‘&’
      #@$ %% spam  # matches "#@$"              # 匹配“#@$”
      **spam       # fails (whitespace isn't encountered before 's')  # 失败 （‘s’前没有空白）

  After each success, the read-head is at the space right before
  the next word "spam" or "%%". After a failure, the read-head
  remains where it started out (at the beginning of the line).
  每个成功匹配后，read-head就在下个单词“spam”或“%%”前面的空格。
  失败后，read-head保持在它开始的地方（即行首）。

  Like 'stray_punct', 'emphs', 'funcs', 'strongs', and 'plain_words',
  et cetera contain tag tables. Each entry in 'tag_words' has its
  appropriate callback functions (all "emitters" of various names,
  because they "emit" the match, along with surrounding markup if
  needed). Most lines each have a "+1" appended to their tuple;
  what this does is specify where to jump in case of a match
  failure. That is, even if these patterns fail to match, we
  continue on--with the read-head in the same position--to try
  matching against the other patterns.
  ‘stray_punct’，‘emphs’，‘funcs’，‘strongs’和‘plain_words’等等
  都包含标签表。‘tag_words’中每个条目都有其恰当的回调函数（所有
  不同名字的“emitters”，因为他们“emit”匹配，以及周围的标识，
  如果需要的话）。绝大部分行都有一个“+1”附加在它们的元组后面；
  这个的作用是在匹配失败的时候表明跳转的去处。也就是说，
  就算这些模式无法匹配，我们可以继续尝试匹配其他模式，而read-head
  还在同样的位置。

  After the basic word patterns each attempt a match, we get to the
  "leadout eater" line. For 'mxTypography.py', a "leadout" is the
  opposite of a "leadin." That is, the latter are things that might
  precede a word pattern, and the former are things that might
  follow a word pattern. The 'leadout_set' includes whitespace
  characters, but it also includes things like a comma, period,
  and question mark, which might end a word. The "leadout eater" uses
  a callback function, too. As designed, it preserves exactly the
  whitespace the input has. However, it would be easy to normalize
  whitespace here by emitting something other than the actual match
  (e.g., a single space always).
  在每个基础单词模式尝试了一个匹配之后，我们到达“leadout吞噬者”行。
  对于‘mxTypography.py’，“leadout”是和“leadin”对立的。后者可能出现在
  某单词模式前面，而前者可能出现在某单词模式的后面。‘leadout_set’包括
  了空白字符，但同时也包含了逗号，句号，和问号，它们可能标志
  一个单词的结束。“leadout吞噬者”也使用了一个回调函数。
  它被设计为准确保存输入中的空白。但是，很容易就可以让它规格化
  此处的空白，只要给出和实际匹配不同的东西（例如，总是一个空格）。
##lwl： leadin leadout 如何翻译?
  
  The 'jump_count' is extremely important; we will come back to it
  momentarily. For now, it is enough to say that we -hope- the line
  never does anything.
  ‘jump_count’极其重要；我们马上回到这个话题。现在，我们可以说
  我们-希望-当前行从来不做事。

  The 'EOF' line is our flow control, in a way. The call made by
  this line is to 'None', which is to say that nothing is actually
  -done- with any match. The command 'EOF' is the important thing
  ('Here' is just a filler value that occupies the tuple position).
  It succeeds if the read-head is past the end of the read buffer.
  On success, the whole tag table 'tag_words' succeeds, and having
  succeeded, processing stops. 'EOF' failure is more interesting.
  Assuming we haven't reached the end of our string, we jump -13
  states (to 'bullet_point'). From there, the whole process starts
  over, hopefully with the read-head advanced to the next word. By
  looping back to the start of the list of tuples, we continue
  eating successive word patterns until the read buffer is
  exhausted (calling callbacks along the way).
  ‘EOF’行在某种程度上是我们的流控制。此行进行的调用时‘None’，
  它表示实际上没有匹配-完成-任何事情。命令‘EOF’是个重要的事情
  （‘Here’只是一个填充值，用来占据元组中的位置）。如果read-head
  越过了读入缓存的末尾，它就是成功了。如果这样的话，整个标签表‘tag_words’
  亦为成功，整个处理也停止了。‘EOF’失败的情形更为有趣。
  假设我们并未到达字符串的末尾，我们跳转-13个状态（到达‘bullet_point’）。
  整个处理过程从那儿又重新开始，而read-head则极可能移向下一个单词。
  通过循环回元组列表的开始，我们继续吃进连续的单词模式，直到读入缓存
  被耗光（因为一直调用回调函数）。

  The 'tag()' call simply launches processing of the tag table we
  pass to it (against the read buffer contained in 'txt'). In our
  case, we do not care about the return value of 'tag()' since
  everything is handled in callbacks. However, in cases where the
  tag table does not loop itself, the returned tuple can be
  used to determine if there is reason to call 'tag()' again with a
  tail of the read buffer.
  呼叫‘tag()’只是开始处理我们传递给它的标签表（相对于包含在‘txt’中
  的读入缓存）。对于我们这种情况，我们并不关系‘tag()’的返回值，
  因为所有事情都在回调中处理了。但是，有些情况是标签表并不自我循环，
  返回的元组可以用来决定是否需要再次调用‘tag()’以处理读入缓存的尾部。

  DEBUGGING A TAG TABLE:
  调试标签表：

  Describing it is easy, but I spent a large number of hours
  finding the exact collection of tag tables that would match every
  pattern I was interested in without mismatching any pattern as
  something it wasn't. While smart ASCII markup seems pretty
  simple, there are actually quite a few complications (e.g.,
  markup characters being used in nonmarkup contexts, or markup
  characters and other punctuation appearing in various sequences).
  Any structured document format that is complicated enough to
  warrant using [mx.TextTools] instead of [string] is likely to
  have similar complications.
  说起来容易，但我花了大把时间来寻找标签表的精确集合，以便匹配我感兴趣
  的每个模式，而不会错误匹配到我不感兴趣的。智能ASCII标识看上去
  相当简单，实际上还是有一些难度的（例如，在非标识上下文中使用了标识字符，
  或者标识字符以及其他标点以不同顺序出现）。任何结构文档格式
  因为足复杂而需要使用[mx.TextTools]而非[string]，都有可能遇到类似的情况。

  Without question, the worst thing that can go wrong in a looping
  state pattern like the one above is that -none- of the listed
  states match from the current read-head position. If that
  happens, your program winds up in a tight infinite loop (entirely
  inside the extension module, so you cannot get at it with Python
  code directly). I wound up forcing a manual kill of the process
  -countless- times during my first brush at [mx.TextTools]
  development.
  毫无疑问，在类似上面这个的循环状态模式中出错的最坏情况是，
  在列出的状态中，-没有一个-匹配当前read-head位置。如果不幸发生
  这种情况，你的程序将陷入一个紧密的无限循环中（完全位于
  扩展模块中，所以你不可能从Python代码中直接进入）。
  我第一次涉足[mx.TextTools]开发时，就以无数次手工杀灭进程而结束。
##lwl: tight 紧密的？不是特别好。考虑删除
  
  Fortunately, there is a solution to the infinite loop problem.
  This is to use a callback like 'jump_count'.
  幸运的是，对于这种无限循环问题，还是有解决方案的。那就是使用
  类似‘jump_count’的回调函数。

      #-------- mxTypography.py infinite loop catcher ---------#
      #-------- mxTypography.py 无限循环捕捉器 ---------#
      def jump_count(taglist,txt,l,r,subtag):
          global head_pos
          if head_pos is None: head_pos = r
          elif head_pos == r:
              raise "InfiniteLoopError", \
                    txt[l-20:l]+'{'+txt[l]+'}'+txt[l+1:r+15]
          else: head_pos = r

  The basic purpose of 'jump_count' is simple: We want to catch the
  situation where our tag table has been run through multiple times
  without matching anything. The simplest way to do this is to
  check whether the last read-head position is the same as the
  current. If it is, more loops cannot get anywhere, since we have
  reached the exact same state twice, and the same thing is fated
  to happen forever. 'mxTypography.py' simply raises an error to
  stop the program (and reports a little bit of buffer context to
  see what is going on).
  ‘jump_count’的基本目的很简单：我们需要捕捉一个情况，就是我们的标签表
  已经被跑了若干遍，而没有匹配到任何东西。最简单的方法就是检查最后
  的read-head位置是否和当前的一样。如此，就算更多循环也没有其他去处，
  因为我们已经两次到达完全一样的位置，而同样的事情注定会永远继续发生。
  ‘mxTypography.py’只是引起一个错误来停止程序（并报告一点点缓存的上下文
  以便用户知道进行到何处）。
#raise an error

  It is also be possible manually to move the read-head, and try
  again from a different starting position. To manipulate the read
  head in this fashion, you could use the 'Call' command in tag
  table items. But a better approach is to create a nonlooping tag
  table that is called repeatedly from a Python loop. This
  Python loop can look at a returned tuple and use adjusted
  offsets in the next call if no match occurred.  Either way,
  since much more time is spent in Python this way than with the
  loop tag table approach, less speed would be gained from
  [mx.TextTools].
  也可以手工移动read-head，然后再次尝试一个不同的开始位置。
  为了照这样操作read-head，你可以使用标签表条目中的命令‘Call’。
  但是还有一个更好的方法，就是创建一个非循环标签表，它被一个Python
  循环反复调用。该Python循环可以检查返回的元组，如果没有匹配发生，
  将在下一个调用中使用调整的位移。这两种方法，都比循环标签表的方法
  花费了更多时间在Python中，所以通过[mx.TextTools]获得的加速将
  不会有以前多。

  Not as bad as an infinite loop, but still undesirable, is
  having patterns within a tag table match when they are not
  supposed to or not match when they are suppose to (but
  something else has to match, or we would have an infinite loop
  issue).  Using callbacks everywhere makes examining this
  situation much easier.  During development, I frequently
  create temporary changes to my 'emit_*' callbacks to print or
  log when certain emitters get called.  By looking at output
  from these temporary 'print' statements, most times you can
  tell where the problem lies.
  另一个情况没有无限循环那么糟糕，但也是不受欢迎的，它就是
  标签表中的模式在不该匹配的时候匹配了，或者在该匹配的时候，
  并未匹配（却被其他模式匹配了，否则我们会有无限循环问题）。
  处处使用回调函数可以帮助检测这种情形。在开发中，我通常
  临时改变我的‘emit_*’回调函数，当特定emitter被调用的时候，
  可以打印或记录。通过检查这些临时‘print’声明的输出，
  绝大部分时间你都可以说出问题所在。
  

  +++

  -*-

  CONSTANTS:
  常数：

  The [mx.TextTools] module contains constants for a number of
  frequently used collections of characters. Many of these
  character classes are the same as ones in the [string] module.
  Each of these constants also has a 'set' version predefined; a
  set is an efficient representation of a character class that may
  be used in tag tables and other [mx.TextTools] functions. You may
  also obtain a character set from a (custom) character class using
  the `mx.TextTools.set()` function:
  [mx.TextTools]模块包含了一些常数，都是一些常用字符的集合。
  许多这些字符类都和[string]模块中的一样。每个此类常数还有一个
  预先定义的‘集合’版本；集合是可用于标签表或者其他[mx.TextTools]函数中
  字符类的高效表示。你可以用`mx.TextTools.set()`函数来从（自定义）
  字符类中获取一个字符集合：

      >>> from mx.TextTools import a2z, set
      >>> varname_chars = a2z + '_'
      >>> varname_set = set(varname_chars)

  mx.TextTools.a2z
  mx.TextTools.a2z_set
      English lowercase letters ("abcdefghijklmnopqrstuvwxyz").
	  英语中小写字母（“abcdefghijklmnopqrstuvwxyz”）。
	  
  mx.TextTools.A2Z
  mx.TextTools.A2Z_set
      English uppercase letters ("ABCDEFGHIJKLMNOPQRSTUVWXYZ").
	  英语中大写字母（“ABCDEFGHIJKLMNOPQRSTUVWXYZ”）。

  mx.TextTools.umlaute
  mx.TextTools.umlaute_set
      Extra German lowercase hi-bit characters.
	  额外的德语小写hi-bit字符

  mx.TextTools.Umlaute
  mx.TextTools.Umlaute_set
      Extra German uppercase hi-bit characters.
	  额外的德语大写hi-bit字符

  mx.TextTools.alpha
  mx.TextTools.alpha_set
      English letters (A2Z + a2z).
	  英语字母（A2Z + a2z）。

  mx.TextTools.german_alpha
  mx.TextTools.german_alpha_set
      German letters (A2Z + a2z + umlaute + Umlaute).
	  德语字母（A2Z + a2z + umlaute + Umlaute）。

  mx.TextTools.number
  mx.TextTools.number_set
      The decimal numerals ("0123456789").
	  十进制数字（“0123456789”）。

  mx.TextTools.alphanumeric
  mx.TextTools.alphanumeric_set
      English numbers and letters (alpha + number).
	  英语数字和字母（alpha + number）。

  mx.TextTools.white
  mx.TextTools.white_set
      Spaces and tabs (" \t\v").  Notice that this is more
      restricted than `string.whitespace`.
	  空格和制表符（“ \t\v”）。注意这个比`string.whitespace`
	  更有限

  mx.TextTools.newline
  mx.TextTools.newline_set
      Line break characters for various platforms ("\n\r").
	  多个平台上的换行符（“\n\r”）。

  mx.TextTools.formfeed
  mx.TextTools.formfeed_set
      Formfeed character ("\f").
	  进纸符（“\f”）。

  mx.TextTools.whitespace
  mx.TextTools.whitespace_set
      Same as `string.whitespace` (white + newline + formfeed).
	  和`string.whitespace`（white + newline + formfeed）一样。

  mx.TextTools.any
  mx.TextTools.any_set
      All characters (0x00-0xFF).
	  所有字符（0x00-0xFF）。

  SEE ALSO, `string.digits`, `string.hexdigits`,
  `string.octdigits`, `string.lowercase`, `string.uppercase`,
  `string.letters`, `string.punctuation`, `string.whitespace`,
  `string.printable`
  参见`string.digits`，`string.hexdigits`，
  `string.octdigits`， `string.lowercase`， `string.uppercase`，
  `string.letters`， `string.punctuation`， `string.whitespace`，
  `string.printable`

  COMMANDS:
  命令：

  Programming in [mx.TextTools] amounts mostly to correctly
  configuring tag tables.  Utilizing a tag table requires just one
  call to the `mx.TextTools.tag()`, but inside a tag table is a
  kind of mini-language--something close to a specialized
  Assembly language, in many ways.
  [mx.TextTools]中的编程很大程度上就是要正确配置标签表。
  使用一个标签表只需要调用一次`mx.TextTools.tag()`，但
  标签表内部是某种迷你语言--在很多方面，都和某个专门的
  汇编语言类似。

  Each tuple within a tag table contains several elements, of the
  form:
  每个标签表中的元组都包含若干个元素，形式如下：

      #*----------- tag table entry format --------------------#
      #*----------- 标签表条目格式 --------------------#
      (tagobj, command[+modifiers], argument
               [,jump_no_match=MatchFail [,jump_match=+1]])

  The "tag object" may be 'None', a callable object, or a string.
  If 'tagobj' is 'None', the indicated pattern may match, but
  nothing is added to a taglist data structure if so, nor is a
  callback invoked.  If a callable object (usually a function) is
  given, it acts as a callback for a match.  If a string is used,
  it is used to name a part of the taglist data structure
  returned by a call to `mx.TextTools.tag()`.
  “标签对象tagobj”可以是‘None’，可调用对象，或者字符串。
  如果‘tagobj’是‘None’，显示的模式可能匹配，但是不会有
  内容添加到标签列表数据结构，也不会调用回调函数。如果给出的
  是一个可调用对象（通常是一个函数），它就像匹配的回调函数一样工作。
  如果使用的是字符串，它将被用来命名一部分调用`mx.TextTools.tag()`
  返回的标签列表数据结构。

  A command indicates a type of pattern to match, and a modifier
  can change the behavior that occurs in case of such a match.
  Some commands succeed or fail unconditionally, but allow you to
  specify behaviors to take if they are reached.  An argument is
  required, but the specific values that are allowed and how they
  are interpreted depends on the command used.
  命令显示欲匹配的此类模式，修饰符可以改变此类匹配中发生的行为。
  一些行为的成功或者失败是无条件地，但允许你指定它们发生后的行为。
  要求给出一个参数，但是允许的参数值以及它们是如何解释的，则依赖
  于使用的命令。
  
  Two jump conditions may optionally be specified.  If no values
  are given, 'jump_no_match' defaults to 'MatchFail'--that is,
  unless otherwise specified, failing to match a tuple in a tag
  table causes the tag table as a whole to fail.  If a value -is-
  given, 'jump_no_match' branches to a tuple the specified number
  of states forward or backward.  For clarity, an explicit
  leading "+" is used in forward branches.  Branches backward,
  will begin with a minus sign. For example:
  用户也可以随意指定两个跳转。如果没有给定值，‘jump_no_match’的默认值
  是‘MatchFail’--也就是说，除非另外指定，无法在标签表中匹配一个
  元组会导致整个标签表失败。如果-给出-了一个值，‘jump_no_match’
  转向一个元组，后者指定一个数字，以决定状态前移或后退。为了清晰起见，
  在前向分支中会使用一个显式的引导“+”。而后向分支，将会以一个
  减号开始。例如：

      #*------ Tuple in tag table with explicit branches ------#
      # Branch forward one state if next character -is not- an X
      # ... branch backward three states if it is an X
      #*------ 标签表中的元组，拥有显式分支 ------#
      # 分支将前移一个状态，如果下一个字符-不是-X
      # ... 分支将后移三个状态，如果是个X
      tupX = (None, Is, 'X', +1, -3)
      # assume all the tups are defined somewhere...
      # 假设所有的tup都已在某处定义...
      tagtable = (tupA, tupB, tupV, tupW, tupX, tupY, tupZ)

  If no value is given for 'jump_match', branching is one state
  forward in the case of a match.
  如果‘jump_match’并未被给定任何值，如果任何匹配成功，
  就向前移动一个状态。

  Version 2.1.0 of [mx.TextTools] adds named jump targets, which
  are often easier to read (and maintain) than numeric offsets.
  An example is given in the [mx.TextTools] documentation:
  版本2.1.0的[mx.TextTools]添加了命名跳转目标，它们通常比数字位移
  更加容易阅读以及维护。在[mx.TextTools]文档中给出了一个例子：

      #*------ Tuple in tag table with named branches ----------#
      #*------ 标签表中的元组，使用了命名分支 ----------#
      tag_table = ('start',
                   ('lowercase',AllIn,a2z,+1,'skip'),
                   ('upper',AllIn,A2Z,'skip'),
                   'skip',
                   (None,AllIn,white+newline,+1),
                   (None,AllNotIn,alpha+white+newline,+1),
                   (None,EOF,Here,'start') )

  It is easy to see that if you were to add or remove a tuple, it
  is less error prone to retain a jump to, for example, 'skip' than
  to change every necessary '+2' to a '+3' or the like.
  如果你要添加或者移除一个元组，使用例如‘skip’此类的跳转会比
  修改每个相关的‘+2’到‘+3’或者类似的改变，引起的错误更少。

  UNCONDITIONAL COMMANDS:
  无条件的命令：

  mx.TextTools.Fail
  mx.TextTools.Jump
      Nonmatch at this tuple.  Used mostly for documentary
      purposes in a tag table, usually with the 'Here' or 'To'
      placeholder.  The tag tables below are equivalent:
	  对此元组无法匹配。绝大部分是在标签表中作为文档目的使用，
	  通常和‘Here’或者‘To’占位符一起使用。下面的标签表是等价的：

      #*--------- Fail command and MatchFail branch -----------#
      #*--------- 失败命令和MatchFail分支 -----------#
      table1 = ( ('foo', Is, 'X', MatchFail, MatchOk), )
      table2 = ( ('foo', Is, 'X', +1, +2),
                 ('Not_X', Fail, Here) )

      The 'Fail' command may be preferred if several other states
      branch to the same failure, or if the condition needs to be
      documented explicitly.
	  如果几个其他状态转向同一个失败，或者相关条件需要在文档中显式说明，
	  ‘Fail’命令将是首选的。

      'Jump' is equivalent to 'Fail', but it is often better
      self-documenting to use one rather than the other; for
      example:
	  ‘Jump’和‘Fail’是等价的，但它比后者的文档更好；例如：

      #*---------- Jump versus Fail for readability -----------#
      #*---------- Jump和Fail的可读性对比 -----------#
      tup1 = (None, Fail, Here, +3)
      tup2 = (None, Jump, To, +3)

  mx.TextTools.Skip
  mx.TextTools.Move
      Match at this tuple, and change the read-head position.
      'Skip' moves the read-head by a relative amount, 'Move' to
      an absolute offset (within the slice the tag table is
      operating on).  For example:
	  匹配吃元组，并改变read-head位置。‘Skip’将read-head移动一个
	  相对数字，而‘Move’则移动一个绝对位移（范围在标签表操作的切片内）。
	  例如：

      #*---------------- Skip and Move examples ---------------#
      # read-head forward 20 chars, jump to next state
      #*---------------- Skip和Move的例子 ---------------#
      # read-head 前移20个字符，跳至下一状态
      tup1 = (None, Skip, 20)
      # read-head to position 10, and jump back 4 states
      # read-head跳至位置10，并跳回4个状态
      tup2 = (None, Move, 10, 0, -4)

      Negative offsets are allowed, as in Python list indexing.
	  可以使用负位移，就和Python列表的下标一样。

  MATCHING PARTICULAR CHARACTERS:
  匹配特别字符

  mx.TextTools.AllIn
  mx.TextTools.AllInSet
  mx.TextTools.AllInCharSet
      Match all characters up to the first that is not included
      in 'argument'.  'AllIn' uses a character string while
      'AllInSet' uses a set as 'argument'.  For version 2.1.0,
      you may also use 'AllInCharSet' to match CharSet objects.
      In general, the set or CharSet form will be faster and is
      preferable.  The following are functionally the same:
	  匹配所有字符，直到第一个不在‘argument’中的字符才停止。
	  ‘AllIn’使用一个字符串，而‘AllInSet’则使用一个集合set作为
	  ‘argument’。对于版本2.1.0，你还可以使用‘AllInCharSet’来
	  匹配CharSet对象。整体来说，set或者CharSet形式都较快，
	  是首选项。下面的代码在功能上是一样的：

      #*-------- Equivalent AllIn and AllInSet tuples ---------#
      #*-------- 等价的AllIn和AllInSet元组 ---------#
      tup1 = ('xyz', AllIn, 'XYZxyz')
      tup2 = ('xyz', AllInSet, set('XYZxyz')
      tup3 = ('xyz', AllInSet, CharSet('XYZxyz'))

      At least one character must match for the tuple to match.
	  至少要有一个字符能匹配到目标元组。

  mx.TextTools.AllNotIn
      Match all characters up to the first that -is- included in
      'argument'.  As of version 2.1.0, [mx.TextTools] does not
      include an 'AllNotInSet' command.  However, the following
      tuples are functionally the same (the second usually
      faster):
	  匹配所有字符，直到第一个-不-在‘argument’中的字符。
	  对于版本2.1.0，[mx.TextTools]不包括‘AllNotInSet’命令。
	  但是，下面的元组在功能上是一样的（第二个通常快一点）：

      #*------ Equivalent AllNotIn and AllInSet tuples --------#
      #*------ 等价的AllNotIn和AllInSet元组 --------#
      from mx.TextTools import AllNotIn, AllInSet, invset
      tup1 = ('xyz', AllNotIn, 'XYZxyz')
      tup2 = ('xyz', AllInSet, invset('xyzXYZ'))

      At least one character must match for the tuple to match.
	  至少要有一个字符能匹配到目标元组。

  mx.TextTools.Is
      Match specified character.  For example:
	  匹配指定的字符。例如：

      #*------------ Match the character 'X' ------------------#
      #*------------ 匹配字符'X' ------------------#
      tup = ('X', Is, 'X')

  mx.TextTools.IsNot
      Match any one character except the specified character.
	  匹配任何字符，但不能是指定的字符。

      #*------------ Match non-'X' character ------------------#
      #*------------ 匹配非'X'字符 ------------------#
      tup = ('X', IsNot, 'X')

  mx.TextTools.IsIn
  mx.TextTools.IsInSet
  mx.TextTools.IsInCharSet
      Match exactly one character if it is in 'argument'.
      'IsIn' uses a character string while 'IsInSet' use a set as
      'argument'.  For version 2.1.0, you may also use
      'IsInCharSet' to match CharSet objects.  In general, the
      set or CharSet form  will be faster and is preferable.
      The following are functionally the same:
	  精确匹配一个字符，如果它在‘argument’中。
	  ‘IsIn’使用一个字符串而‘IsInSet’使用一个集合set来作为‘argument’。
	  对于版本2.1.0，你还可以使用‘IsInCharSet’来匹配CharSet对象。
	  整体来说，set或者CharSet形式都较快，是首选项。
	  下面的代码在功能上是一样的：

      #*-------- 等价的IsIn和IsInSet元组 ---------#
      tup1 = ('xyz', IsIn, 'XYZxyz')
      tup2 = ('xyz', IsInSet, set('XYZxyz')
      tup3 = ('xyz', IsInSet, CharSet('XYZxyz')

  mx.TextTools.IsNotIn
      Match exactly one character if it is -not- in 'argument'.
      As of version 2.1.0, [mx.TextTools] does not include an '
      'AllNotInSet' command.  However, the following tuples are
      functionally the same (the second usually faster):
	  精确匹配一个字符，如果它-不-在‘argument’中。
	  对于版本2.1.0，[mx.TextTools]不包括‘AllNotInSet’命令。
	  但是，下面的元组在功能上是一样的（第二个通常快一点）：

      #*------ Equivalent IsNotIn and IsInSet tuples --------#
      #*------ 等价的IsNotIn和IsInSet元组 --------#
      from mx.TextTools import IsNotIn, IsInSet, invset
      tup1 = ('xyz', IsNotIn, 'XYZxyz')
      tup2 = ('xyz', IsInSet, invset('xyzXYZ'))


  MATCHING SEQUENCES:
  匹配序列：

  mx.TextTools.Word
      Match a word at the current read-head position.  For example:
	  在当前的read-head位置匹配一个单词。例如：

      #*-------------------- Word example ---------------------#
      #*-------------------- 单词例子 ---------------------#
      tup = ('spam', Word, 'spam')

  mx.TextTools.WordStart
  mx.TextTools.sWordStart
  mx.TextTools.WordEnd
  mx.TextTools.sWordEnd
      Search for a word, and match up to the point of the match.
      Searches performed in this manner are extremely fast, and
      this is one of the most powerful elements of tag tables.
      The commands 'sWordStart' and 'sWordEnd' use "search
      objects" rather than plaintexts (and are significantly
      faster).
	  寻找一个单词，并跟上匹配的点。
	  ## 不懂
	  以此形式进行的搜索极其迅速，而且它是标签表中最强大元素之一。
	  命令‘sWordStart’和‘sWordEnd’使用“search objects”而非
	  普通文本（还非常明显的快呢）。

      'WordStart' and 'sWordStart' leave the read-head
      immediately prior to the matched word, if a match succeeds.
      'WordEnd' and 'sWordEnd' leave the read-head immediately
      after the matched word.  On failure, the read-head is not
      moved for any of these commands.
	  如果有匹配成功，‘WordStart’和‘sWordStart’将把read-head直接
	  留在匹配单词的前面，而‘WordEnd’和‘sWordEnd’将把read-head直接
	  留在匹配单词的后面。如果失败，对于这些命令而言，read-head将
	  不会做任何移动。

      >>> from mx.TextTools import *
      >>> s = 'spam and eggs taste good'
      >>> tab1 = ( ('toeggs', WordStart, 'eggs'), )
      >>> tag(s, tab1)
      (1, [('toeggs', 0, 9, None)], 9)
      >>> s[0:9]
      'spam and '
      >>> tab2 = ( ('pasteggs', sWordEnd, BMS('eggs')), )
      >>> tag(s, tab2)
      (1, [('pasteggs', 0, 13, None)], 13)
      >>> s[0:13]
      'spam and eggs'

      SEE ALSO, `mx.TextTools.BMS()`, `mx.TextTools.sFindWord`
	  参见`mx.TextTools.BMS()`, `mx.TextTools.sFindWord`

  mx.TextTools.sFindWord
      Search for a word, and match only that word.  Any
      characters leading up to the match are ignored.  This
      command accepts a search object as an argument.  In case of
      a match, the read-head is positioned immediately after the
      matched word.
	  搜索并只匹配一个单词。在此匹配前的任何字符都将被忽略。
	  该命令接受搜索对象作为参数。如果匹配到，read-head将
	  被紧靠着放在匹配单词的后面。

      >>> from mx.TextTools import *
      >>> s = 'spam and eggs taste good'
      >>> tab3 = ( ('justeggs', sFindWord, BMS('eggs')), )
      >>> tag(s, tab3)
      (1, [('justeggs', 9, 13, None)], 13)
      >>> s[9:13]
      'eggs'

      SEE ALSO, `mx.TextTools.sWordEnd`
	  参见`mx.TextTools.sWordEnd`

  mx.TextTools.EOF
      Match if the read-head is past the end of the string slice.
      Normally used with placeholder argument 'Here', for
      example:
	  如果read-head已经超过字符串切片的末尾，则匹配成功。
	  通常和占位符参数‘Here’一起使用，例如：

      #*-------------------- EOF example ----------------------#
      #*-------------------- EOF例子 ----------------------#
      tup = (None, EOF, Here)


  COMPOUND MATCHES:
  混合匹配：

  mx.TextTools.Table
  mx.TextTools.SubTable
      Match if the table given as 'argument' matches at the
      current read-head position.  The difference between the
      'Table' and the 'SubTable' commands is in where matches get
      inserted.  When the 'Table' command is used, any matches in
      the indicated table are nested in the data structure
      associated with the tuple.  When 'SubTable' is used,
      matches are written into the current level taglist.  For
      example:
	  如果作为‘参数’给出的表在当前read-head位置匹配，才算匹配。
	  命令‘Table’和‘SubTable’之间的差异在于，匹配在何处被嵌入。  ##不懂
	  当使用‘Table’命令时，显示的表中任何的匹配都嵌套在与元组关联的
	  数据结构中。而使用‘SubTable’时，匹配被写入当前层的标签列表中。
	  例如：

      >>> from mx.TextTools import *
      >>> from pprint import pprint
      >>> caps = ('Caps', AllIn, A2Z)
      >>> lower = ('Lower', AllIn, a2z)
      >>> words = ( ('Word', Table, (caps, lower)),
      ...           (None, AllIn, whitespace, MatchFail, -1) )
      >>> from pprint import pprint
      >>> pprint(tag(s, words))
      (0,
       [('Word', 0, 4, [('Caps', 0, 1, None), ('Lower', 1, 4, None)]),
        ('Word', 5, 19, [('Caps', 5, 6, None), ('Lower', 6, 19, None)]),
        ('Word', 20, 29, [('Caps', 20, 24, None), ('Lower', 24, 29, None)]),
        ('Word', 30, 35, [('Caps', 30, 32, None), ('Lower', 32, 35, None)])
       ],
       35)
      >>> flatwords = ( (None, SubTable, (caps, lower)),
      ...               (None, AllIn, whitespace, MatchFail, -1) )
      >>> pprint(tag(s, flatwords))
      (0,
       [('Caps', 0, 1, None),
        ('Lower', 1, 4, None),
        ('Caps', 5, 6, None),
        ('Lower', 6, 19, None),
        ('Caps', 20, 24, None),
        ('Lower', 24, 29, None),
        ('Caps', 30, 32, None),
        ('Lower', 32, 35, None)],
       35)

      For either command, if a match occurs, the read-head is
      moved to immediately after the match.
	  对于两者而言，如果匹配发生，read-head被移动到紧接着匹配后面。

      The special constant 'ThisTable' can be used instead of a
      tag table to call the current table recursively.
	  特别常数‘ThisTable’可以用来代替标签表，以迭代调用当前表。

  mx.TextTools.TableInList
  mx.TextTools.SubTableInList
      Similar to 'Table' and 'SubTable' except that
      the 'argument' is a tuple of the form
      '(list_of_tables,index)'.  The advantage (and the danger)
      of this is that a list is mutable and may have tables
      added after the tuple defined--in particular, the
      containing tag table may be added to 'list_of_tables' to
      allow recursion.  Note, however, that the special value
      'ThisTable' can be used with the 'Table' or 'SubTable'
      commands and is usually more clear.
	  和‘Table’以及‘SubTable’类似，除了‘参数’是‘(list_of_tables,index)’
	  形式的元组。这个的好处（以及危险之处）就是，列表是不可变的，
	  但在元组定义以后，有可能有表需要加入--特别地，
	  可以在‘list_of_tables’中加入包含性标签表，以允许迭代。
	  但是注意，特殊值‘ThisTable’可以和‘Table’或者‘SubTable’命令
	  一起使用，通常也更加清晰。

      SEE ALSO, `mx.TextTools.Table`, `mx.TextTools.SubTable`
	  参见`mx.TextTools.Table`， `mx.TextTools.SubTable`

  mx.TextTools.Call
      Match on any computable basis.  Essentially, when the
      'Call' command is used, control over parsing/matching is
      turned over to Python rather than staying in the
      [mx.TextTools] engine.  The function that is called must
      accept arguments 's', 'pos', and 'end'--where 's' is the
      underlying string, 'pos' is the current read-head position,
      and 'end' is ending of the slice being processed.  The
      called function must return an integer for the new
      read-head position; if the return is different from 'pos',
      the match is a success.
	  匹配任何可计算成分。本质上，当使用‘Call’命令时，解析/匹配
	  的控制权被交回Python，而非留在[mx.TextTools]引擎。
	  调用的函数必须接受参数‘s’，‘pos’以及‘end’--此处‘s’是
	  下面的字符串，‘pos’是当前read-head位置，而‘end’是处理过的
	  切片的末尾。呼叫的函数必须返回一个整数作为新的read-head位置；
	  如果返回值和‘pos’不同，匹配成功。
	  ## computable basis

      As an example, suppose you want to match at a certain
      point only if the next N characters make up a dictionary
      word.  Perhaps an efficient stemmed data structure is
      used to represent the dictionary word list.  You might
      check dictionary membership with a tuple like:
	  作为一个例子，假设你只想要匹配一个特定点，如果下面N个字符
	  凑成一个字典单词的话。也许一个有效的数据结构被用来表现
	  字典单词列表。你可以用以下这样一个元组来检查字典成员：

      #*-------------------- Call example ---------------------#
      #*-------------------- Call例子 ---------------------#
      tup = ('DictWord', Call, inDict)

      Since the function 'inDict' is written in Python, it will
      generally not operate as quickly as does an [mx.TextTools]
      pattern tuple.
	  因为函数‘inDict’是使用Python编写，它通常不会和[mx.TextTools]
	  模式元组的操作一样快。

  mx.TextTools.CallArg
      Same as 'Call', except 'CallArg' allows passing additional
      arguments. For example, suppose the dictionary example
      given in the discussion of 'Call' also allows you to
      specify language and maximum word length for a match:
	  和‘Call’一样，除了‘CallArg’允许传入额外的参数。例如，
	  假设在‘Call’中讨论的字典例子，它允许你指定语言和匹配
	  的最大单词长度：

      #*-------------------- Call example ---------------------#
      #*-------------------- Call例子 ---------------------#
      tup = ('DictWord', Call, (inDict,['English',10]))

      SEE ALSO, `mx.TextTools.Call`
	  参见`mx.TextTools.Call`

  MODIFIERS:
  修饰词：
  ## 此处什么意思？缺行？

  mx.TextTools.CallTag
      Instead of appending '(tagobj,l,r,subtags)' to the taglist
      upon a successful match, call the function indicated as the
      tag object (which must be a function rather than 'None' or
      a string).  The function called must accept the arguments
      'taglist', 's', 'start', 'end', and 'subtags'--where
      'taglist' is the present taglist, 's' is the underlying
      string, 'start' and 'end' are the slice indices of the
      match, and 'subtags' is the nested taglist.  The function
      called -may-, but need not, append to or modify 'taglist' or
      'subtags' as part of its action.  For example, a code
      parsing application might include:
	  成功匹配时，并非追加‘(tagobj,l,r,subtags)’到标签列表，
	  而是调用作为标签对象显示的函数（它必须是个函数，不能是
	  ‘None’或者字符串）。 调用的函数必须接受参数‘taglist’，
	  ‘s’，‘start’，‘end’以及‘subtags’--此处‘taglist’是当前的标签表，
	  ‘s’是下面的字符串，‘start’和‘end’是匹配的切片下标，而‘subtags’
	  是嵌套的标签列表。呼叫的函数-可以-，但不是必要，将追加
	  或者修改‘taglist’或‘subtags’作为部分活动。例如，一个解析
	  程序的代码可能包括：
	  
      #*----------------- CallTag example ---------------------#
      #*----------------- CallTag例子 ---------------------#
      >>> def todo_flag(taglist, s, start, end, subtags):
      ...     sys.stderr.write("Fix issue at offset %d\n" % start)
      ...
      >>> tup = (todo_flag, Word+CallTag, 'XXX')
      >>> tag('XXX more stuff', (tup,))
      Fix issue at offset 0
      (1, [], 3)


  mx.TextTools.AppendMatch
      Instead of appending '(tagobj,start,end,subtags)' to the
      taglist upon successful matching, append the match found as
      string.   The produced taglist is "flattened" and cannot be
      used in the same manner as "normal" taglist data
      structures.  The flat data structure is often useful for
      joining or for list processing styles.
	  成功匹配时，并非追加‘(tagobj,start,end,subtags)’到标签列表，
	  而是将匹配内容作为字符串添加。产生的标签列表是“平坦的”，
	  不可以和“正常”标签列表数据结构同样使用。这种平数据结构通常
	  用于连接或者采用列表处理风格的时候。

      >>> from mx.TextTools import *
      >>> words = (('Word', AllIn+AppendMatch, alpha),
      ...          (None, AllIn, whitespace, MatchFail, -1))
      >>> tag('this and that', words)
      (0, ['this', 'and', 'that'], 13)
      >>> join(tag('this and that', words)[1], '-')
      'this-and-that'

      SEE ALSO, `string.split()`
	  参加`string.split()`

  mx.TextTools.AppendToTagobj
      Instead of appending '(tagobj,start,end,subtags)' to the
      taglist upon successful matching, call the '.append()'
      method of the tag object.  The tag object must be
      a list (or a descendent of 'list' in Python 2.2+).
	  成功匹配时，并非追加‘(tagobj,start,end,subtags)’到标签列表，
	  而是调用标签对象的‘.append()’方法。标签对象必须是一个列表
	  （在Python 2.2以后，也可以是‘列表’的派生）。

      >>> from mx.TextTools import *
      >>> ws = []
      >>> words = ((ws, AllIn+AppendToTagobj, alpha),
      ...          (None, AllIn, whitespace, MatchFail, -1))
      >>> tag('this and that', words)
      (0, [], 13)
      >>> ws
      [(None, 0, 4, None), (None, 5, 8, None), (None, 9, 13, None)]

      SEE ALSO, `mx.TextTools.CallTag`
	  参见`mx.TextTools.CallTag`

  mx.TextTools.AppendTagobj
      Instead of appending '(tagobj,start,end,subtags)' to the
      taglist upon successful matching, append the tag object.
      The produced taglist is usually nonstandard and cannot be
      used in the same manner as "normal" taglist data
      structures.  A flat data structure is often useful for
      joining or for list processing styles.
	  成功匹配时，并非追加‘(tagobj,start,end,subtags)’到标签列表，
	  而是追加标签对象。产生的标签列表通常不是标准的，不可以和
	  “正常”的标签列表数据结构一样使用。这种平数据结构通常
	  用于连接或者采用列表处理风格的时候。
## tag object： 标签目标 还是 标签对象？

      >>> from mx.TextTools import *
      >>> words = (('word', AllIn+AppendTagobj, alpha),
      ...          (None, AllIn, whitespace, MatchFail, -1))
      >>> tag('this and that', words)
      (0, ['word', 'word', 'word'], 13)

  mx.TextTools.LookAhead
      If this modifier is used, the read-head position is not
      changed when a match occurs.  As the name suggests, this
      modifier allows you to create patterns similar to regular
      expression lookaheads.
	  如果使用了此修饰词，当匹配发生时，read-head位置不发生变化。
	  正如名字所暗示，此修饰词允许你创建于正则表达式中前观（lookhead）
	  类似的模式。

      >>> from mx.TextTools import *
      >>> from pprint import pprint
      >>> xwords = ((None, IsIn+LookAhead, 'Xx', +2),
      ...           ('xword', AllIn, alpha, MatchFail, +2),
      ...           ('other', AllIn, alpha),
      ...           (None, AllIn, whitespace, MatchFail, -3))
      >>> pprint(tag('Xylophone trumpet xray camera', xwords))
      (0,
       [('xword', 0, 9, None),
        ('other', 10, 17, None),
        ('xword', 18, 22, None),
        ('other', 23, 29, None)],
       29)

  CLASSES:
  类：

  mx.TextTools.BMS(word [,translate])
  mx.TextTools.FS(word [,translate])
  mx.TextTools.TextSearch(word [,translate [,algorithm=BOYERMOORE]])
      Create a search object for the string 'word'.  This is
      similar in concept to a compiled regular expression.  A
      search object has several methods to locate its encoded
      string within another string.   The 'BMS' name is short for
      "Boyer-Moore," which is a particular search algorithm.  The
      name 'FS' is reserved for accessing the "Fast Search"
      algorithm in future versions, but currently both classes
      use Boyer-Moore.  For [mx.TextTools] 2.1.0+, you are
      urged to use the '.TextSearch()' constructor.
	  为字符串‘word’创建一个搜索对象。这个和编译后的正则表达式在概念上
	  比较相似。一个搜索对象拥有若干个方法，以在其他字符串中定位它的
	  编码后的字符串。名字‘BMS’是“Boyer-Moore”的缩写，它是一种特定
	  的搜索算法。名字‘FS’被保留以在未来版本中使用“Fast Search”算法，
	  但目前两个类都使用Boyer-Moore。从[mx.TextTools] 2.1.0版本起，
	  你被期望使用‘.TextSearch()’构造器。
## 前面有些object的翻译需要修正

      If a 'translate' argument is given, the searched string is
      translated during the search.  This is equivalent to
      transforming the string with `string.translate()` prior to
      searching it.
	  如果给出了一个‘translate’参数，搜索的字符串会在此过程中被翻译。
	  这个和在搜索之前使用`string.translate()`来转换字符串是等价的。

      SEE ALSO, `string.translate()`
	  参见`string.translate()`

  mx.TextTools.CharSet(definition)
      Version 2.1.0 of [mx.TextTools] adds the Unicode-compatible
      'CharSet' object.  'CharSet' objects may be initialized
      to supports character ranges, as in regular expressions;
      for example  'definition="a-mXYZ"'.  In most respects,
      CharSet objects are similar to older sets.
	  版本2.1.0的[mx.TextTools]增加了兼容Unicode的‘CharSet’对象。
	  ‘CharSet’对象可以初始化以支持字符范围，如同正则表达式中一样；
	  例如，'definition="a-mXYZ"'。CharSet对象和旧的集合在绝大部分方面
	  都很相似。

  METHODS AND ATTRIBUTES:
  方法和属性：

  mx.TextTools.BMS.search(s [,start [,end]])
  mx.TextTools.FS.search(s [,start [,end]])
  mx.TextTools.TextSearch.search(s [,start [,end]])
      Locate as a slice the first match of the search object
      against 's'.  If optional arguments 'start' and 'end' are
      used, only the slice 's[start:end]' is considered.  Note:
      As of version 2.1.0, the documentation that accompanies
      [mx.TextTools] inaccurately describes the 'end' parameter
      of search object methods as indicating the length of the
      slice rather than its ending offset.
	  在‘s’中将搜索对象的第一次匹配作为切片定位。## lwl：翻译不好
	  如果使用了可选参数‘start’和‘end’，只考虑‘s[start:end]’切片。
	  注意：在版本2.1.0中，[mx.TextTools]的随机文档对于搜索对象方法
	  的‘end’参数描述不准确，因为它实际上是切片的长度，而非结束位移。

  mx.TextTools.BMS.find(s, [,start [,end]])
  mx.TextTools.FS.find(s, [,start [,end]])
  mx.TextTools.TextSearch.search(s [,start [,end]])
      Similar to `mx.TextTools.BMS.search()`, except return only
      the starting position of the match.  The behavior is
      similar to that of `string.find()`.
	  和`mx.TextTools.BMS.search()`相似，除了只返回匹配的开始位置。
	  行为和`string.find()`很相似。

      SEE ALSO, `string.find()`, `mx.TextTools.find()`
	  参见 `string.find()`， `mx.TextTools.find()`

  mx.TextTools.BMS.findall(s [,start [,end]])
  mx.TextTools.FS.findall(s [,start [,end]])
  mx.TextTools.TextSearch.search(s [,start [,end]])
      Locate as slices -every- match of the search object
      against 's'. If the optional arguments 'start' and 'end'
      are used, only the slice 's[start:end]' is considered.
	  在‘s’中将搜索对象的-每一次-匹配作为切片定位。## 翻译不是很好
	  如果使用了可选参数‘start’和‘end’，只考虑‘s[start:end]’切片。

      >>> from mx.TextTools import BMS, any, upper
      >>> foosrch = BMS('FOO', upper(any))
      >>> foosrch.search('foo and bar and FOO and BAR')
      (0, 3)
      >>> foosrch.find('foo and bar and FOO and BAR')
      0
      >>> foosrch.findall('foo and bar and FOO and BAR')
      [(0, 3), (16, 19)]
      >>> foosrch.search('foo and bar and FOO and BAR', 10, 20)
      (16, 19)

      SEE ALSO, `re.findall`, `mx.TextTools.findall()`
	  参见`re.findall`, `mx.TextTools.findall()`

  mx.TextTools.BMS.match
  mx.TextTools.FS.match
  mx.TextTools.TextSearch.match
      The string that the search object will look for in the
      search text (read-only).
	  搜索对象将用于寻找搜索文本的字符串（只读）。

  mx.TextTools.BMS.translate
  mx.TextTools.FS.translate
  mx.TextTools.TextSearch.match
      The translation string used by the object, or 'None' if no
      'translate' string was specified.
	  对象使用的翻译字符串，如果没有指定‘translate’字符串，则为‘None’。

  mx.TextTools.CharSet.contains(c)
      Return a true value if character 'c' is in the CharSet.
	  如果字符‘c’在CharSet中的话，返回true值。

  mx.TextTools.CharSet.search(s [,direction [,start=0 [,stop=len(s)]]])
      Return the position of the first CharSet character that
      occurs in 's[start:end]'.  Return 'None' if there is no
      match.  You may specify a negative 'direction' to search
      backwards.
	  返回‘s[start:end]’中第一个CharSet字符发生的位置。
	  如果没有匹配，则返回‘None’。你可以指定一个负的‘direction’
	  以反向搜索。

      SEE ALSO, `re.search()`
	  参阅`re.search()`

  mx.TextTools.CharSet.match(s [,direction [,start=0 [,stop=len(s)]]])
      Return the length of the longest contiguous match of the
      CharSet object against substrings of 's[start:end]'.
	  在子字符串‘s[start:end]’中匹配CharSet对象，返回最长
	  的相邻匹配的长度。

  mx.TextTools.CharSet.split(s [,start=0 [,stop=len(text)]])
      Return a list of substrings of 's[start:end]' divided by
      occurrences of characters in the CharSet.
	  返回一个列表，其中是‘s[start:end]’被CharSet中字符所分割
	  的子字符串。

      SEE ALSO, `re.search()`
	  参见`re.search()`

  mx.TextTools.CharSet.splitx(s [,start=0 [,stop=len(text)]])
      Like `mx.TextTools.CharSet.split()` except retain
      characters from CharSet in interspersed list elements.
	  和`mx.TextTools.CharSet.split()`相像，除了将CharSet中
	  的字符保留于分散的列表元素中。

  mx.TextTools.CharSet.strip(s [,where=0 [,start=0 [,stop=len(s)]]])
      Strip all characters in 's[start:stop]' appearing in the
      character set.
	  若‘s[start:stop]’中的字符在CharSet中出现，将其移除。
	  ## lwl：理解是否正确？

  FUNCTIONS:
  功能：

  Many of the functions in [mx.TextTools] are used by the tagging
  engine.  A number of others are higher-level utility functions
  that do not require custom development of tag tables.  The
  latter are listed under separate heading and generally
  resemble faster versions of functions in the [string] module.
  标签引擎使用了许多[mx.TextTools]中的许多功能。其他的功能有许多
  是高层使用函数，它们不需要对标签表进行自定义开发。
  后者被列于分别的标题下，通常类似于[string]模块下函数的更快版本。

  mx.TextTools.cmp(t1, t2)
      Compare two valid taglist tuples on their slice
      positions.   Taglists generated with multiple passes of
      `mx.TextTools.tag()`, or combined by other means, may not
      have tuples sorted in string order.  This custom comparison
      function is coded in C and is very fast.
	  在两个有效的标签列表元组的切片位置上比较它们。多次使用
	  `mx.TextTools.tag()`而产生的标签列表，或者是和其他方法混合产生的，
	  也许不能以字符串顺序对元组进行分类。此自定义比较函数以C写成，
	  速度极快。

      >>> import mx.TextTools
      >>> from pprint import pprint
      >>> tl = [('other', 10, 17, None),
      ...       ('other', 23, 29, None),
      ...       ('xword', 0, 9, None),
      ...       ('xword', 18, 22, None)]
      >>> tl.sort(mx.TextTools.cmp)
      >>> pprint(tl)
      [('xword', 0, 9, None),
       ('other', 10, 17, None),
       ('xword', 18, 22, None),
       ('other', 23, 29, None)]

  mx.TextTools.invset(s)
      Identical to 'mx.TextTools.set(s, 0)'.
	  与‘mx.TextTools.set(s, 0)’一样。

      SEE ALSO, `mx.TextTools.set()`
	  参见`mx.TextTools.set()`

  mx.TextTools.set(s [,includechars=1])
      Return a bit-position encoded character set. Bit-position
      encoding makes tag table commands like 'InSet' and
      'AllInSet' operate more quickly than their character-string
      equivalents (e.g, 'In', 'AllIn').
	  返回一个比特位置编码的字符集合。比特位置编码使得标签表命令，
	  例如‘InSet’和‘AllInSet’，比它们等价的字符串（例如，‘In’和‘AllIn’），
	  操作更快。

      If 'includechars' is set to 0, invert the character set.
	  如果‘includechars’被设置为0，将反转此字符集。#取字符集的反集？

      SEE ALSO, `mx.TextTools.invset()`
	  参见`mx.TextTools.invset()`
##utility 实用工具

  mx.TextTools.tag(s, table [,start [,end [,taglist]]])
      Apply a tag table to a string.  The return value is a
      tuple of the form '(success, taglist, next)'.  'success' is
      a binary value indicating whether the table matched.
      'next' is the read-head position after the match attempt.
      Even on a nonmatch of the table, the read-head might have
      been advanced to some degree by member tuples matching.
      The 'taglist' return value contains the data structure
      generated by application.  Modifiers and commands within
      the tag table can alter the composition of 'taglist'; but
      in the normal case, 'taglist' is composed of zero or more
      tuples of the form '(tagname, start, end, subtaglist)'.
	  将标签表应用到字符串。返回值是个‘(success, taglist, next)’
	  形式的元组。‘success’是个二进制值，显示该表是否匹配。
	  ‘next’是尝试匹配后的read-head位置。就算该表没有成功匹配，
	  read-head也可能因为成员元组被匹配到而进行某种程度的前移。
	  ‘taglist’返回的值包含了程序生成的数据结构。标签表中
	  修饰语和命令能改变‘taglist’的成分；但在正常情况下，
	  ‘taglist’是由零个或更多‘(tagname, start, end, subtaglist)’
	  形式的元组组成。

      Assuming a "normal" taglist is created, 'tagname' is a
      string value that was given as a tag object in a tuple
      within the tag table.  'start' and 'end' the slice ends of
      that particular match.  'subtaglist' is either 'None' or a
      taglist for a subtable match.
	  假设创建了一个“正常的”标签表，‘tagname’是一个字符串值，
	  它在标签表中作为元组中的标签对象给出。‘start’和‘end’是
	  该特别匹配的切片的开始和结尾。‘subtaglist’要么是‘None’，
	  要么是子表匹配的标签列表。

      If 'start' or 'end' are given as arguments to
      `mx.TextTools.tag()`, application is restricted to the
      slice 's[start:end]' (or 's[start:]' if only 'start' is
      used).  If a 'taglist' argument is passed, that list object
      is used instead of a new list.  This allows extending a
      previously generated taglist, for example.  If 'None' is
      passed as 'taglist', no taglist is generated.
	  如果‘start’或者‘end’被作为参数传给`mx.TextTools.tag()`，
	  程序被限制为作用于切片‘s[start:end]’（或者‘s[start:]’，
	  如果只使用了‘start’的话）。如果传入了‘taglist’参数，
	  则使用列表对象而非新的列表。例如，这样允许扩展以前产生
	  的标签列表。如果作为‘taglist’传递的是‘None’，则
	  不生成标签列表。

      See the application examples and command illustrations for
      a number of concrete uses of `mx.TextTools.tag()`.
	  参见程序例子和命令说明，其中有许多 `mx.TextTools.tag()`
	  的具体用法。

  UTILITY FUNCTIONS:
  使用函数：

  mx.TextTools.charsplit(s, char, [start [,end]])
      Return a list split around each 'char'.  Similar to
      `string.split()`, but faster.  If the optional arguments
      'start' and 'end' are used, only the slice 's[start:end]'
      is operated on.
	  返回一个被‘char’分割的列表。和`string.split()`类似，但是速度
	  更快。如果使用了可选参数‘start’和‘end’，则只操作
	  切片‘s[start:end]’。

      SEE ALSO, `string.split()`, `mx.TextTools.setsplit()`
	  参见`string.split()`, `mx.TextTools.setsplit()`

  mx.TextTools.collapse(s, sep=' ')
      Return a string with normalized whitespace.  This is
      equivalent to 'string.join(string.split(s),sep)', but
      faster.
	  返回一个经过规格化空白的字符串。这个和
	  ‘string.join(string.split(s),sep)’是等价的，
	  但是速度更快。

      >>> from mx.TextTools import collapse
      >>> collapse('this and   that','-')
      'this-and-that'

      SEE ALSO, `string.join()`, `string.split()`
	  参见`string.join()`， `string.split()`

  mx.TextTools.countlines(s)
      Returns the number of lines in 's' in a platform-portable
      way.  Lines may end with CR (Mac-style), LF (Unix-style),
      or CRLF (DOS-style), including a mixture of these.
	  将‘s’中的众多行以一种平台可移植的方式返回。这些行
	  可以以CR（Mac风格），LF（Unix风格）或者CRLF（DOS风格）结尾，
	  也可以是将它们混合起来。

      SEE ALSO, `FILE.readlines()`, `mx.TextTools.splitlines()`
	  参见`FILE.readlines()`, `mx.TextTools.splitlines()`

  mx.TextTools.find(s, search_obj, [start, [,end]])
      Return the position of the first match of 'search_obj'
      against 's'.  If the optional arguments 'start' and 'end'
      are used, only the slice 's[start:end]' is considered.
      This function is identical to the search object method of
      the same name; the syntax is just slightly different.  The
      following are synonyms:
	  返回‘s’中‘search_obj’第一次匹配的位置。如果使用了可选参数
	  ‘start’和‘end’，将只考虑切片‘s[start:end]’。此函数和搜索对象
	  的同名方法是一样的；只是语法略有不同。下列代码是同义词：
## lwl：syntax 翻译需要统一，语法还是句法？

      #*------ Function and method versions of find() ---------#
      #*------ find()的函数和方法版本 ---------#
      from mx.TextTools import BMS, find
      s = 'some string with a pattern in it'
      pos1 = find(s, BMS('pat'))
      pos2 = BMS('pat').find(s)

      SEE ALSO, `string.find()`, `mx.TextTools.BMS.find()`
	  参见`string.find()`， `mx.TextTools.BMS.find()`

  mx.TextTools.findall(s, search_obj [,start [,end]])
      Return as slices -every- match of 'search_obj' against
      's'.  If the optional arguments 'start' and 'end' are used,
      only the slice 's[start:end]' is considered. This function
      is identical to the search object method of the same name;
      the syntax is just slightly different.  The following are
      synonyms:
	  返回‘s’中每个‘search_obj’的-每个-匹配的切片。
	  如果使用了可选参数‘start’和‘end’，只考虑‘s[start:end]’。
	  此函数和搜索对象的同名函数是一样的；只是语法略有不同。
	  下列代码是同义词：

      #*------ Function and method versions of findall() ------#
      from mx.TextTools import BMS, findall
      s = 'some string with a pattern in it'
      pos1 = findall(s, BMS('pat'))
      pos2 = BMS('pat').findall(s)

      SEE ALSO, `mx.TextTools.find()`, `mx.TextTools.BMS.findall()`
	  参见`mx.TextTools.find()`， `mx.TextTools.BMS.findall()`

  mx.TextTools.hex2str(hexstr)
      Returns a string based on the hex-encoded string 'hexstr'.
	  返回一个字符串，它是基于十六进制编码的字符串‘hexstr’的。

      >>> from mx.TextTools import hex2str, str2hex
      >>> str2hex('abc')
      '616263'
      >>> hex2str('616263')
      'abc'

      SEE ALSO, `mx.TextTools.str2hex()`
	  参见`mx.TextTools.str2hex()`

  mx.TextTools.is_whitespace(s [,start [,end]])
      Returns a Boolean value indicating whether 's[start:end]'
      contains only whitespace characters.  'start' and 'end' are
      optional, and will default to '0' and 'len(s)',
      respectively.
	  返回一个布尔值，显示‘s[start:end]’是否只包含空白符。
	  ‘start’和‘end’是可选的，默认分别为‘0’和‘len(s)’。

  mx.TextTools.isascii(s)
      Returns a Boolean value indicating whether 's' contains
      only ASCII characters.
	  返回一个布尔值，显示‘s’是否只包含ASCII字符。

  mx.TextTools.join(joinlist [,sep='' [,start [,end]]])
      Return a string composed of slices from other strings.
      'joinlist' is a sequence of tuples of the form
      '(s, start, end, ...)' each indicating the source string
      and offsets for the utilized slice.  Negative offsets do
      not behave like Python slice offsets and should not be
      used.  If a 'joinlist' item tuple contains extra entries,
      they are ignored, but are permissible.
	  返回一个字符，它由其他字符串的切片组成。‘joinlist’是一个
	  ‘(s, start, end, ...)’形式的元组序列组成，每个元组
	  显示了使用的切片的源字符串和位移。负位移的行为和Python切片
	  的不同，不应该使用。如果‘joinlist’元组包括多余的条目，
	  它们将被忽略，但这是允许的。

      If the optional argument 'sep' is specified, a delimiter
      between each joined slice is added.  If 'start' and 'end'
      are specified, only 'joinlist[start:end]' is utilized in
      the joining.
	  如果指定了可选参数‘sep’，拼接的切片之间都将加上一个分隔符。
	  如果指定了‘start’和‘end’，在拼接中将只使用‘joinlist[start:end]’。

      >>> from mx.TextTools import join
      >>> s = 'Spam and eggs for breakfast'
      >>> t = 'This and that for lunch'
      >>> jl = [(s, 0, 4), (s, 9, 13), (t, 0, 4), (t, 9, 13)]
      >>> join(jl, '/', 1, 4)
      '/eggs/This/that'

      SEE ALSO, `string.join()`
	  参见`string.join()`

  mx.TextTools.lower(s)
      Return a string with any uppercase letters converted to
      lowercase.  Functionally identical to `string.lower()`, but
      much faster.
	  返回一个字符串，其中的大写字母已经转换成小写字母。
	  和`string.lower()`在功能上是一样的，但是速度更快。

      SEE ALSO, `string.lower()`, `mx.TextTools.upper()`
	  参见`string.lower()`， `mx.TextTools.upper()`

  mx.TextTools.prefix(s, prefixes [,start [,stop [,translate]]])
      Return the first prefix in the tuple 'prefixes' that
      matches the end of 's'.  If 'start' and 'end'
      are specified, only operate on the slice 's[start:end]'.
      Return 'None' if no prefix matches.
	  返回元组‘prefixes’中匹配‘s’末尾的第一个前缀。
	  如果指定了‘start’和‘end’，将只操作切片‘s[start:end]’。
	  如果没有前缀匹配，返回‘None’。

      If a 'translate' argument is given, the searched string is
      translated during the search.  This is equivalent to
      transforming the string with `string.translate()` prior to
      searching it.
	  如果给出一个‘translate’参数，在搜索中，字符串将被翻译。
	  这个和在搜索之前使用`string.translate()`来转换字符串是等价的。

      >>> from mx.TextTools import prefix
      >>> prefix('spam and eggs', ('spam','and','eggs'))
      'spam'

      SEE ALSO, `mx.TextTools.suffix()`
	  参见按`mx.TextTools.suffix()`

  mx.TextTools.multireplace(s ,replacements [,start [,stop]])
      Replace multiple nonoverlapping slices in 's' with string
      values. 'replacements' must be list of tuples of the form
      '(new, left, right)'.  Indexing is always relative to 's',
      even if an earlier replacement changes the length of the
      result.  If 'start' and 'end' are specified, only operate
      on the slice 's[start:end]'.
	  使用字符串值来替换‘s’中多个不互相覆盖的切片。
	  ‘replacements’必须是‘(new, left, right)’形式的元组列表。
	  下标总是和‘s’相关的，就算前面的替换已经改变了结构的长度。
	  如果指定了‘start’和‘end’，将只操作切片‘s[start:end]’。

      >>> from mx.TextTools import findall, multireplace
      >>> s = 'spam, bacon, sausage, and spam'
      >>> repls = [('X',l,r) for l,r in findall(s, 'spam')]
      >>> multireplace(s, repls)
      'X, bacon, sausage, and X'
      >>> repls
      [('X', 0, 4), ('X', 26, 30)]

  mx.TextTools.replace(s, old, new [,start [,stop]])
      Return a string where the pattern matched by search object
      'old' is replaced by string 'new'.  If 'start' and 'end'
      are specified, only operate on the slice 's[start:end]'.
      This function is much faster than `string.replace()`, since
      a search object is used in the search aspect.
	  返回一个字符串，其中被搜索对象‘old’匹配到的模式，将被字符串‘new’
	  替换。如果指定了‘start’和‘end’，将只操作切片‘s[start:end]’。
	  此函数比`string.replace()`快的多，因为在搜索方面使用了一个
	  搜索对象。

      >>> from mx.TextTools import replace, BMS
      >>> s = 'spam, bacon, sausage, and spam'
      >>> spam = BMS('spam')
      >>> replace(s, spam, 'eggs')
      'eggs, bacon, sausage, and eggs'
      >>> replace(s, spam, 'eggs', 5)
      ' bacon, sausage, and eggs'

      SEE ALSO, `string.replace()`, `mx.TextTools.BMS`
	  参见`string.replace()`， `mx.TextTools.BMS`

  mx.TextTools.setfind(s, set [,start [,end]])
      Find the first occurence of any character in 'set'.  If
      'start' is specified, look only in 's[start:]'; if 'end'
      is specified, look only in 's[start:end]'.  The argument
      'set' must be a set.
	  寻找第一次出现的‘set’中的任何字符。
	  如果指定了‘start’，将只在‘s[start:]’中寻找；
	  如果还指定了‘end’，将只在‘s[start:end]’中寻找。
	  参数‘set’必须是个集合。

      >>> from mx.TextTools import *
      >>> s = 'spam and eggs'
      >>> vowel = set('aeiou')
      >>> setfind(s, vowel)
      2
      >>> setfind(s, vowel, 7, 10)
      9

      SEE ALSO, `mx.TextTools.set()`
	  参见`mx.TextTools.set()`

  mx.TextTools.setsplit(s, set [,start [,stop]])
      Split 's' into substrings divided at any characters in
      'set'.  If 'start' is specified, create a list of
      substrings of 's[start:]'; if 'end' is specified, use
      's[start:end]'. The argument 'set' must be a set.
	  使用‘set’中的所有字符将‘s’分割成子字符串。
	  如果指定了‘start’，将只在‘s[start:]’中寻找；
	  如果还指定了‘end’，将只在‘s[start:end]’中寻找。
	  参数‘set’必须是个集合。
	  
      SEE ALSO, `string.split()`, `mx.TextTools.set()`,
                `mx.TextTools.setsplitx()`
	  参见`string.split()`, `mx.TextTools.set()`,
                `mx.TextTools.setsplitx()`

  mx.TextTools.setsplitx(text,set[,start=0,stop=len(text)])
      Split 's' into substrings divided at any characters in
      'set'.  Include the split characters in the returned list.
      Adjacent characters in 'set' are returned in the same list
      element.  If 'start' is specified, create a list of
      substrings of 's[start:]'; if 'end' is specified, use
      's[start:end]'. The argument 'set' must be a set.
	  使用‘set’中的所有字符将‘s’分割成子字符串。
	  在返回列表中包含了分割字符。‘set’中的邻近字符在同一个
	  列表元素中返回。
	  如果指定了‘start’，将只在‘s[start:]’中寻找；
	  如果还指定了‘end’，将只在‘s[start:end]’中寻找。
	  参数‘set’必须是个集合。
	  
      >>> s = 'do you like spam'
      >>> setsplit(s, vowel)
      ['d', ' y', ' l', 'k', ' sp', 'm']
      >>> setsplitx(s, vowel)
      ['d', 'o', ' y', 'ou', ' l', 'i', 'k', 'e', ' sp', 'a', 'm']

      SEE ALSO, `string.split()`, `mx.TextTools.set()`,
                `mx.TextTools.setsplit()`
      参见 `string.split()`， `mx.TextTools.set()`，
                `mx.TextTools.setsplit()`

  mx.TextTools.splitat(s, char, [n=1 [,start [end]]])
      Return a 2-element tuple that divides 's' around the 'n'th
      occurence of 'char'.  If 'start' and 'end' are specified,
      only operate on the slice 's[start:end]'.
	  将‘s’在第‘n’个‘char’处分割，置于一个二元素的元组中返回。
	  如果指定了‘start’和‘end’，在拼接中将只使用切片‘s[start:end]’。

      >>> from mx.TextTools import splitat
      >>> s = 'spam, bacon, sausage, and spam'
      >>> splitat(s, 'a', 3)
      ('spam, bacon, s', 'usage, and spam')
      >>> splitat(s, 'a', 3, 5, 20)
      (' bacon, saus', 'ge')

  mx.TextTools.splitlines(s)
      Return a list of lines in 's'.  Line-ending combinations
      for Mac, PC, and Unix platforms are recognized in any
      combination, which makes this function more portable than
      is 'string.split(s,"\n")' or `FILE.readlines()`.
	  返回一个列表，其中是‘s’中的行。任何Mac、PC和Unix平台的
	  行尾符之间的混合都可以识别，这个使得此函数比
	  ‘string.split(s,"\n")’或者`FILE.readlines()`更具有可移植性。

      SEE ALSO, `string.split()`, `FILE.readlines()`,
                `mx.TextTools.setsplit()`,
                `mx.TextTools.countlines()`
      参见 `string.split()`， `FILE.readlines()`，
                `mx.TextTools.setsplit()`，
                `mx.TextTools.countlines()`

  mx.TextTools.splitwords(s)
      Return a list of whitespace-separated words in 's'.
      Equivalent to 'string.split(s)'.
	  返回一个列表，其中是使用空白将‘s’分割后的单词。
	  和‘string.split(s)’是等价的。

      SEE ALSO, `string.split()`
	  参见按`string.split()`

  mx.TextTools.str2hex(s)
      Returns a hexadecimal representation of a string.  For
      Python 2.0+, this is equivalent to 's.encode("hex")'.
	  返回字符串的十六进制表示。对于Python 2.0以后版本，
	  这个和‘s.encode("hex")’是等价的。

      SEE ALSO, `"".encode()`, `mx.TextTools.hex2str()`
	  参见`"".encode()`， `mx.TextTools.hex2str()`

  mx.TextTools.suffix(s, suffixes [,start [,stop [,translate]]])
      Return the first suffix in the tuple 'suffixes' that
      matches the end of 's'.  If 'start' and 'end'
      are specified, only operate on the slice 's[start:end]'.
      Return 'None' if no suffix matches.
	  返回元组‘suffix’中匹配‘s’末尾的第一个后缀。
	  如果指定了‘start’和‘end’，将只操作切片‘s[start:end]’。
	  如果没有前缀匹配，返回‘None’。
	  
      If a 'translate' argument is given, the searched string is
      translated during the search.  This is equivalent to
      transforming the string with `string.translate()` prior to
      searching it.
	  如果给出一个‘translate’参数，在搜索中，字符串将被翻译。
	  这个和在搜索之前使用`string.translate()`来转换字符串是等价的。

      >>> from mx.TextTools import suffix
      >>> suffix('spam and eggs', ('spam','and','eggs'))
      'eggs'

      SEE ALSO, `mx.TextTools.prefix()`
	  参见`mx.TextTools.prefix()`

  mx.TextTools.upper(s)
      Return a string with any lowercase letters converted to
      uppercase.  Functionally identical to `string.upper()`, but
      much faster.
	  返回一个字符串，其中的小写字母已经转换成大写字母。
	  和‘string.upper()’在功能上是一样的，但是速度更快。

      SEE ALSO, `string.upper()`, `mx.TextTools.lower()`
	  参见`string.upper()`， `mx.TextTools.lower()`


  TOPIC -- High-Level EBNF Parsing
  主题 -- 高级EBNF解析
  --------------------------------------------------------------------

  =================================================================
    MODULE -- SimpleParse : A Parser Generator for mx.TextTools
	模块 -- SimpleParse ：针对mx.TextTools的解析器生成器
  =================================================================

  [SimpleParse] is an interesting tool.  To use this module, you
  need to have the [mx.TextTools] module installed.  While there
  is nothing you can do with [SimpleParse] that cannot be done
  with [mx.TextTools] by itself, [SimpleParse] is often much
  easier to work with.   There exist other modules to provide
  higher-level APIs for [mx.TextTools]; I find [SimpleParse]
  to be the most useful of these, and the only one that this
  book will present.  The examples in this section were written
  against [SimpleParse] version 1.0, but the documentation is
  updated to include new features of 2.0.  Version 2.0 is fully
  backward compatible with existing [SimpleParse] code.
  [SimpleParse]是个有趣的工具。你需要先安装[mx.TextTools]模块才能
  使用它。你用[SimpleParse]能做的事情都能用[mx.TextTools]本身完成，
  所以使用[SimpleParse]经常简单的多。也有其他模块为[mx.TextTools]提供
  高层API；我发现[SimpleParse]是其中最有用的一个，而它也是本书中
  唯一讲述的那个。本节中的例子使用[SimpleParse]版本1.0写成，但是
  文档已经更新，包括了2.0中的新特性。版本2.0和已经存在的[SimpleParse]
  代码是完全向下兼容的。

  [SimpleParse] substitutes an EBNF-style grammar for the
  low-level state matching language of [mx.TextTools] tag tables.
  Or more accurately, [SimpleParse] is a tool for generating tag
  tables based on friendlier and higher-level EBNF grammars.  In
  principle, [SimpleParse] lets you access and modify tag tables
  before passing them to `mx.TextTools.tag()`.  But in practice,
  you usually want to stick wholly with [SimpleParse]'s EBNF
  variant when your processing is amenable to a grammatical
  description of the text format.
  [SimpleParse]用EBNF风格的语法来代替[mx.TextTools]标签表的
  低层的状态机。或者更准确地说，[SimpleParse]是个基于更友好
  ，更高级的EBNF语法的工具，用于产生标签表。原则上，
  [SimpleParse]允许你在将标签表传入到`mx.TextTools.tag()`之前
  就可以存取和修改它们。但在实际运用中，当你的处理有义务
  对文本格式进行语法描述时，你通常需要完全
  坚持使用[SimpleParse]的EBNF变量。

  An application based on [SimpleParse] has two main aspects.
  The first aspect is the grammar that defines the structure of a
  processed text.  The second aspect is the traversal and use of
  a generated [mx.TextTools] taglist.  [SimpleParse] 2.0 adds
  facilities for the traversal aspect, but taglists present a
  data structure that is quite easy to work with in any case.
  The tree-walking tools in [SimpleParse] 2.0 are not covered
  here, but the examples given in the discussion of
  [mx.TextTools] illustrate such traversal.
  基于[SimpleParse]的程序有两个主要方面。第一个方面是定义
  处理文本的结构的语法。第二个是对生成的[mx.TextTools]标签列表的遍历和使用。 
  ## traversal？？横断？ 遍历？
  [SimpleParse]2.0添加了针对遍历的工具，但是表示数据结构的
  标签列表在任何情况下都是很容易操作的。这儿没有涉及
  [SimpleParse]2.0中的tree-walking工具，但[mx.TextTools]
  的讨论中给出的例子展示了此类的遍历。

  EXAMPLE: Marking up smart ASCII (Redux)
  例子：标记智能ASCII（Redux）
  --------------------------------------------------------------------

  Elsewhere in this book, applications to process the smart ASCII
  format are also presented. Appendix D lists the 'Txt2Html'
  utility, which uses a combination of a state machine for parsing
  paragraphs and regular expressions for identifying inline markup.
  A functionally similar example was given in the discussion of
  [mx.TextTools], where a complex and compound tag table was
  developed to recognize inline markup elements. Using
  [SimpleParse] and an EBNF grammar is yet another way to perform
  the same sort of processing. Comparing the several styles will
  highlight a number of advantages that [SimpleParse] has--its
  grammars are clear and concise, and applications built around it
  can be extremely fast.
  在本书的其他地方也讲述了处理智能ASCII格式的程序。附录D列出了
  ‘Txt2Html’的实用工具，它混合使用了状态机和正则表达式，前者解析
  段落，后者识别行内标识。在[mx.TextTools]的讨论中也给出了一个
  功能上相似的例子，它开发了一个复合标签表来识别行内标识元素。
  使用[SimpleParse]和高级EBNF语法，是实现同类处理的另外一种方法。
  对于多种风格的比较可以突出[SimpleParse]拥有的若干个好处--它的
  语法清晰而简洁，围绕其建立的程序特别快。

  The application 'simpleTypography.py' is quite simple; most of
  the work of programming it lies in creating a grammar to
  describe smart ASCII. EBNF grammars are almost self-explanatory
  to read, but designing one -does- require a bit of thought and
  testing:
  ‘simpleTypography.py’程序相当简单；绝大部分编程工作都是创建语法来
  描述智能ASCII。EBNF语法几乎都是不解自明的，但是真要设计一个
  -确实-需要一点思考和测试：

      #------------------ typography.def ----------------------#
      para           := (plain / markup)+
      plain          := (word / whitespace / punctuation)+
      <whitespace>   := [ \t\r\n]+
      <alphanums>    := [a-zA-Z0-9]+
      <word>         := alphanums, (wordpunct, alphanums)*, contraction?
      <wordpunct>    := [-_]
      <contraction>  := "'", ('am'/'clock'/'d'/'ll'/'m'/'re'/'s'/'t'/'ve')
      markup         := emph / strong / module / code / title
      emph           := '-', plain, '-'
      strong         := '*', plain, '*'
      module         := '[', plain, ']'
      code           := "'", plain, "'"
      title          := '_', plain, '_'
      <punctuation>  := (safepunct / mdash)
      <mdash>        := '--'
      <safepunct>    := [!@#$%^&()+=|\{}:;<>,.?/"]

  This grammar is almost exactly the way you would describe the
  smart ASCII language verbally, which is a nice sort of clarity. A
  paragraph consist of some plaintext and some marked-up text.
  Plain text consists of some collection of words, whitespace, and
  punctuation. Marked-up text might be emphasized, or strongly
  emphasized, or module names, and so on. Strongly emphasized text
  is surrounded by asterisks. And so on. A couple of features like
  just what a "word" really is, or just what a contraction can end
  with, take a bit of thought, but the syntax of EBNF doesn't get
  in the way.
  上述语法几乎正是你将要如何口头描述智能ASCII的，这可以使得概念清晰。
  段落包括一些普通文本，以及一些标识文本。
  普通文本包括一些单词的集合，空白，标点符号。
  标识文本可以是强调，强烈强调，或者模块名字，等等。
  强烈强调文本前后有星号。等等。
  一些特性，例如到底什么是一个“单词”，或者缩写能以什么结尾，
  都需要小小思考一下，但是EBNF的语法不会妨碍。

  Notice that some declarations have their left side surrounded in
  angle brackets. Those productions will not be written to the
  taglist--this is the same as using 'None' as a 'tagobj' in an
  [mx.Texttools] tag table. Of course, if a production is not
  written to the taglist, then its children cannot be, either. By
  omitting some productions from the resultant taglist, a simpler
  data structure is produced (with only those elements that
  interest us).
  注意到一些声明的左边有尖括号。这些产品将不会写到标签列表中--这
  和在[mx.TextTools]标签表以‘None’作为‘tagobj’是一样的。当然，
  如果一个产品没有被写入标签列表，它的子孙也不能被写入。
  通过从结果标签列表中忽略一些产品，可以产生一个更简单的数据结构
  （其中只有我们感兴趣的元素）。

  In contrast to the grammar above, the same sort of rules can be
  described even more tersely using regular expressions. This is
  what the 'Txt2Html' version of the smart ASCII markup program
  does. But this terseness is much harder to write and harder still
  to tweak later. The [re] code below expresses largely (but not
  precisely) the same set of rules:
  为了与上面的语法对比，同样的规则使用正则表达式可以更精炼地描述。
  这正是‘Txt2Html’版本的智能ASCII标识程序的所作所为。
  但是这样的简洁导致编写更加困难，将来的调整也更加困难。
  下面的[re]代码很大程度地（但是不准确）表达了同样的一套规则：

      #-------- Python regexes for smart ASCII markup ---------#
      # [module] names
      re_mods =   r"""([\(\s'/">]|^)\[(.*?)\]([<\s\.\),:;'"?!/-])"""
      # *strongly emphasize* words
      re_strong = r"""([\(\s'/"]|^)\*(.*?)\*([\s\.\),:;'"?!/-])"""
      # -emphasize- words
      re_emph =   r"""([\(\s'/"]|^)-(.*?)-([\s\.\),:;'"?!/])"""
      # _Book Title_ citations
      re_title =  r"""([\(\s'/"]|^)_(.*?)_([\s\.\),:;'"?!/-])"""
      # 'Function()' names
      re_funcs =  r"""([\(\s/"]|^)'(.*?)'([\s\.\),:;"?!/-])"""

  If you discover or invent some slightly new variant of the
  language, it is -a lot- easier to play with the EBNF grammar than
  with those regular expressions. Moreover, using
  [SimpleParse]--and therefore [mx.TextTools]--will generally be
  even faster in performing the manipulations of the patterns.
  如果你发现或者发明此语言的一个略新品种，使用EBNF语法比
  使用正则表达式会-容易得多-。而且，使用[SimpleParse]-也就是
  使用[mx.TextTools]-在操纵模式时候，通常会快很多。

  GENERATING AND USING A TAGLIST:
  生成并使用一个标签列表：

  For 'simpleTypography.py', I put the actual grammar in a separate
  file. For most purposes, this is a good organization to use.
  Changing the grammar is usually a different sort of task than
  changing the application logic, and the files reflect this. But
  the grammar is just read as a string, so in principle you could
  include it in the main application (or even dynamically generate
  it in some way).
  对于‘simpleTypography.py’，我把实际的语法放在一个单独的文件里面。
  对于绝大部分用途而言，这样的组织都很好用。改变语法和改变程序逻辑
  相比，通常是个不同种类的任务，这些文件可以反映这些。但是
  语法只是作为字符串读入，理论上，你可以将其包括在主要程序中（或者
  以某种方式动态生成）。

  Let us look at the entire--compact--tagging application:
  让我们来看看这个完全--简洁--的标签程序：

      #---------------- simpleTypography.py -------------------#
      from sys import stdin, stdout, stderr
      from simpleparse import generator
      from mx.TextTools import TextTools
      from typo_html import codes
      from pprint import pprint

      src = stdin.read()
      decl = open('typography.def').read()
      parser = generator.buildParser(decl).parserbyname('para')
      taglist = TextTools.tag(src, parser)
      pprint(taglist, stderr)

      for tag, beg, end, parts in taglist[1]:
          if tag == 'plain':
              stdout.write(src[beg:end])
          elif tag == 'markup':
              markup = parts[0]
              mtag, mbeg, mend = markup[:3]
              start, stop = codes.get(mtag, ('<!-- unknown -->',
                                             '<!-- /unknown -->'))
              stdout.write(start + src[mbeg+1:mend-1] + stop)
          else:
              raise TypeError, "Top level tagging should be plain/markup"

  With version 2.0 of [SimpleParse], you may use a somewhat more
  convenient API to create a taglist:
  使用2.0版本的[SimpleParse]，你可以使用某种程度上更方便的API来
  创建标签列表：

      #*------------- simpleTypography20.py -------------------#
      from simpleparse.parser import Parser
      parser = Parser(open('typography.def').read(), 'para')
      taglist = parser.parse(src)

  Here is what it does. First read in the grammar and create an
  [mx.TextTools] parser from the grammar. The generated parser is
  similar to the tag table that is found in the hand-written
  'mxTypography.py' module discussed earlier (but without the
  human-friendly comments and structure). Next, apply the tag
  table/parser to the input source to create a taglist. Finally,
  loop through the taglist, and emit some new marked-up text. The
  loop could, of course, do anything else desired with each
  production encountered.
  下面是它做的内容。首先读入语法，利用其创建一个[mx.TextTools]解析器。
  产生的解析器和前面讨论的手写‘mxTypography.py’模块中的标签表相似
  (只是没有对人类友好的注释和结构)。接着，将标签表/解析器应用
  到输入源，创建一个标签列表。最后，循环遍标签列表，发出一些
  新的标识文本。当然这个循环可以在遇到产品时，另外做一些想要
  的事情。
  ## lwl: emit？？

  For the particular grammar used for smart ASCII, everything in
  the source text is expected to fall into either a "plain"
  production or a "markup" production.  Therefore, it suffices to
  loop across a single level in the taglist (except when we look
  exactly one level lower for the specific markup production,
  such as "title"). But a more free-form grammar--such as occurs
  for most programming languages--could easily recursively
  descend into the taglist and look for production names at
  every level.  For example, if the grammar were to allow nested
  markup codes, this recursive style would probably be used.
  Readers might enjoy the exercise of figuring out how to adjust
  the grammar (hint:  Remember that productions are allowed to be
  mutually recursive).
  对于智能ASCII使用的特殊语法，源文件的所有内容都被期望是
  一个“普通”产品或者一个“标识”产品。因此，对于在一个单层中循环，
  它是足够了（除非我们检查特定标识产品的下一层，例如“标题”）。
  但是更加自由形式的语法--例如绝大部分编程语言的发生--可以  ##lwl： occurs？？
  轻松回归降解到标签列表，并检查每一层的产品名字。
  例如，如果语法允许嵌套标识代码，很有可能使用这种回归风格。
  读者也许会对如何调整语法的练习有兴趣（提示：记住产品允许进行
  双向回归）。

  The particular markup codes that go to the output live in yet
  another file for organizational, not essential, reasons. A little
  trick of using a dictionary as a 'switch' statement is used here
  (although the 'otherwise' case remains too narrow in the
  example). The idea behind this organization is that we might in
  the future want to create multiple "output format" files for,
  say, HTML, DocBook, LaTeX, or others. The particular markup file
  used for the example just looks like:
  那些用于输出的特殊标识代码存在于另外一个文件中，因为组织的原因，
  尽管不是必要的。此处利用了一个小技巧，就是把字典作为一个‘开关’声明
  （尽管此例中的‘otherwise’情况剩余太窄了）。如此组织后面的原因是，
  将来我们可能想要创建多个“输出格式”文件，例如HTML，DocBook，LaTeX，
  或者其他。这个例子中使用的特别标识文件看上去如下：

      #-------------------- typo_html.py ----------------------#
      codes = \
      { 'emph'    : ('<em>', '</em>'),
        'strong'  : ('<strong>', '</strong>'),
        'module'  : ('<em><code>', '</code></em>'),
        'code'    : ('<code>', '</code>'),
        'title'   : ('<cite>', '</cite>'),
      }

  Extending this to other output formats is straightforward.
  很容易就可将此输出格式扩展到其他输出格式。


  THE TAGLIST AND THE OUTPUT:
  标签列表和输出：

  The -tag table- generated from the grammar in 'typography.def'
  is surprisingly complicated and includes numerous recursions.
  Only the exceptionally brave of heart will want to attempt
  manual--let alone automated--modification of tag tables created
  by [SimpleParse].  Fortunately, an average user need not even
  look at these tags, but simply -use- them, as is done with
  'parser' in 'simpleTypography.py'.
  从‘typography.def’中语法产生的-标签表-复杂得让人吃惊，
  其中包括了无数递归。只有拥有异常勇敢的心，才想要尝试手工
  -先不管自动地-修改[SimpleParse]创建的标签表。幸运的是，
  一般用户甚至不需要看到这些标签，只是-使用-它们，如同
  ‘simpleTypography.py’中的‘parser’所为。

  The -taglist- produced by applying a grammar, in contrast, can be
  remarkably simple.  Here is a run of 'simpleTypography.py'
  against a small input file:
  相反，通过语法应用而产生的-标签列表-，简单非凡。这儿是
  ‘simpleTypography.py’对于一个小的输入文件运行后的结果：

      #*----------- Test run of simpleTypography.py -----------#
      % python simpleTypography.py < p.txt > p.html
      (1,
       [('plain', 0, 15, []),
        ('markup', 15, 27, [('emph', 15, 27, [('plain', 16, 26, [])])]),
        ('plain', 27, 42, []),
        ('markup', 42, 51, [('module', 42, 51, [('plain', 43, 50, [])])]),
        ('plain', 51, 55, []),
        ('markup', 55, 70, [('code', 55, 70, [('plain', 56, 69, [])])]),
        ('plain', 70, 90, []),
        ('markup', 90, 96, [('strong', 90, 96, [('plain', 91, 95, [])])]),
        ('plain', 96, 132, []),
        ('markup', 132, 145, [('title', 132, 145, [('plain',133,144,[])])]),
        ('plain', 145, 174, [])],
       174)

  Most productions that were satisfied are not written into the
  taglist, because they are not needed for the application.  You
  can control this aspect simply by defining productions with or
  without angle braces on the left side of their declaration.
  The output looks like you would expect:
  绝大部分满意的产品都没有被写入标签列表，因为程序不需要它们。
  控制这个方面，你只需要定义产品的时候，规定它们声明的左边
  是否有尖括号。输出应该如你所愿：

      #*-------- Input and output of simpleTypography run -----#
      % cat p.txt
      Some words are -in italics-, others
      name [modules] or 'command lines'.
      Still others are *bold* -- that's how
      it goes. Maybe some _book titles_.
      And some in-fixed dashes.
      % cat p.html
      Some words are <em>in italics</em>, others
      name <em><code>modules</code></em> or <code>command lines</code>.
      Still others are <strong>bold</strong> -- that's how
      it goes. Maybe some <cite>book titles</cite>.
      And some in-fixed dashes.

  -*-

  GRAMMAR:
  语法：

  The language of [SimpleParse] grammars is itself defined using a
  [SimpleParse] EBNF-style grammar.  In principle, you could
  refine the language [SimpleParse] uses by changing the
  variable 'declaration' in 'bootstrap.py', or
  'simpleparsegrammar.py' in recent versions.  For example,
  extended regular expressions, W3C XML Schemas, and some
  EBNF variants allow integer occurrence quantification.  To
  specify that three to seven 'foo' tokens occur, you could use
  the following declaration in [SimpleParse]:
  [SimpleParse]使用[SimpleParse]的EBNF风格的语法定义了它
  本身这个语言。理论上，你可以重新定义[SimpleParse]语言，
  只要修改‘bootstrap.py’中的‘declaration’变量，或者
  最近版本中的‘simpleparsegrammar.py’。例如，扩展正则表达式，
  W3C的XML方案，以及一些允许整数次现身量词的EBNF变量。
  为了指定出现了三到七个‘foo’记号，你可以在[SimpleParse]中
  使用下列声明：

      #*---------------- 3-to-7 quantification ----------------#
      foos := foo, foo, foo, foo?, foo?, foo?, foo?

  Hypothetically, it might be more elegant to write something
  like:
  假设一下，这样写岂不是更加优雅：

      #*------------- Improved 3-to-7 quantification ----------#
      foos := foo{3,7}

  In practice, only someone developing a custom/enhanced parsing
  module would have any reason to fiddle quite so deeply; "normal"
  programmers should use the particular EBNF variant defined by
  default. Nonetheless, taking a look at 'simpleparse/bootstrap.py'
  can be illustrative in understanding the module.
  在实际运用中，只有那些开发自定义或者增强版本的解析模块的人，
  才有理由钻研至此；“正常的”程序员应该使用默认的那些EBNF变量。
  尽管如此，看一眼‘simpleparse/bootstrap.py’可以帮助理解此模块。

  DECLARATION PATTERNS:
  声明模式：

  A [SimpleParse] grammar consists of a set of one or more
  declarations.  Each declaration generally occurs on a line by
  itself; within a line, horizontal whitespace may be used as
  desired to improve readability.  A common strategy is to align
  the right sides of declarations, but any other use of internal
  whitespace is acceptable. A declaration contains a term,
  followed by the assignment symbol ":=", followed by a
  definition.  An end-of-line comment may be added to a
  declaration, following an unquoted "#" (just as in Python).
  [SimpleParse]语法由一套声明组成，数量从一个到多个。
  一行通常只有一个声明；可以在一行内故意使用水平空白来提高可读性。
  一个常见的策略是对齐声明的右边，但是也可以把空白在内部作为其他用途。
  一个声明包含一个术语，后面跟着一个分配符“:=”，再后面是定义。
  声明后面也可以加上行尾注释，只需要跟上不在括号内的“#”（如同Python中一样）。

  In contrast to most imperative-style programming, the
  declarations within a grammar may occur in any order.  When a
  parser generator's '.parserbyname()' method is called, the "top
  level" of the grammar is given as an argument.  The documented
  API for [SimpleParse] uses a call of the form:
  作为和绝大部分命令风格编程的比较，语法的声明可以以任何顺序出现。
  当调用一个解析器产生器的‘.parserbyname()’方法时，语法的“顶层”
  会作为参数给出。有记录的[SimpleParse]的AP使用以下形式的调用：

      #*--------- How to create a SimpleParse parser ----------#
      #*--------- 如何创建SimpleParse解析器 ----------#
      from simpleparse import generator
      parser = generator.buildParser(decl).parserbyname('toplevel')
      from mx.TextTools import TextTools
      taglist = TextTools.tag(src, parser)

  Under [SimpleParse] 2.0, you may simplify this to:
  在[SimpleParse]2.0下，你可以简化其为：

      #*------- How to create a SimpleParse v2 parser ---------#
      #*------- 如何创建SimpleParse v2的解析器 ---------#
      from simpleparse.parser import Parser
      parser = Parser(decl,'toplevel')
      taglist = parser.parse(src)

  A left side term may be surrounded by angle brackets ("'<'",
  "'>'") to prevent that production from being written into a
  taglist produced by `mx.TextTools.tag()`. This is called an
  "unreported" production. Other than in relation to the final
  taglist, an unreported production acts just like a reported one.
  Either type of term may be used on the right sides of other
  productions in the same manner (without angle brackets when
  occurring on the right side).
  左边的术语可以用尖括号围住（“<”，“>”），以阻止该产品被写入
  `mx.TextTools.tag()`产生的标签列表。这叫做“未报告的”产品。
  除了和最终标签列表的关系以外，未报告的产品的举动就和报告的产品
  一样。随便哪种术语都可以用同种方式用在其他产品的右边（当出现
  在右边时，没有尖括号）。

  In [SimpleParse] 2.0 you may also use reversed angle brackets
  to report the children of a production, but not the production
  itself.  As with the standard angle brackets, the production
  functions normally in matching inputs; it differs only in
  produced taglist.  For example:
  在[SimpleParse]2.0中，你可以使用颠倒顺序的尖括号来报告一个产品
  的子孙，但其不是产品本身。和使用标准尖括号一样，此产品在匹配
  输入时功能正常；它的区别仅在于产生的标签表。例如：

      #*--------- Reported and Unreported productions ---------#
      #*--------- 报告的和未报告的产品 ---------#
      PRODUCTIONS               TAGLIST
	  产品                      标签列表
      --------------------------------------------------------
      a   := (b,c)              ('a', l, r, [
      b   := (d,e)                  ('b', l, r, [...]),
      c   := (f,g)                  ('c', l, r, [...]) ] )
      --------------------------------------------------------
      a   := (b,c)              ('a', l, r, [
      <b> := (d,e)                  # no b, and no children
      c   := (f,g)                  ('c', l, r, [...]) ] )
      --------------------------------------------------------
      # Only in 2.0+            ('a', l, r, [
      a   := (b,c)                  # no b, but raise children
      >b< := (d,e)                  ('d', l, r, [...]),
      c   := (f,g)                  ('e', l, r, [...]),
                                    ('c', l, r, [...]) ] )
      --------------------------------------------------------

  The remainder of the documentation of the [SimpleParse] module
  covers elements that may occur on the right sides of
  declarations.  In addition to the elements listed, a term from
  another production may occur anywhere any element may.  Terms
  may thus stand in mutually recursive relations to one another.
  [SimpleParse]模块文档的剩余部分涵盖了那些可能出现在声明右边的元素。
  除了列出的元素，其他产品的元素也可能出现在任何元素的位置。
  因此术语们可以彼此之间有回归的关系。

  LITERALS:
  字面：  ##lwl:如何翻译？

  Literal string
  字面字符串
      A string enclosed in single quotes matches the exact string
      quoted.  Python escaping may be used for the characters
      '\a', '\b', '\f', '\n', '\r', '\t', and '\v', and octal
      escapes of one to three digits may used.  To include a
      literal backslash, it should be escaped as '\\'.
	  单括号围住的字符串，会原样匹配引用的字符串。可以使用
	  Python转义符来表示字符‘\a’，‘\b’，‘\f’，‘\n’，‘\r’，‘\t’以及‘\v’，
	  另外一位到三位的八进制转义也可以使用。要包括一个字面的反斜杠，
	  它应该转义为‘\\’。

      #*----------- A character literal declaration -----------#
      #*----------- 一个字面字符声明 -----------#
      foo := "bar"

  Character class: "[", "]"
  字符类：“[”，“]”
      Specify a set of characters that may occur at a position.
      The list of allowable characters may be enumerated with no
      delimiter.  A range of characters may be indicated with a
      dash ("-").  Multiple ranges are allowed within a class.
	  指定某位置可能出现的字符集合。此表列举允许的字符，中间没有
	  分隔符。某个范围的字符也可以通过破折号（“-”）来显示。
	  一个类中允许多个范围。

      To include a "]" character in a character class, make it
      the first character.  Similarly, a literal "-" character
      must be either the first (after the optional "]"
      character) or the last character.
	  为了在某字符类中包含“]”字符，将其放在第一个。相似地，
	  一个字面的“-”字符，要么是第一个（在可选的“]”字符后），
	  要么是最后一个字符。

      #*----------- A character class declaration -------------#
      #*----------- 一个字符类声明 -------------#
      varchar := [a-zA-Z_0-9]

  QUANTIFIERS:
  量词：

  Universal quantifier: "*"
  通用量词：“*”
      Match zero or more occurrences of the preceding expression.
      Quantification has a higher precedence than alternation or
      sequencing; grouping may be used to clarify quantification
      scope as well.
	  匹配前面表达式的零次或者更多次的出现。量词比轮换或者先后顺序
	  有更高的优先级；也可以使用组合来澄清量词的范围。

      #*---------- Universal quantifier declaration -----------#
      #*---------- 通用量词声明 -----------#
      any_Xs     := "X"*
      any_digits := [0-9]*

  Existential quantifier: "+"
  存在量词：“+”
      Match one or more occurrences of the preceding expression.
      Quantification has a higher precedence than alternation or
      sequencing; grouping may be used to clarify quantification
      scope as well.
	  匹配前面表达式的一次或者更多次出现。量词比轮换或者先后顺序
	  有更高的优先级；也可以使用组合来澄清量词的范围。

      #*---------- Existential quantifier declaration ---------#
      #*---------- 存在量词声明 ---------#
      some_Xs     := "X"+
      some_digits := [0-9]+

  Potentiality quantifier: "?"
  可能性量词：“?”
      Match at most one occurrence of the preceding expression.
      Quantification has a higher precedence than alternation or
      sequencing; grouping may be used to clarify quantification
      scope as well.
	  匹配前面的表达式，顶多出现一次。量词比轮换或者先后顺序
	  有更高的优先级；也可以使用组合来澄清量词的范围。

      #*---------- Existential quantifier declaration ---------#
##lwl：原作者错误
      #*---------- 可能性量词声明 ---------#
      maybe_Xs     := "X"?
      maybe_digits := [0-9]?

  Lookahead quantifier: "?"
  前观量词：“?”
      In [SimpleParse] 2.0+, you may place a question mark
      -before- a pattern to assert that it occurs, but should not
      actually claim the pattern.  As with regular expressions,
      you can create either positive or negative lookahead
      assertions.
	  在[SimpleParse]2.0中，你可以在一个模式前放置一个问号，
	  以断言其会出现，但是不会实际取得该模式。正如正则表达式一样，
	  你可以创建或者正的，或者负的前观断言。

      #*---------- Lookahead quantifier declaration -----------#
      #*---------- 前观量词声明 -----------#
      next_is_Xs         := ?"X"
      next_is_not_digits := ?-[0-9]

  Error on Failure: "!"
  失败时的错误：“!”
  ##lwl：翻译不好
      In [SimpleParse] 2.0+, you may cause a descriptive
      exception to be raised when a production does not match,
      rather than merely stopping parsing at that point.
	  在[SimpleParse]2.0以后，当一个产品不匹配时，
	  你可以提出一个描述性的例外，不用在该点停止解析。

      #*------------ Error on failure declaration ------------#
      #*------------ 失败时的错误声明 ------------#
      require_Xs   := "X"!
      require_code := ([A-Z]+, [0-9])!
      contraction := "'", ('clock'/'d'/'ll'/'m'/'re'/'s'/'t'/'ve')!

      For example, modifying the 'contraction' production from
      the prior discussion could require that every apostrophe is
      followed by an ending.  Since this doesn't hold, you might
      see an exception like:
	  例如，修改前面讨论的‘contraction’产品，要求每个省略号后面
	  就是结尾。因为这个无效，你可能看见一个例外，就像：

      #*--------- Example error-on-failure exception -----------#
      #*--------- 失败时的错误例外之例子 -----------#
      % python typo2.py < p.txt
      Traceback (most recent call last):
      [...]
      simpleparse.error.ParserSyntaxError:  ParserSyntaxError:
      Failed parsing production "contraction" @pos 84 (~line 1:29).
      Expected syntax: ('clock'/'d'/'ll'/'m'/'re'/'s'/'t'/'ve')
      Got text: 'command lines'.  Still others are *bold*

  STRUCTURES:
  结构：

  Alternation operator: "/"
  轮换符：“/”
      Match the first pattern possible from several alternatives.
      This operator allows any of a list of patterns to match.
      Some EBNF-style parsers will match the -longest- possible
      pattern, but [SimpleParse] more simply matches the -first-
      possible pattern.  For example:
	  从若干个可选项目中，匹配第一个可能的模式。
	  此操作符允许匹配列表中的任何一个模式。一些EBNF风格的解析将
	  匹配-最长的-那个模式，但是[SimpleParse]只是匹配-第一个-可能
	  的模式。例如：

      >>> from mx.TextTools import tag
      >>> from simpleparse import generator
      >>> decl = '''
      ... short := "foo", " "*
      ... long  := "foobar", " "*
      ... sl    := (short / long)*
      ... ls    := (long / short)*
      ... '''
      >>> parser = generator.buildParser(decl).parserbyname('sl')
      >>> tag('foo foobar foo bar', parser)[1]
      [('short', 0, 4, []), ('short', 4, 7, [])]
      >>> parser = generator.buildParser(decl).parserbyname('ls')
      >>> tag('foo foobar foo bar', parser)[1]
      [('short', 0, 4, []), ('long', 4, 11, []), ('short', 11, 15, [])]

  Sequence operator: ","
  顺序操作符：“，”
      Match the first pattern followed by the second pattern
      (followed by the third pattern, if present, ...).  Whenever
      a definition needs several elements in a specific order, the
      comma sequence operator is used.
	  匹配第一个模式，再接着匹配第二个模式（如果出现的话，继续
	  匹配第三个模式，...）。任何时候，一个定义如果需要若干个元素
	  以某特定顺序出现，就应该使用逗号顺序操作符。

      #*-------------- Sequence declaration --------------------#
      #*-------------- 顺序声明 --------------------#
      term := someterm, [0-9]*, "X"+, (otherterm, stillother)?

  Negation operator: "-"
  否定操作符：“-”
      Match anything that the next pattern -does not- match.  The
      pattern negated can be either a simple term or a compound
      expression.
	  匹配下一模式-不-匹配的任何东西。否定的模式可以是单个的术语，
	  也可以是一个混合表达式。

      #*------------ Declarations with negation ---------------#
      #*------------ 带否定的声明 ---------------#
      nonletters   := -[a-zA-Z]
      nonfoo       := -foo
      notfoobarbaz := -(foo, bar, baz)

      An expression modified by the negation operator is very
      similar conceptually to a regular expression with a
      negative lookahead assertion.  For example:
	  使用否定操作符修改的表达式在概念上，和正则表达式中的
	  否定性前观断言，是极其相似的。例如：

      >>> from mx.TextTools import tag
      >>> from simpleparse import generator
      >>> decl = '''not_initfoo := [ \t]*, -"foo", [a-zA-Z ]+'''
      >>> p = generator.buildParser(decl).parserbyname('not_initfoo')
      >>> tag('  foobar and baz', p)     # no match
      (0, [], 0)
      >>> tag('  bar, foo and baz', p)   # match on part
      (1, [], 5)
      >>> tag('  bar foo and baz', p)    # match on all
      (1, [], 17)

  Grouping operators: "(", ")"
  分组操作符：“(”，“)”
      Parentheses surrounding any pattern turn that pattern into
      an expression (possibly within a larger expression).
      Quantifiers and operators refer to the immediately
      adjacent expression, if one is defined, otherwise to the
      adjacent literal string, character class, or term.
	  模式周围的括号使得该模式成为一个表达式（可能是在一个更大
	  的表达式内）。如果定义了量词和操作符，它们作用于最邻近的
	  表达式，否则作用于邻近字面字符串，字符类，或者术语。

      >>> from mx.TextTools import tag
      >>> from simpleparse import generator
      >>> decl = '''
      ... foo      := "foo"
      ... bar      := "bar"
      ... foo_bars := foo, bar+
      ... foobars  := (foo, bar)+
      ... '''
      >>> p1 = generator.buildParser(decl).parserbyname('foobars')
      >>> p2 = generator.buildParser(decl).parserbyname('foo_bars')
      >>> tag('foobarfoobar', p1)
      (1, [('foo', 0, 3, []), ('bar', 3, 6, []),
           ('foo', 6, 9, []), ('bar', 9, 12, [])], 12)
      >>> tag('foobarfoobar', p2)
      (1, [('foo', 0, 3, []), ('bar', 3, 6, [])], 6)
      >>> tag('foobarbarbar', p1)
      (1, [('foo', 0, 3, []), ('bar', 3, 6, [])], 6)
      >>> tag('foobarbarbar', p2)
      (1, [('foo', 0, 3, []), ('bar', 3, 6, []),
           ('bar', 6, 9, []), ('bar', 9, 12, [])], 12)

  USEFUL PRODUCTIONS:
  有用产品：

  In version 2.0+, [SimpleParse] includes a number of useful
  productions that may be included in your grammars.  See the
  examples and documentation that accompany [SimpleParse] for
  details on the many included productions and their usage.
  从版本2.0起，[SimpleParse]包含了许多有用的产品，你可以在语法中
  包含它们。关于这么多包含的产品以及它们的用法，你可以在
  随同[SimpleParse]一起的例子和文档中找到各种细节。

  The included productions, at the time of this writing, fall
  into the categories below:
  这些包含的产品，在这次写作的时间，分为以下几类：

  simpleparse.common.calendar_names
      Locale specific names of months, and days of week, including
      abbreviated forms.
	  本地化特定的月份名字，一周七天的名字，包括缩写形式。
	  ##lwl: days of week, 如何翻译？

  simpleparse.common.chartypes
      Locale-specific categories of characters, such as digits,
      uppercase, octdigits, punctuation, locale_decimal_point,
      and so on.
	  本地化特定的字符种类，例如数字，大写字母，octdigits，标点，
	  本地化的小数点，等等。

  simpleparse.common.comments
      Productions to match comments in a variety of programming
      languages, such as hash ('#') end-of-line comments (Python,
      Bash, Perl, etc.); C paired comments ('/* comment */'); and
      others.
	  此产品用来匹配多种编程语言中的注释，例如井号（‘#’），行尾注释
	  （Python，Bash，Perl，等等）；C的成对的注释（‘/* 注释 */’）；
	  以及其他。

  simpleparse.common.iso_date
      Productions for strictly conformant ISO date and time
      formats.
	  此产品被用来严格制造ISO日期和时间格式。

  simpleparse.common.iso_date_loose
      Productions for ISO date and time formats with some leeway
      as to common variants in formatting.
	  此产品用来产生ISO日期和时间格式，它们和常见变量相比在格式上
	  有些回旋余地。

  simpleparse.common.numbers
      Productions for common numeric formats, such as integers,
      floats, hex numbers, binary numbers, and so on.
	  常见数字格式的产品，例如整数，浮点数，十六进制数，二进制数，
	  等等。

  simpleparse.common.phonetics
      Productions to match phonetically spelled words.
      Currently, the US military style of "alpha, bravo, charlie,
      ..." spelling is the only style supported (with some leeway
      in word spellings).
	  此产品用来匹配按照发音拼写的单词。目前，只支持美国军方风格
	  的“alpha, bravo, charlie, 等等”（在单词拼写中，有一些回旋余地）。

  simpleparse.common.strings
      Productions to match quoted strings as used in various
      programming languages.
	  此产品用来匹配多种编程语言中引用的字符串。

  simpleparse.common.timezone_names
      Productions to match descriptions of timezones, as you
      might find in email headers or other data/time fields.
	  此产品用来匹配时区的描述，正如你可以在电子邮件的头部
	  或者其他数据/时间域中所能发现。

  GOTCHAS:
  逮到你啦：

  There are a couple of problems that can easily arise in
  constructed [SimpleParse] grammars.  If you are having problems
  in your application, keep a careful eye out for these issues:
  在建造[SimpleParse]语法时，很容易就出现一些问题。
  如果你的程序有难关，请注意以下问题：

  1.  Bad recursion.  You might fairly naturally construct a
      pattern of the form:
	  坏递归。你可以很自然的建造一个如下格式的模式：

      #*----------- Bad recursion in SimpleParse --------------#
      #*----------- SimpleParse中的坏递归 --------------#
      a := b, a?

      Unfortunately, if a long string of 'b' rules are matched,
      the repeated recognition can either exceed the C-stack's
      recursion limit, or consume inordinate amounts of memory to
      construct nested tuples.  Use an alternate pattern like:
	  不幸的是，如果匹配到符合‘b’规则的一个很长的字符串，
	  不断重复的识别工作将会，要么超过C堆栈的递归限制，
	  要么消耗不计其数的内存以建立嵌套的元组。使用一个轮换的模式，
	  例如：

      #*------- Good quantification in SimpleParse ------------#
      #*------- SimpleParse中的好量词 ------------#
      a := b+

      This will grab all the 'b' productions in one tuple
      instead (you could separately parse out each 'b' if
      necessary).
	  而这个会提出一个元组内所有的‘b’产品（你可以分离性的解析出
	  所有的‘b’，只要你愿意）。

  2.  Quantified potentiality. That is a mouthful; consider
      patterns like:
	  定量的潜在性。这个是慢慢一口；思考一下模式，例如：

      #*------ Quantified potentiality in SimpleParse ---------#
      #*------ SimpleParse中定量的潜在性 ---------#
      a := (b? / c)*
      x := (y?, z?)+

      The first alternate 'b?' in the first--and both 'y?' and
      'z?' in the second--are happy to match zero characters
      (if a 'b' or 'y' or 'z' do not occur at the current
      position).  When you match "as many as possible" zero-width
      patterns, you get into an infinite loop. Unfortunately, the
      pattern is not always simple; it might not be 'b' that is
      qualified as potential, but rather 'b' productions (or the
      productions -in- 'b' productions, etc.).
	  第一个轮换‘b?’是第一个--‘y?’和‘z?’两个都是第二个--将会很乐意，
	  如果匹配到零个字符(如果一个‘b’或者‘y’或者‘z’在当前位置没有出现)。
	  当你匹配“尽可能多的”零宽度的模式时，你陷入一个无限循环。
	  不幸的是，模式不会总是简单的，它可能不是‘b’，它被量化为可能，
	  更恰当的是‘b’产品（或者‘b’产品-中的-产品，等等）。

  3.  No backtracking.  Based on working with regular expression,
      you might expect [SimpleParse] productions to use
      backtracking.  They do not.  For example:
	  没有后向跟踪。根据你对正则表达式的经验，你可能期待
	  [SimpleParse]产品使用后向跟踪。但它们没有。例如：
	  ##lwl：backtracking， 需要统一

      #*---------------- No backtracking ----------------------#
      #*---------------- 没有后向追踪 ----------------------#
      a := ((b/c)*, b)

      If this were a regular expression, it would match a string
      of 'b' productions, then back up one to match the final
      'b'.  As a [SimpleParse] production, this definition can
      never match. If any 'b' productions occur, they will be
      claimed by '(b/c)*', leaving nothing for the final 'b' to
      grab.
	  如果这是个正则表达式，它将匹配‘b’产品的一个字符串，然后回退
	  一步，以匹配最终的‘b’。作为一个[SimpleParse]产品，这个定义
	  将永远不会匹配。如果任何‘b’产品出现，它们将被‘(b/c)*’认领，
	  而最后的‘b’将抓取不到任何东西。


  TOPIC -- High-Level Programmatic Parsing
  主题 -- 高级纲领式解析
  ##lwl：Programmatic如何翻译？
  --------------------------------------------------------------------

  =================================================================
    MODULE -- PLY : Python Lex-Yacc
	模块 -- PLY ： Python Lex-Yacc
  =================================================================

  One module that I considered covering to round out this chapter
  is John Aycock's [Spark] module. This module is both widely used
  in the Python community and extremely powerful. However, I
  believe that the audience of this book is better served by
  working with David Beazley's [PLY] module than with the older
  [Spark] module.
  我曾经考虑以John Aycock的[Spark]模块来结束本章，它在Python社区
  中被广泛使用，功能极其强大。但是，我相信供应给本书的观众以
  David Beazley的[PLY]模块，会比老一点的[Spark]模块更好一点。

  In the documentation accompanying [PLY], Beazley consciously
  acknowledges the influence of [Spark] on his design and
  development.  While the [PLY] module is far from being a clone
  of [Spark]--the APIs are significantly different--there is a
  very similar -feeling- to working with each module.  Both
  modules require a very different style of programming and style
  of thinking than do [mx.TextTools], [SimpleParse], or the state
  machines discussed earlier in this chapter.  In particular,
  both [PLY] and [Spark] make heavy use of Python introspection
  to create underlying state machines out of specially named
  variables and functions.
  在随同[PLY]的文档中，Beazley特意承认了[Spark]对其设计和开发的影响。
  尽管[PLY]模块远不是[Spark]的克隆--API是明显的不同--和每个模块工作
  时，让人有一种非常熟悉的-感觉-。两个模块都要求一种编程和思考风格，
  它和[mx.TextTools]的，[SimpleParse]的，或者本章早先讲过的状态机的，
  都有不同。特别地，[PLY]和[Spark]都大量使用了Python的自省，
  以从特别命名的变量和函数中，创建底层的状态机。

  Within an overall similarity, [PLY] has two main advantages over
  [Spark] in a text processing context. The first, and probably
  greatest, advantage [PLY] has is its far greater speed. Although
  [PLY] has implemented some rather clever optimizations--such as
  preconstruction of state tables for repeated runs--the main
  speed difference lies in the fact that [PLY] uses a far faster,
  albeit slightly less powerful, parsing algorithm. For text
  processing applications (as opposed to compiler development),
  [PLY]'s LR parsing is plenty powerful for almost any requirement.
  两者总体上很相似，但是[PLY]在文本处理情景下，相比[Spark]有两个
  主要优点。第一个，大概也是最大的，[PLY]的优点是速度快得多。
  尽管[PLY]已经实现一些相当聪明的优化--例如为了重复的允许，预先
  创建状态表--主要的速度差异存在于一个事实中，那就是[PLY]使用一个
  虽然功能上少少逊色但是快的多的解析算法。对于文本处理程序
  （和编译器开发相反），[PLY]的LR解析对于绝大部分要求都是足够强大了。

  A second advantage [PLY] has over every other Python parsing
  library that I am aware of is a flexible and fine-grained error
  reporting and error correction facility. Again, in a text
  processing context, this is particularly important. For compiling
  a programming language, it is generally reasonable to allow
  compilation to fail in the case of even small errors. But for
  processing a text file full of data fields and structures, you
  usually want to be somewhat tolerant of minor formatting errors;
  getting as much data as possible from a text automatically is
  frequently the preferred approach. [PLY] does an excellent job of
  handling "allowable" error conditions gracefully.
  我所知道的[PLY]的第二个击败其他Python解析库的优点是，它有一个
  灵活的，细致的错误报告和错误修正工具。再一次，在文本处理情境中，
  这个是特别地重要。对于编译一个编程语言，就算发生的错误很小，
  让编译过程出错也是合理的。但是在处理一个文本文件，其中全是数据域和
  结构，你通常想在某种程度上容忍较小的格式错误；从文本中自动获取
  尽可能多的数据是首选的方法。[PLY]在优雅地处理“可允许的”错误条件方面
  做得很优秀。

  [PLY] consists of two modules: a lexer/tokenizer named 'lex.py',
  and a parser named 'yacc.py'. The choice of names is taken from
  the popular C-oriented tools 'lex' and 'yacc', and the behavior
  is correspondingly similar. Parsing with [PLY] usually consists
  of the two steps that were discussed at the beginning of this
  chapter: (1) Divide the input string into a set of
  nonoverlapping tokens using 'lex.py'. (2) Generate a parse tree
  from the series of tokens using 'yacc.py'.
  [PLY]由两个模块组成：一个名叫‘lex.py’的lexer/特征标识器，
  还有一个叫‘yacc.py’的解析器。名字的来源是流行的基于C的工具‘lex’
  和‘yacc’，而行为也相应地比较相似。使用[PLY]解析通常由两步组成,
  它们在本章开始已经讨论过了：(1) 使用‘lex.py’将输入字符串分解成
  一套互不覆盖的特征。(2) 使用‘yacc.py’从这一系列的特征生成一个解析树。

  When processing text with [PLY], it is possible to attach "action
  code" to any lexing or parsing event. Depending on application
  requirements, this is potentially much more powerful than
  [SimpleParse]. For example, each time a specific token is
  encountered during lexing, you can modify the stored token
  according to whatever rule you wish, or even trigger an entirely
  different application action. Likewise, during parsing, each time
  a node of a parse tree is constructed, the node can be modified
  and/or other actions can be taken. In contrast, [SimpleParse]
  simply delivers a completed parse tree (called a "taglist") that
  must be traversed separately.  However, while [SimpleParse]
  does not provide the fine-tunable event control that [PLY]
  does, [SimpleParse] offers a higher-level and cleaner grammar
  language--the choice between the two modules is full of pros and
  cons.
  当用[PLY]处理文本时，可以在任何词法分析或解析事情上，附上“行动代码”。
  根据不同程序的要求，这个模块在潜力上比[SimpleParse]强大得多。
  例如，每次在词法分析时遇到某个指定特征，你可以根据任何你希望的规则，
  修改储存的特征，或者甚至触发一个完全不同的程序动作。同样地，
  在解析过程中，每次建立解析树的的一个节点，该节点可以修改，
  并且/或者采取其他动作。作为对比，[SimpleParse]只是投递一个完成的
  解析树（称作“标签列表”），它必须被单独遍历。[SimpleParse]
  不提供[PLY]的可调整的事件控制，但是，
  [SimpleParse]提供了更高层次和更整洁的语法语言--选择任何一个模块
  都是有得有失。
  ##lwl：最后一句需要调整

  EXAMPLE: Marking up smart ASCII (yet again)
  例子： 标识智能ASCII（再来一次）
  --------------------------------------------------------------------

  This chapter has returned several times to applications for
  processing smart ASCII: a state machine in Appendix D; a
  functionally similar example using [mx.TextTools]; an EBNF
  grammar with [SimpleParse]. This email-like markup format is not
  in itself all that important, but it presents just enough
  complications to make for a good comparison between programming
  techniques and libraries. In many ways, an application using
  [PLY] is similar to the [SimpleParse] version above--both use
  grammars and parsing strategies.
  本章已经返回几次到程序来处理智能ASCII：附录D中的状态机；
  一个使用[mx.TextTools]的例子，功能相似；[SimpleParse]的
  一个EBNF语法。这个类似于电子邮件标记格式的格式，本身不是
  全部都很重要的，但它呈现了足够的复杂度，以在编程技术和库之间
  作出良好的比较。在很多方面，使用[PLY]的程序和上述的[SimpleParse]
  版本相似--在使用语法和解析策略两个方面。


  GENERATING A TOKEN LIST:
  产生一个特征表：

  The first step in most [PLY] applications is the creation of a
  token stream. Tokens are identified by a series of regular
  expressions attached to special pattern names of the form
  't_RULENAME'. By convention, the [PLY] token types are in all
  caps. In the simple case, a regular expression string is merely
  assigned to a variable. If action code is desired when a token is
  recognized, the rule name is defined as a function, with the
  regular expression string as its docstring; passed to the
  function is a 'LexToken' object (with attributes '.value',
  '.type', and '.lineno'), which may be modified and returned. The
  pattern is clear in practice:
  大部分[PLY]程序的第一步，是创建一个特征流。特征是由一系列的
  正则表达式识别，后者被附于名字为形式‘t_RULENAME’的特殊模式上。
  习惯上，[PLY]特质类型都是大写。简单的情况下，正则表达式字符串
  仅仅被赋值给一个变量。如果当特征被识别时，想要行动代码，
  规则名字被定义为一个函数，而正则表达式字符串就是它的docstring；
  传递给函数的是一个叫‘LexToken’的对象（它有属性‘.value’，
  ‘.type’和‘lineno’），它可以修改后再返回。
  在实际运用中，模式是清楚的：

      #------------------- wordscanner.py ---------------------#
      # List of token names.  This is always required.
      # 特征名字的列表。这个总是需要的
      tokens = [ 'ALPHANUMS','SAFEPUNCT','BRACKET','ASTERISK',
                'UNDERSCORE','APOSTROPHE','DASH' ]

      # Regular expression rules for simple tokens
      # 对于简单特征的正则表达式规则
      t_ALPHANUMS     = r"[a-zA-Z0-9]+"
      t_SAFEPUNCT     = r'[!@#$%^&()+=|\{}:;<>,.?/"]+'
      t_BRACKET       = r'[][]'
      t_ASTERISK      = r'[*]'
      t_UNDERSCORE    = r'_'
      t_APOSTROPHE    = r"'"
      t_DASH          = r'-'

      # Regular expression rules with action code
      # 带行动代码的正则表达式
      def t_newline(t):
          r"\n+"
          t.lineno += len(t.value)

      # Special case (faster) ignored characters
      # 特殊情况 （更快） 忽略字符
      t_ignore = " \t\r"

      # Error handling rule
      # 错误处理规则
      def t_error(t):
          sys.stderr.write("Illegal character '%s' (%s)\n"
                           % (t.value[0], t.lineno))
          t.skip(1)

      import lex, sys
      def stdin2tokens():
          lex.input(sys.stdin.read())     # Give the lexer some input
          toklst = []                     # Tokenize
          while 1:
              t = lex.token()
              if not t: break   # No more input
              toklst.append(t)
          return toklst

      if __name__=='__main__':
          lex.lex()                       # Build the lexer
          for t in stdin2tokens():
              print '%s<%s>' % (t.value.ljust(15), t.type)

  You are required to list the token types you wish to recognize,
  using the 'tokens' variable.  Each such token, and any special
  patterns that are not returned as tokens, is defined either as
  a variable or as a function.  After that, you just initialize
  the lexer, read a string, and pull tokens off sequentially.
  Let us look at some results:
  你被要求使用‘tokens’变量，列出你想要识别的特征类别。每个这样的
  特征，以及任何特别模式，它们没有作为特征返回，都被定义为，要么是
  变量，要么是函数。这以后，你只要初始化词法分析器，读入一个字符串，
  连续地取走特征。让我们看一些结果：

      #*--------- Tokenization of smart ASCII text ------------#
      #*--------- 智能ASCII文本的特征标识 ------------#
      % cat p.txt
      -Itals-, [modname]--let's add ~ underscored var_name.
      % python wordscanner.py < p.txt
      Illegal character '~' (1)
      -              <DASH>
      Itals          <ALPHANUMS>
      -              <DASH>
      ,              <SAFEPUNCT>
      [              <BRACKET>
      modname        <ALPHANUMS>
      ]              <BRACKET>
      -              <DASH>
      -              <DASH>
      let            <ALPHANUMS>
      '              <APOSTROPHE>
      s              <ALPHANUMS>
      add            <ALPHANUMS>
      underscored    <ALPHANUMS>
      var            <ALPHANUMS>
      _              <UNDERSCORE>
      name           <ALPHANUMS>
      .              <SAFEPUNCT>

  The output illustrates several features. For one thing, we have
  successfully tagged each nondiscarded substring as constituting
  some token type. Notice also that the unrecognized tilde
  character is handled gracefully by being omitted from the token
  list--you could do something different if desired, of course.
  Whitespace is discarded as insignificant by this tokenizer--the
  special 't_ignore' variable quickly ignores a set of characters,
  and the 't_newline()' function contains some extra code to
  maintain the line number during processing.
  输出展示了若干特性。首先，在建立一些特征类别时，
  我们已经成功地标识每个没有抛弃的子字符串。还需要注意的是，没有识别
  的波浪符则被从特征列表中忽略，这样处理比较优雅--当然，如果你想的话，
  可以做一些不同的事情。这个特征标识器将空白视作无意义的而抛弃了--
  ‘t_ignore’变量可以快速忽略一套字符，‘t_newline（）’函数包含了一些
  额外的代码，以在处理过程中维持行号。

  The simple tokenizer above has some problems, however.  Dashes
  can be used either in an m-dash or to mark italicized phrases;
  apostrophes can be either part of a contraction or a marker for
  a function name; underscores can occur both to mark titles and
  within variable names.  Readers who have used [Spark] will know
  of its capability to enhance a lexer or parser by
  inheritance; [PLY] cannot do that, but it can utilize Python
  namespaces to achieve almost exactly the same effect:
  但是，上面简单的特征标识器也有一些问题。破折号可以用在一个
  m-dash中，也可以用于标记斜体字；单引号可以用作缩写的一部分，
  也可以用作标识函数名；下划线可以用于标识标题，也可以出现在
  变量内部。使用过[Spark]的读者会知道它可以通过继承来增强
  ## lwl: 前面有apostrophe的翻译出错，记得修改

      #----------------- wordplusscanner.py -------------------#
      "Enhanced word/markup tokenization"
      from wordscanner import *
      tokens.extend(['CONTRACTION','MDASH','WORDPUNCT'])
      t_CONTRACTION   = r"(?<=[a-zA-Z])'(am|clock|d|ll|m|re|s|t|ve)"
      t_WORDPUNCT     = r'(?<=[a-zA-Z0-9])[-_](?=[a-zA-Z0-9])'
      def t_MDASH(t): # Use HTML style mdash
          r'--'
          t.value = '&mdash;'
          return t

      if __name__=='__main__':
          lex.lex()                       # Build the lexer # 建立词法分析器
          for t in stdin2tokens():
              print '%s<%s>' % (t.value.ljust(15), t.type)

  Although the tokenization produced by 'wordscanner.py' would
  work with the right choice of grammar rules, producing more
  specific tokens allows us to simplify the grammar accordingly.
  In the case of 't_MDASH()', 'wordplusscanner.py' also modifies
  the token itself as part of recognition:
  尽管‘wordscanner.py’产生的特征可以和正确选择的语法规则一起工作，
  生成更多特定的特征，可以允许我们简化相应的语法。
  在‘t_MDASH()’的情况中，作为识别的一部分，
  ‘wordplusscanner.py’还修改特征本身：

      #*----- Improved tokenization of smart ASCII text -------#
      % python wordplusscanner.py < p.txt
      Illegal character '~' (1)
      -              <DASH>
      Itals          <ALPHANUMS>
      -              <DASH>
      ,              <SAFEPUNCT>
      [              <BRACKET>
      modname        <ALPHANUMS>
      ]              <BRACKET>
      &mdash;        <MDASH>
      let            <ALPHANUMS>
      's             <CONTRACTION>
      add            <ALPHANUMS>
      underscored    <ALPHANUMS>
      var            <ALPHANUMS>
      _              <WORDPUNCT>
      name           <ALPHANUMS>
      .              <SAFEPUNCT>


  *Parsing a token list*
  *解析特征表*

  A parser in [PLY] is defined in almost the same manner as a
  tokenizer. A collection of specially named functions of the form
  'p_rulename()' are defined, each containing an EBNF-style pattern
  to match (or a disjunction of several such patterns). These
  functions receive as argument a 'YaccSlice' object, which is
  list-like in assigning each component of the EBNF declaration to
  an indexed position.
  [PLY]中的解析器拥有和特征标识器几乎一样的定义。一套特别命名为
  ‘p_rulename()’格式的函数被定义了，每个包含了一个EBNF风格的模式
  来匹配（或者分离若干个这样的模式）。这些函数接受一个‘YaccSlice’
  对象作为参数，后者像列表，因为它将EBNF声明的每个成分赋值给
  一个索引后的位置。

  The code within each function should assign a useful value to
  't[0]', derived in some way from 't[1:]'.  If you would like to
  create a parse tree out of the input source, you can define a
  'Node' class of some sort and assign each right-hand rule or
  token as a subnode/leaf of that node; for example:
  每个函数内的代码都应该给‘t[0]’赋上一个有用的值，后者可以从
  ‘t[1:]’中以某种方式推导出。如果你想要从输入源中创建一个解析树，
  你可以定义一个某种类型的‘Node’类，将该节点的子节点/叶子赋给
  右手规则或者特征；例如：

      #*------------ Assigning YaccSlice indices --------------#
      #*------------ 分配YaccSlice下标 --------------#
      def p_rulename(t):
          'rulename : somerule SOMETOKEN otherrule'
          #   ^          ^         ^         ^
          #  t[0]       t[1]      t[2]      t[3]
          t[0] = Node('rulename', t[1:])

  Defining an appropriate 'Node' class is left as an exercise. With
  this approach, the final result would be a traversable tree
  structure.
  如何定义一个恰当的‘Node’类被留给读者作为练习。通过这种方法，
  最终结果将会是一个可遍历树结构。

  It is fairly simple to create a set of rules to combine the
  fairly smart token stream produced by 'wordplusscanner.py'. In
  the sample application, a simpler structure than a parse tree is
  built. 'markupbuilder.py' simply creates a list of matched
  patterns, interspersed with added markup codes. Other data
  structures are possible too, and/or you could simply take some
  action each time a rule is matched (e.g., write to STDOUT).
  很简单就可以创建一套规则，以结合‘wordplusscanner.py’产生的
  那些相当聪明的特征流。在例子程序中，建立了一个比解析树简单的结构。
  ‘markupbuilder.py’只是创建匹配到的模式清单，以增加的标识代码
  作为点缀。其他的数据结构也是可能的，而且/或者当一个规则
  被匹配到时，你可以只采取一些行动（例如，写到标准输出STDOUT）

      #------------------- markupbuilder.py -------------------#
      import yacc
      from wordplusscanner import *

      def p_para(t):
          '''para : para plain
                  | para emph
                  | para strong
                  | para module
                  | para code
                  | para title
                  | plain
                  | emph
                  | strong
                  | module
                  | code
                  | title '''
          try:    t[0] = t[1] + t[2]
          except: t[0] = t[1]

      def p_plain(t):
          '''plain : ALPHANUMS
                   | CONTRACTION
                   | SAFEPUNCT
                   | MDASH
                   | WORDPUNCT
                   | plain plain '''
          try:    t[0] = t[1] + t[2]
          except: t[0] = [t[1]]

      def p_emph(t):
          '''emph : DASH plain DASH'''
          t[0] = ['<i>'] + t[2] + ['</i>']

      def p_strong(t):
          '''strong : ASTERISK plain ASTERISK'''
          t[0] = ['<b>'] + t[2] + ['</b>']

      def p_module(t):
          '''module : BRACKET plain BRACKET'''
          t[0] = ['<em><tt>'] + t[2] + ['</tt></em>']

      def p_code(t):
          '''code : APOSTROPHE plain APOSTROPHE'''
          t[0] = ['<code>'] + t[2] + ['</code>']

      def p_title(t):
          '''title : UNDERSCORE plain UNDERSCORE'''
          t[0] = ['<cite>'] + t[2] + ['</cite>']

      def p_error(t):
          sys.stderr.write('Syntax error at "%s" (%s)\n'
                           % (t.value,t.lineno))

      if __name__=='__main__':
          lex.lex()               # Build the lexer
          yacc.yacc()             # Build the parser
          result = yacc.parse(sys.stdin.read())
          print result

  The output of this script, using the same input as above, is:
  使用和上面同样的输入，此脚本的输出为：

      #*----------- Parse list of smart ASCII text ------------#
      % python markupbuilder.py < p.txt
      Illegal character '~' (1)
      ['<i>', 'Itals', '</i>', ',', '<em><tt>', 'modname',
      '</tt></em>', '&mdash;', 'let', "'s", 'add', 'underscored',
      'var', '_', 'name', '.']

  One thing that is less than ideal in the [PLY] grammar is that
  it has no quantifiers.  In [SimpleParse] or another EBNF
  library, we might give, for example, a 'plain' declaration
  as:
  [PLY]语法中的一个不是很理想的地方就是，它没有量词。例如，
  在[SimpleParse]或者另一个EBNF库中，我们也许可以给出一个‘普通’声明如下：

      #*------------ EBNF style plain declaration -------------#
      #*------------ EBNF风格的普通声明 -------------#
      plain := (ALPHANUMS | CONTRACTION | SAFEPUNCT | MDASH | WORDPUNCT)+

  Quantification can make declarations more direct. But you can
  achieve the same effect by using self-referential rules whose
  left-hand terms also occur on the right-hand side.  This style
  is similar to recursive definitions, for example:
  量词可以使得声明更加直接。但你也可以取得同样效果，只要使用
  自我引用的规则，即左边的术语也出现在右边。这种风格和递归定义
  相似，例如：
  

      #*----------- Recursive para declaration ----------------#
      #*----------- 递归参数定义 ----------------#
      plain : plain plain
            | OTHERSTUFF

  For example, 'markupbuilder.py', above, uses this technique.
  例如，上面的‘markupbuilder.py’，使用了这种技术。

  If a tree structure were generated in this parser, a 'plain'
  node might wind up being a subtree containing lower 'plain'
  nodes (and terminal leaves of 'ALPHANUMS', 'CONTRACTION',
  etc.).  Traversal would need to account for this possibility.
  The flat list structure used simplifies the issue, in this
  case.  A particular 'plain' object might result from the
  concatenation of several smaller lists, but either way it is a
  list by the time another rule includes the object.
  如果使用这种解析器产生的树结构，其中‘普通’的节点可能以一个
  包含更低‘普通’节点的子树结束（以及‘ALPHANUMS’，‘CONTRACTION’
  等终端叶子）。遍历会需要考虑这样的可能性。这个情况里，
  使用的平列表结构简化了这个问题。拼接若干个小列表，可能会导致
  特别的‘普通’对象，但是不论哪种方法，在另一个规则包括此对象时，
  它还是一个列表。

  LEX:
  词法分析：

  A [PLY] lexing module that is intended as support for a parsing
  application must do four things. A lexing module that
  constitutes a stand-alone application must do two additional
  things:
  想要支持解析程序的[PLY]词法分析模块必须做四件事情。
  组成一个独立的程序词法解析程序必须再多做两件事情。
  

  1.  Import the [lex] module:
  1.  导入[lex]模块：

      #*-------------- Importing lex.py -----------------------#
      #*-------------- 导入lex.py -----------------------#
      import lex

  2.  Define a list or tuple variable 'tokens' that contains
      the name of every token type the lexer is allowed to
      produce.  A list may be modified in-place should you wish
      to specialize the lexer in an importing module; for
      example:
  2.  定义一个列表或者元组变量‘tokens’，它包含了词法解析器允许
	  产生的所有特征类别的名字。列表可以就地修改，如果你希望
	  在导入的模块中指定词法分析器的话；例如：

      #*-------------- Defining a 'tokens' list ---------------#
      #*-------------- 定义一个‘特征’列表 ---------------#
      tokens = ['FOO', 'BAR', 'BAZ', 'FLAM']

  3.  Define one or more regular expression patterns matching
      tokens.  Each token type listed in 'tokens' should have a
      corresponding pattern; other patterns may be defined also,
      but the corresponding substrings will not be included in
      the token stream.
  3.  定义一个或更多正则表达式模式以匹配特征。每个‘tokens’中
	  列出的特征类别，都应该有个对应的模式；也可以定义其他模式，
	  但是对应的子字符串不会被包含在特征流之中。

      Token patterns may be defined in one of two ways: (1) By
      assigning a regular expression string to a specially named
      variable. (2) By defining a specially named function whose
      docstring is a regular expression string.  In the latter
      case, "action code" is run when the token is matched.
      In both styles, the token name is preceded by the prefix
      't_'.  If a function is used, it should return the
      'LexToken' object passed to it, possibly after some
      modification, unless you do not wish to include the token
      in the token stream.  For example:
	  特征模式可以两种方式定义：(1) 通过将一个正则表达式字符串
	  赋值给一个特别命名的变量。 (2) 通过定义一个特别命名的函数，
	  它的docstring是一个正则表达式字符串。在后一种情况中，
	  当特征匹配时，运行“action code”。在两种风格中，特征名字都有
	  前缀‘t_’。如果使用了函数，它应该返回传给它的‘LexToken’对象，
	  它很有可能现在已经修改过了，除非你不想将其包含在特征流之中。
	  例如：

      #*-------------- Defining token patterns ----------------#
      #*-------------- 定义特征模式 ----------------#
      t_FOO = r"[Ff][Oo]{1,2}"
      t_BAR = r"[Bb][Aa][Rr]"
      def t_BAZ(t):
          r"([Bb][Aa][Zz])+"
          t.value = 'BAZ'   # canonical caps BAZ
          return t
      def t_FLAM(t):
          r"(FLAM|flam)*"
          # flam's are discarded (no return)

      Tokens passed into a pattern function have three
      attributes: '.type', '.value', and '.lineno'.  '.lineno'
      contains the current line number within the string being
      processed and may be modified to change the reported
      position, even if the token is not returned.  The attribute
      '.value' is normally the string matched by the regular
      expression, but a new string, or a compound value like a
      tuple or instance, may be assigned instead.  The '.type' of
      a 'LexToken', by default, is a string naming the token (the
      same as the part of the function name after the 't_'
      prefix).
	  传递给模式函数的特征有三个属性：‘.type’，‘.value’以及‘.lineno’。
	  ‘.lineno’包含当前处理的字符串所在的行号，可修改以改变报告的位置，
	  就算没有返回特征。属性‘.value’通常是正则表达式匹配到的字符串，
	  但是也可以将新字符串，或者混合值，如元组或实例赋值给其。
	  ‘LexToken’的‘.type’默认是一个字符串，用于命名此特征（与‘t_’前缀后
	  的那部分函数名字相同）。

      There is a special order in which various token patterns
      will be considered.  Depending on the patterns used,
      several patterns could grab the same substring--so it is
      important to allow the desired pattern first claim on a
      substring.   Each pattern defined with a function is
      considered in the order it is defined in the lexer file;
      all patterns defined by assignment to a variable are
      considered -after- every function-defined pattern.
      Patterns defined by variable assignment, however, are not
      considered in the order they are defined, but rather by
      decreasing length.  The purpose of this ordering is to let
      longer patterns match before their subsequences (e.g., "=="
      would be claimed before "=", allowing the former comparison
      operator to match correctly, rather than as sequential
      assignments).
	  各种各样的特征模式会按照某个特殊的顺序进行处理。根据使用的模式，
	  可以若干个模式抓到同样的子字符串--所以允许想要的模式先认领
	  子字符串。同一个函数中的定义的模式是按照它们在词法解析器文件
	  中被定义的顺序；所有通过赋值给变量而定义的模式，在所有函数
	  定义的模式-之后-才被考虑。但是，通过变量赋值定义的模式，
	  并不是按照定义它们的顺序处理，而是根据逐减的长度。
	  这个顺序的目的是让长模式比它的子序列更先匹配到（距离来说，
	  “==”会在“=”之前被认领，使得前一个比较符能正确匹配，而非后面
	  的那个等号）。

      The special variable 't_ignore' may contain a string of
      characters to skip during pattern matching.  These
      characters are skipped more efficiently than is a token
      function that has no return value.  The token name 'ignore'
      is, therefore, reserved and may not be used as a regular
      token (if the all-cap token name convention is followed, it
      assures no such conflict).
	  特殊变量‘t_ignore’可能包含一串模式匹配时要忽略的字符。
	  这样忽略字符比没有返回值的特征函数更有效率。特征名字‘ignore’
	  因此是保留的，不可以作为常规特征使用（如果按照特征名字全部大写
	  的惯例，可以确保没有这样的冲突）。

      The special function 't_error()' may be used to process
      illegal characters.  The '.value' attribute of the
      passed-in 'LexToken' will contain the remainder of the
      string being processed (after the last match).  If you
      want to skip past a problem area (perhaps after taking some
      corrective action in the body of 't_error()'), use the
      '.skip()' method of the passed-in 'LexToken'.
	  特殊函数‘t_error()’可以用来处理非法字符。传入的‘LexToken’的
	  ‘.value’属性将容纳正在处理的字符串（在最后一次匹配后）剩下
	  的部分。如果你想要略过一个问题区域（也许在‘t_error()’中采取
	  一些纠正行动之后），使用传入的‘LexToken’的‘.skip()’方法。

  4.  Build the lexer.  The [lex] module performs a bit of
      namespace magic so that you normally do not need to name
      the built lexer.  Most applications can use just one
      default lexer.  However, if you wish to--or if you need
      multiple lexers in the same application--you may bind a
      built lexer to a name.  For example:
	  打造词法分析器。[lex]模块会表演一点名字空间的魔术，所以
	  你通常不需要命名建造的词法分析器。绝大部分程序可以只使用
	  一个默认的词法分析器。但是，如果你希望的话--或者如果
	  你在同一个程序中，需要多个词法分析器--你可以绑定词法分析器
	  到一个名字。举例如下：

      #*----------------- Building the lexer ------------------#
      #*----------------- 打造词法分析器 ------------------#
      mylexer = lex.lex()   # named lexer  # 有名字的词法分析器
      lex.lex()             # default lexer # 默认词法分析器
      mylexer.input(mytext) # set input for named lexer  # 设定有名字的词法分析器的输入
      lex.input(othertext)  # set input for default lexer # 设定默认词法分析器的输入

  5.  Give the lexer a string to process.  This step is handled
      by the parser when [yacc] is used in conjunction with
      [lex], and nothing need be done explicitly.  For stand-alone
      tokenizers, set the input string using 'lex.input()' (or
      similarly with the '.input()' method of named lexers).
  5.  给词法分析器一个字符串以处理。这一步是当[yacc]和[lex]联合使用时，
	  由解析器处理的，不需要显式做任何事情。对于独立运行的特征标识器，
	  使用‘lex.input()’设定输入字符串（或者，相似地，使用命名词法分析器
	  的‘.input()’方法）。

  6.  Read the token stream (for stand-alone tokenizers) using
      repeated invocation of the default 'lex.token()' function
      or the '.token()' method of a named lexer.  Unfortunately,
      as of version 1.1, [PLY] does not treat the token stream as
      a Python 2.2 iterator/generator.  You can create an
      iterator wrapper with:
  6.  重复调用默认的‘lex.token()’函数或者命名词法分析器的
	  ‘.token()’方法，（为独立运行的特征标识器）读入特征流。
	  不幸的是，在版本1.1中，[PLY]并不把特征流作为Python2.2中的
	  迭代子/generator对待。你可以创建一个迭代子的包装如下：
	  ## generator？

      #*------------ Wrap token stream in iterator ------------#
      #*------------ 将特征流封装在迭代子中 ------------#
      from __future__ import generators
      # ...define the lexer rules, etc...
      def tokeniterator(lexer=lex):
          while 1:
              t = lexer.token()
              if t is None:
                  raise StopIteration
              yield t
      # Loop through the tokens
	  # 循环遍特征
      for t in tokeniterator():
          # ...do something with each token...
          # ...对每个特征做一些工作...

      Without this wrapper, or generally in earlier versions of
      Python, you should use a 'while 1' loop with a break
      condition:
	  没有这个封装，或者在早期版本的Python中，你应该使用一个
	  带跳出条件的‘while 1’循环：

      #*--------- While loop with break token stream ----------#
      # ...define the lexer rules, etc...
      #*--------- 带跳出条件流的while循环 ----------#
      # ...定义词法分析器条件，等等...
      while 1:
          t = lex.token()
          if t is None:   # No more input
              break
          # ...do something with each token...

  YACC:
  YACC：

  A [PLY] parsing module must do five things:
  [PLY]解析模块必须做五件事：

  1.  Import the 'yacc' module:
  1.  导入‘yacc’模块：

      #*-------------- Importing yacc.py ----------------------#
      #*-------------- 导入 yacc.py ----------------------#
      import yacc

  2.  Get a token map from a lexer.  Suppose a lexer module named
      'mylexer.py' includes requirements 1 through 4 in the
      above LEX description.  You would get the token map with:
  2.  从词法分析器取得一个特征地图。假设词法分析器模块命名了一个
	  ‘mylexer.py’，它符合上面LEX描述的1到4个要求。你可以通过以下
	  方式来方式来获得特征地图：

      #*-------------- Importing the lexer --------------------#
      #*-------------- 导入词法分析器 --------------------#
      from mylexer import *

      Given the special naming convention 't_*' used for token
      patterns, the risk of namespace pollution from 'import *'
      is minimal.
	  考虑到‘t_*’这样的命名惯例被用与特征模式，从‘import *’中
	  污染命名空间的危险被减到最小。

      You could also, of course, simply include the necessary
      lexer setup code in the parsing module itself.
	  当然，你还可以在解析模块本身中，只包括必要的词法分析器
	  设置代码。

  3.  Define a collection of grammar rules.  Grammar rules
      are defined in a similar fashion to token functions.
      Specially named functions having a 'p_' prefix contain one
      or more productions and corresponding action code.
      Whenever a production contained in the docstring of a
      'p_*()' function matches, the body of that function runs.
  3.  定义一套语法规则。语法规则的定义类似于特征函数的。
	  特殊命名的函数拥有一个‘p_’前缀，包含一个或更多产品，
	  以及对应的行动代码。任何时候当‘p_*()’函数的docstring包含的
	  产品匹配到，则运行函数体内的代码。

      Productions in [PLY] are described with a simplified EBNF
      notation.  In particular, no quantifiers are available in
      rules; only sequencing and alternation is used (the rest
      must be simulated with recursion and component
      productions).
	  [PLY]中的产品是用一个简化的EBNF记号来描述的。特别地，
	  规则中没有量词；只使用了顺序和轮换（其他的必须用递归
	  和成员产品来模拟）。

      The left side of each rule contains a single rule name.
      Following the rule name is one or more spaces, a colon, and
      an additional one or more spaces.  The right side of a rule
      is everything following this.  The right side of a rule can
      occupy one or more lines; if alternative patterns are
      allowed to fulfill a rule name, each such pattern occurs on
      a new line, following a pipe symbol ("|").  Within each
      right side line, a production is defined by a
      space-separated sequence of terms--which may be either
      tokens generated by the lexer or parser productions.  More
      than one production may be included in the same 'p_*()'
      function, but it is generally more clear to limit each
      function to one production (you are free to create more
      functions). For example:
	  每条规则的左边只包含一个规则名字。规则名字后面是一个或者
	  多个空格，一个冒号，以及额外的一个或多个空格。规则的右边
	  就是后面跟着的所有的东西。规则的右边可以占据一行或多行；
	  如果允许轮换模式来完成规则名字，每个这样的模式出现在一行，
	  前面有一个管道符（“|”）。在每个右边的行中，产品是通过由
	  空格分隔的术语序列来定义的--后者要么是词法分析器产生的特征，
	  要么是解析器产品。同一个‘p_*()’函数可能包括超过一个产品，
	  但是限制每个函数只有一个产品，会显得整洁（你可随意创建更多
	  函数）。例如：

      #*------ Parser function with multiple productions ------#
      #*------ 拥有多个产品的解析器函数 ------#
      def p_rulename(t):
          '''rulename   : foo SPACE bar
                        | foo bar baz
                        | bar SPACE baz
             otherrule  : this that other
                        | this SPACE that '''
      #...action code...
      #...行动代码...

      The argument to each 'p_*()' function is a 'YaccSlice'
      object, which assigns each component of the rule to an
      indexed position.  The left side rule name is index
      position 0, and each term/token on the right side is listed
      thereafter, left to right.  The list-like 'YaccSlice' is
      sized just large enough to contain every term needed; this
      might vary depending on which alternative production is
      fulfilled on a particular call.
	  每个‘p_*()’函数的参数就是一个‘YaccSlice’对象，它将规则
	  的每个成分赋值到一个有索引的位置。左边的规则名字是下标
	  位置0，每个右边的术语/特征被随后列出，从左到右。类似于列表
	  的‘YaccSlice’的尺寸正好足够大到包含每个需要的术语；这个可能
	  变化，因为依赖于在某特定调用中，完成的是哪个轮换产品。

      Empty productions are allowed by [yacc] (matching
      zero-width); you never need more than one empty production
      in a grammar, but this empty production might be a
      component of multiple higher-level productions.  An empty
      production is basically a way of getting around the absence
      of (potentiality) quantification in [PLY]; for example:
	  [yacc]允许空产品（也就是匹配零长度）；在一个语法中，你从不会
	  需要超过一个空产品，但此空产品可能是多个更高层次产品的成分。
	  空产品基本上是用来绕过没有[PLY]（可能性）量词这个缺陷；
	  例如：

      #*------------- Empty productions in yacc.py ------------#
      #*------------- yacc.py中的空产品 ------------#
      def p_empty(t):
          '''empty : '''
          pass
      def p_maybefoo(t):
          '''foo  : FOOTOKEN
                  | empty '''
          t[0] = t[1]
      def p_maybebar(t):
          '''bar  : BARTOKEN
                  | empty '''
          t[0] = t[1]

      If a fulfilled production is used in other productions
      (including itself recursively), the action code should
      assign a meaningful value to index position 0.  This
      position -is- the value of the production.  Moreover what
      is returned by the actual parsing is this position 0 of the
      top-level production.  For example:
	  如果其他产品使用了一个完成的产品（包括递归使用自己），
	  动作代码应该将一个有意义的值赋给下标为0的位置。这个位置
	  -是-产品的值。此外，实际解析返回的是顶层产品的位置。例如：

      #*------------ Assigning YaccSlice indices --------------#
      # Sum N different numbers: "1.0 + 3 + 3.14 + 17"
      #*------------ 给YaccSlice下标夫子 --------------#
      # 对N个不同数字求和: "1.0 + 3 + 3.14 + 17"
      def p_sum(t):
          '''sum : number PLUS number'''
          #   ^      ^      ^    ^
          #  t[0]   t[1]  t[2]  t[3]
          t[0] = t[1] + t[3]
      def p_number(t):
          '''number : BASICNUMBER
                    | sum         '''
          #    ^        ^
          #   t[0]     t[1]
          t[0] = float(t[1])
      # Create the parser and parse some strings
      # 创建解析器，解析一些字符串。
      yacc.yacc()
      print yacc.parse('1.0')

      The example simply assigns a numeric value with every
      production, but it could also assign to position 0 of the
      'YaccSlice' a list, 'Node' object, or some other data
      structure that was useful to higher-level productions.
	  这个例子只是给每个产品赋予一个数字值，但它也可以将一个列表，
	  ‘Node’对象，或者其他数据结构，只要对高层产品有用的，
	  赋给‘YaccSlice’的位置0。

  4.  To build the parser the [yacc] module performs a bit of
      namespace magic so that you normally do not need to name
      the built parser.  Most applications can use just one
      default parser.  However, if you wish to--or if you need
      multiple parsers in the same application--you may bind a
      built parser to a name.  For example:
  4.  为了创建解析器，[yacc]模块表演了一点点名字空间上的魔术，
  	  所以你通常不需要命名创建的解析器。绝大部分程序可以只使用
	  默认解析器。但是，如果你想要--或者如果你在同一个程序中
	  需要多个解析器--你可以绑定一个名字到创建的解析器上.例如:

      #*----------------- Building the lexer ------------------#
      myparser = yacc.yacc()      # named parser
      yacc.yacc()                 # default parser
      r1 = myparser.parse(mytext) # set input for named parser
      r0 = yacc.parse(othertext)  # set input for default parser
      #*----------------- 建立词法分析器 ------------------#
      myparser = yacc.yacc()      # 命名解析器
      yacc.yacc()                 # 默认解析器
      r1 = myparser.parse(mytext) # 设定命名解析器的输入
      r0 = yacc.parse(othertext)  # 设定默认解析器的输入

      When parsers are built, [yacc] will produce diagnostic
      messages if any errors are encountered in the grammar.
	  当建立一个解析器时候,如果语法中遇到任何错误，
	  [yacc]会产生诊断信息。

  5.  Parse an input string.  The lexer is implicitly called
      to get tokens as needed by the grammar rules.  The return
      value of a parsing action can be whatever thing invocation
      of matched rules builds.  It might be an abstract syntax
      tree, if a 'Node' object is used with each parse rule; it
      might be a simple list as in the smart ASCII example; it
      might be a modified string based on concatenations and
      modifications during parsing; or the return value could
      simply be 'None' if parsing was done wholly to trigger side
      effects in parse rules.  In any case, what is returned is
      index position 0 of the root rule's 'LexToken'.
  5.  解析输入字符。词法分析器被隐式调用，以取得语法规则需要的
  	  特征。解析动作的返回值可以是，任何调用匹配到的规则所建立的
	  东西。如果每个解析规则使用了一个‘Node’对象，
	  返回值可以是一个抽象的句法树；也可以如同智能ASCII例子中
	  一样，返回一个列表；也可以返回一个字符串，后者在解析中
	  已经被拼接和修改了；或者如果解析工作
	  主要是通过触发解析规则的负效应完成，返回值可以干脆为‘None’。
	  在任何情况下，返回的值是根规则的‘LexToken’的下标0的位置。
	  ## lwl： 这一段完全不知所云

  MORE ON PLY PARSERS:
  更多关于PLY解析器：

  Some of the finer points of [PLY] parsers will not be covered
  in this book.  The documentation accompanying [PLY] contains
  some additional implementational discussion, and a book devoted
  more systematically to parsing theory will address theoretical
  issues.  But a few aspects can at least be touched on.
  还有一些[PLY]解析器的优点不会在本书中涉及。[PLY]的随机文档
  包含了一些额外的关于实现的讨论，以及一本系统讲述解析理论的书，
  它将讲解理论问题。但是至少现在我们可以略微谈谈一些方面。

  +++

  *Error Recovery*
  *错误恢复*

  A [PLY] grammar may contain a special 'p_error()' function to
  catch tokens that cannot be matched (at the current position)
  by any other rule.  The first time 'p_error()' is invoked,
  [PLY] enters an "error-recovery" mode.  If the parser cannot
  process the next three tokens successfully, a traceback is
  generated.  You may include the production 'error' in other
  rules to catch errors that occur at specific points in the
  input.
  [PLY]语法可以包含一个特别的'p_error'函数，以捕获（在当前位置）
  不能被任何其他规则匹配的特征。第一次调用‘p_error()’的时候，
  [PLY]进入一个“error-recovery”模式。如果解析器不能成功处理接下去
  的三个特征，将会产生一个trackback。你可以将产品‘error’包含在
  其他规则中，以捕获输入中某个特定点发生的错误。

  To implement recovery within the 'p_error()' function, you may
  use the functions/methods 'yacc.token()', 'yacc.restart()', and
  'yacc.errok()'. The first grabs the next token from the lexer;
  if this token--or some sequence of tokens--meets some recovery
  criteria, you may call 'yacc.restart()' or 'yacc.errok()'. The
  first of these, 'yacc.restart()', returns the parser to its
  initial state--basically, only the final substring of the input
  is used in this case (however, a separate data structure you have
  built will remain as it was). Calling 'yacc.errok()' tells the
  parser to stay in its last state and just ignore any bad tokens
  pulled from the lexer (either via the call to 'p_error()' itself,
  or via calls to 'yacc.token()' in the body).
  为了实现'p_error()'函数中的恢复，你可以使用一些函数/方法
  'yacc.token()'， 'yacc.restart()'， 以及'yacc.errok()'。
  第一个从词法分析器中取得下一个特征；如果这个特征－－
  或者一些特征序列--满足一些恢复标准，你可以调用'yacc.restart()'
  或者'yacc.errok()'。第一个函数，'yacc.restart()'，将解析器返回
  到它的初始状况--基本上，只有输入的最终子字符串才在这个场合里
  使用（但是，你曾经建造过的单独的数据结构将保持原样）。
  调用'yacc.errok()'则告诉解析器保持在最后一个状态，忽略任何
  从词法解析器中拉出的特征（要么通过调用'p_error()'本身，要么
  通过函数体内调用'yacc.token()'）

  +++

  *The Parser State Machine*
  *解析器状态机*

  When a parser is first compiled, the files 'parsetab.py' and
  'parser.out' are generated.  The first, 'parsetab.py', contains
  more or less unreadable compact data structures that are used
  by subsequent parser invocations.  These structures are used
  even during later invocation of the applications; timestamps
  and signatures are compared to determine if the grammar has
  been changed.  Pregenerating state tables speeds up later
  operations.
  当第一次编译某个解析器时，会产生两个文件'parsetab.py'和
  'parser.out'。第一个文件'parsetab.py'，或多或少地包含
  一些不适合阅读的紧凑的数据结构，它们是给子序列解析器调用
  使用的。这些结构甚至在程序后面的调用中都会使用；时间戳
  和签名会被比较，以确定该语法是否已被改变。预先生成状态
  表，可以加速后面的操作。

  The file 'parser.out' contains a fairly readable description of
  the actual state machine generated by [yacc].  Although you
  cannot manually modify this state machine, examination of
  'parser.out' can help you in understanding error messages and
  undesirable behavior you encounter in your grammars.
  文件'parser.out'包含一个相当可读的，对于[yacc]产生的实际
  状态机的描述。尽管你不能手工修改此状态机，检查'parser.out'能
  帮助你理解错误信息，以及你在你的语法中所遇到的不希望的行为。

  +++

  *Precedence and Associativity*
  *领先和联合*
  ## 以下翻译不是很好，再商榷……

  To resolve ambiguous grammars, you may set the variable
  'precedence' to indicate both the precedence and the
  associativity of tokens.  Absent an explicit indication, [PLY]
  always shifts a new symbol rather than reduce a rule where both
  are allowable by some grammar rule.
  为了解决语法模糊，你可以设置变量'precedence'以表明特征的
  领先和联合。缺少显式指示，[PLY]总是轮换一个新的符号，而不是
  减少一个规程，尽管此处有些语法规则同时允许两者。

  The [PLY] documentation gives an example of an ambiguous
  arithmetic expression, such as '3Ó*Ó4Ó+Ó5'. After the tokens '3',
  '*', and '4' have been read from the token list, a 'p_mul()' rule
  might allow reduction of the product. But at the same time, a
  'p_add()' rule might contain 'NUMBER PLUS NUMBER', which would
  allow a lookahead to the 'PLUS' token (since '4' is a 'NUMBER'
  token). Moreover, the same token can have different meanings in
  different contexts, such as the unary-minus and minus operators
  in '3Ó-Ó4Ó*Ó-5'.
  [PLY]文档给出一个歧义的算术表达式的例子，例如'3Ó*Ó4Ó+Ó5'.
  在特征'3'，'*'，'4'已经从特征列表中读入之后，'p_mul()'规则
  可以允许减少产品。但在此同时，'p_add()'规则可能包含'数字 加 数字'，
  它将允许向前寻找'加'特征（因为'4'是一个'数字'特征）。而且，
  这个同样的特征在不同的上下文中，有不同的意义，例如
  '3Ó-Ó4Ó*Ó-5'中的负号和减号。

  To solve both the precedence ambiguity and the ambiguous
  meaning of the token 'MINUS', you can declare an explicit
  precedence and associativity such as:
  为了同时解决领先歧义和特征'减号'的歧义，你可以声明一个显式
  的领先和联合，例如：

  #--------- Declaring precedence and associativity ----------#
  #--------- 声明领先和联合 ----------#
  precedence = (
      ('left', 'PLUS', 'MINUS'),
      ('left', 'TIMES, 'DIVIDE'),
      ('right', 'UMINUS'),
  )
  def p_expr_uminus(t):
      'expr : MINUS expr % prec UMINUS'
      t[0] = -1 * t[2]
  def p_expr_minus(t):
      'expr : expr MINUS expr'
      t[0] = t[1] - t[3]
  def p_expr_plus(t):
      'expr : expr PLUS expr'
      t[0] = t[1] + t[3]


