CHAPTER III -- REGULAR EXPRESSIONS
第三章 --- 正则表达式
-------------------------------------------------------------------

  Regular expressions allow extremely valuable text processing
  techniques, but ones that warrant careful explanation. Python's
  [re] module, in particular, allows numerous enhancements to basic
  regular expressions (such as named backreferences, lookahead
  assertions, backreference skipping, non-greedy quantifiers, and
  others). A solid introduction to the subtleties of regular
  expressions is valuable to programmers engaged in text processing
  tasks.
  正则表达式是极有价值的文字处理技术, 但是需要详细的解释. 
  Python的[re]模块, 对基本的正则表达式进行了众多的增强
  (比如向回命名引用<+??+>, 向前断言<+??+>, 
  略过向回引用<+??+>, 非贪婪性限定词,以及其他).
  如有文章能详细介绍正则表达式的精妙之处, 对于从事文本处理的程序员会
  很有价值. 

  The prequel of this chapter contains a tutorial on regular
  expressions that allows a reader unfamiliar with regular
  expressions to move quickly from simple to complex elements of
  regular expression syntax. This tutorial is aimed primarily at
  beginners, but programmers familiar with regular expressions in
  other programming tools can benefit from a quick read of the
  tutorial, which explicates the particular regular expression
  dialect in Python.
  本章的导语包含了一个关于正则表达式的指导手册, 它可以帮助
  不熟悉正则表达式的读者迅速由简到繁地掌握相关语法. 
  该手册主要针对初学者, 如果读者已经熟悉其他编程工具中的正则表达式, 
  也可以快速阅读该手册, 以获取Python中关于正则表达式的方言. 

  It is important to note up-front that regular expressions,
  while very powerful, also have limitations.  In brief, regular
  expressions cannot match patterns that nest to arbitrary
  depths.  If that statement does not make sense, read Chapter 4,
  which discusses parsers--to a large extent, parsing exists to
  address the limitations of regular expressions.  In general, if
  you have doubts about whether a regular expression is
  sufficient for your task, try to understand the examples in
  Chapter 4, particularly the discussion of how you might spell a
  floating point number.
  坦白说正则表达式很强大, 但是也并非万能. 简单来说, 正则表达式
  不能匹配那些任意深度嵌套的模式. 如果不明白以上说法, 请参阅第四章, 
  因为那里深度讨论了解析器, 而解析器的存在正说明了正则表达式的
  不足之处. 广泛来说, 如果你怀疑正则表达式能否胜任你的工作, 可以
  尝试去理解第四章中的例子, 特别是关于如何拼写浮点数的讨论. 

  Section 3.1 examines a number of text processing problems that
  are solved most naturally using regular expression.  As in
  other chapters, the solutions presented to problems can
  generally be adopted directly as little utilities for performing
  tasks.  However, as elsewhere, the larger goal in presenting
  problems and solutions is to address a style of thinking about
  a wider class of problems than those whose solutions are
  presented directly in this book.  Readers who are interested
  in a range of ready utilities and modules will probably want to
  check additional resources on the Web, such as the Vaults of
  Parnassus <http://www.vex.net/parnassus/> and the Python
  Cookbook <http://aspn.activestate.com/ASPN/Python/Cookbook/>.
  3.1节检查了一些用正则表达式即可自然解决的文本处理问题. 和其他章一样, 
  相关的解决方案可以直接被采用为完成任务的小工具. 但是, 读者需要
  注意到是问题背后所表达的关于更广问题的思维方式, 而非仅仅这些代码. 
  读者如果对现成的工具和模块有兴趣, 可以查看网上的资源, 比如说
  the Vaults of Parnassus <http://www.vex.net/parnassus/> 和
  the Python Cookbook <http://aspn.activestate.com/ASPN/Python/Cookbook/>.  

  
  Section 3.2 is a "reference with commentary" on the Python
  standard library modules for doing regular expression tasks.
  Several utility modules and backward-compatibility regular
  expression engines are available, but for most readers, the only
  important module will be [re] itself. The discussions
  interspersed with each module try to give some guidance on why
  you would want to use a given module or function, and the
  reference documentation tries to contain more examples of actual
  typical usage than does a plain reference. In many cases, the
  examples and discussion of individual functions address common
  and productive design patterns in Python. The cross-references
  are intended to contextualize a given function (or other thing)
  in terms of related ones (and to help a reader decide which is
  right for her). The actual listing of functions, constants,
  classes, and the like are in alphabetical order within each
  category.
  3.2节是个"带评论的参考手册",介绍了如何使用Python标准库模块来完成
  正则表达式相关任务. 
  其中还涉及了若干工具模块和向后兼容的正则表达式引擎, 但是对绝大部分读者
  而言, 唯一重要的模块就是 [re] 本身. 讨论散布于各个模块, 并试图
  让你明白为何要使用给出的那个模块或者函数. 参考文档比普通参考包含了
  更多的实际使用例子. 在许多情况下, 单独函数的例子和讨论说明了
  Python中普通而又多产的设计模式. 交叉引用试图对给定的函数(或者其他东西)
  给出上下文关系, 列举相关内容(这样读者可以自行决定什么合适自己). 
  每个分类都按照字母顺序列出了函数, 常数, 类等. 


SECTION 0 -- A Regular Expression Tutorial
第0节 -- 一个关于正则表达式的简明教程
------------------------------------------------------------------------

    Some people, when confronted with a problem, think "I know,
    I'll use regular expressions." Now they have two problems.
     -- Jamie Zawinski, '<alt.religion.emacs>' (08/12/1997)
	有的人遇到问题时候会想：“我知道, 我可以使用正则表达式. ”
	然后他们就有了两个问题. 
     -- Jamie Zawinski, '<alt.religion.emacs>' (08/12/1997)

  TOPIC -- Just What is a Regular Expression, Anyway?
  主题 -- 赶紧地, 啥是正则表达式呀？
  --------------------------------------------------------------------

  Many readers will have some background with regular
  expressions, but some will not have any.  Those with
  experience using regular expressions in other languages (or in
  Python) can probably skip this tutorial section.  But readers
  new to regular expressions (affectionately called 'regexes' by
  users) should read this section; even some with experience can
  benefit from a refresher.
  并非所有读者都接触过正则表达式, 正则表达式的新朋友们
   (他们被亲切地称呼为'regexes') 应该读一读本节. 
  如果你已有过此类经验(无论是否与Python相关) , 均可略过本节. 
  不过温故而知新, 再看一遍也许有新的发现呢. 

  
  A regular expression is a compact way of describing complex
  patterns in texts. You can use them to search for patterns
  and, once found, to modify the patterns in complex ways. They
  can also be used to launch programmatic actions that depend on
  patterns.
  正则表达式是一种描述文本中的复杂模式的简洁方法. 你可以用它们
  来搜索模式, 一旦找到, 你就可以用复杂的方法修改该模式. 它们还可以
  用于进行一些依赖于模式的编程动作. 

  Jamie Zawinski's tongue-in-cheek comment in the epigram is
  worth thinking about. Regular expressions are amazingly
  powerful and deeply expressive. That is the very reason that
  writing them is just as error-prone as writing any other
  complex programming code. It is always better to solve a
  genuinely simple problem in a simple way; when you go beyond
  simple, think about regular expressions.
  Jamie Zawinski在其讽刺短诗中半开玩笑的评论值得深思. 正则表达式
  具有让人惊讶无比的能力, 同时也富有表现力. 但也因为如此, 
  和其他复杂程序代码一样, 编写它们也容易出错. 如果能用一种简单
  的方法解决一个真正简单的问题总是更好；但当你脱离了简单, 请考虑
  正则表达式. 

  A large number of tools other than Python incorporate regular
  expressions as part of their functionality. Unix-oriented
  command-line tools like 'grep', 'sed', and 'awk' are mostly
  wrappers for regular expression processing. Many text editors
  allow search and/or replacement based on regular expressions.
  Many programming languages, especially other scripting languages
  such as Perl and TCL, build regular expressions into the heart of
  the language. Even most command-line shells, such as Bash or the
  Windows-console, allow restricted regular expressions as part of
  their command syntax.
  除了Python外还有众多工具支持正则表达式的功能. 'grep','sed' 和 'awk'
  等起源于unix的命令行工具实际上是对正则表达式的包装. 许多文本
  编辑器允许基于正则表达式的搜索/替换. 很多程序语言, 特别是其他脚本
  语言, 例如Perl和TCL, 都内建正则表达式支持. 甚至命令行shell, 
  例如Bash或者Windows的控制台, 他们的语法都允许有限的正则表达式. 

  There are some variations in regular expression syntax between
  different tools that use them, but for the most part regular
  expressions are a "little language" that gets embedded inside
  bigger languages like Python. The examples in this tutorial
  section (and the documentation in the rest of the chapter) will
  focus on Python syntax, but most of this chapter transfers
  easily to working with other programming languages and tools.
  不同工具使用正则表达式的语法都略有不同. 但是绝大部分的正则表达式
  就如同"小语种"一样嵌入在大编程语言中, 例如Python. 本节中的例子
  (以及本章中其他的例子), 都专注于Python的语法, 不过本章内容可以很容易
  地转换到其他的编程语言和工具. 

  As with most of this book, examples will be illustrated by use of
  Python interactive shell sessions that readers can type
  themselves, so that they can play with variations on the
  examples. However, the [re] module has little reason to include a
  function that simply illustrates matches in the shell. Therefore,
  the availability of the small wrapper program below is implied in
  the examples:
  本书中的绝大部分例子是通过Python的交互性命令行处理程序(shell)执行的, 
  读者们可以自己键入, 方便查看例子中的变量. 尽管 [re] 模块
  本身并未为shell提供函数来轻松展示匹配情况, 下面的这个小包装函数可以提供
  如此功能. 

      #---------- re_show.py ----------#
      import re
      def re_show(pat, s):
          print re.compile(pat, re.M).sub("{\g<0>}", s.rstrip()),'\n'

      s = '''Mary had a little lamb
      And everywhere that Mary
      went, the lamb was sure
      to go'''

  Place the code in an external module and 'import' it. Those
  new to regular expressions need not worry about what the above
  function does for now. It is enough to know that the first
  argument to 're_show()' will be a regular expression pattern,
  and the second argument will be a string to be matched against.
  The matches will treat each line of the string as a separate
  pattern for purposes of matching beginnings and ends of lines.
  The illustrated matches will be whatever is contained between
  curly braces (and is typographically marked for emphasis).
  请把代码放入一个外部模块并将其'导入(import)'. 刚接触正则表达式的朋友
  现在不用担心以上代码的作用, 只要知道 're_show()' 第一个参数是
  一个正则表达式模式, 第二个参数是用来匹配的字符串就可以了. 
  为了匹配行首和行尾, 字符串的每一行都将被单独匹配. 

  TOPIC -- Matching Patterns in Text: The Basics
  主题 -- 文本匹配模式： 基础
  --------------------------------------------------------------------

  The very simplest pattern matched by a regular expression is a
  literal character or a sequence of literal characters. Anything
  in the target text that consists of exactly those characters in
  exactly the order listed will match. A lowercase character is not
  identical with its uppercase version, and vice versa. A space in
  a regular expression, by the way, matches a literal space in the
  target (this is unlike most programming languages or command-line
  tools, where a variable number of spaces separate keywords).
  最简单的正则表达式匹配模式是一个或一串可见字符(非控制符, 译者注). 
  目标文本中如果存在完全一样的字符, 而且处于完全一样的顺序, 则为命中. 
  大小写字母互不等同. 顺便说一下,  正则表达式中的空格就匹配目标中
  字面上的空格(这和绝大部分编程语言或者命令行工具不同, 因为后者都是
  使用空格来分隔关键词). 

      >>> from re_show import re_show, such
      >>> re_show('a', s)
      M{a}ry h{a}d {a} little l{a}mb.
      And everywhere th{a}t M{a}ry
      went, the l{a}mb w{a}s sure
      to go.

      >>> re_show('Mary', s)
      {Mary} had a little lamb.
      And everywhere that {Mary}
      went, the lamb was sure
      to go.

  -*-

  A number of characters have special meanings to regular
  expressions. A symbol with a special meaning can be matched,
  but to do so it must be prefixed with the backslash character
  (this includes the backslash character itself:  to match one
  backslash in the target, the regular expression should include
  '\\'). In Python, a special way of quoting a string is
  available that will not perform string interpolation. Since
  regular expressions use many of the same backslash-prefixed
  codes as do Python strings, it is usually easier to compose
  regular expression strings by quoting them as "raw strings"
  with an initial "r".
  一些字符对正则表达式来说有特殊表示. 一个符号必须跟在反斜杠后面
  才能表示它的特别意义(特殊符号里面包含反斜杠自己, 所以如果要在
  目标文本中匹配反斜杠, 正则表达式需要包含'\\'). Python提供了一种
  无需格式替换的方法来引用字符串, 就是在字符串最前面加一个"r"字, 这
  被称为"原始字符串"(意即未处理过的字符串,译者注). 因为正则表达式
  和Python字符串一样, 使用了许多以反斜杠为前缀的代码, 这样处理会
  让编写正则表达式轻松很多. 

      >>> from re_show import re_show
      >>> s = '''Special characters must be escaped.*'''
      >>> re_show(r'.*', s)
      {Special characters must be escaped.*}

      >>> re_show(r'\.\*', s)
      Special characters must be escaped{.*}

      >>> re_show('\\\\', r'Python \ escaped \ pattern')
      Python {\} escaped {\} pattern

      >>> re_show(r'\\', r'Regex \ escaped \ pattern')
      Regex {\} escaped {\} pattern

  -*-

  Two special characters are used to mark the beginning and end
  of a line:  caret ("^") and dollarsign ("$"). To match a caret
  or dollarsign as a literal character, it must be escaped (i.e.,
  precede it by a backslash "\").
  有两个特殊字符被用来标注一行的开头和结尾: 脱字符 ("^") 和美元
  符号 ("$"). 为了匹配一个脱字符或者美元符号本身, 它需要被转义
  (例如在其之前加一个反斜杠 "\"). 

  An interesting thing about the caret and dollarsign is that
  they match zero-width patterns. That is, the length of the
  string matched by a caret or dollarsign by itself is zero (but
  the rest of the regular expression can still depend on the
  zero-width match). Many regular expression tools provide
  another zero-width pattern for word-boundary ("\b"). Words
  might be divided by whitespace like spaces, tabs, newlines, or
  other characters like nulls; the word-boundary pattern matches
  the actual point where a word starts or ends, not the
  particular whitespace characters.
  关于脱字符和美元符还有一个有趣的事情, 那就是它们匹配的是零长度模式, 
  也就是说, 只有一个脱字符或者美元符匹配到的字符串长度是零
  (剩余的正则表达式仍可依赖于这个零长度的匹配). 许多正则表达式
  工具还提供另外一个零长度模式来识别单词的边界 ("\b").  (英文) 单词之间常用空白
  来分隔, 包括空格, 制表符, 换行以及其他字符例如空符号；单词边界模式
  匹配一个单词实际开始或者结束的地方, 但不包括那些特别的空白字符. 

      >>> from re_show import re_show, s
      >>> re_show(r'^Mary', s)
      {Mary} had a little lamb
      And everywhere that Mary
      went, the lamb was sure
      to go

      >>> re_show(r'Mary$', s)
      Mary had a little lamb
      And everywhere that {Mary}
      went, the lamb was sure
      to go

      >>> re_show(r'$','Mary had a little lamb')
      Mary had a little lamb{}

  -*-

  In regular expressions, a period can stand for any character.
  Normally, the newline character is not included, but optional
  switches can force inclusion of the newline character also (see
  later documentation of [re] module functions). Using a period
  in a pattern is a way of requiring that "something" occurs
  here, without having to decide what.
  通常来说, 在正则表达式中, (英文)句号可以表示任何字符, 除了换行符. 但也
  有可选项开关可以迫使句号也能代表换行符(请参阅后面关于 [re] 模块函数的文档). 
  在模式中使用句号是为了表示此处有"东西", 不需要确定是什么. 

  Readers who are familiar with DOS command-line wildcards will
  know the question mark as filling the role of "some character"
  in command masks. But in regular expressions, the
  question mark has a different meaning, and the period is used
  as a wildcard.
  读者如果熟悉DOS命令行的通配符就会知道, 问号是用来表示"一些字符", 
  但是在正则表达式中, 问号符却有不同的意义, 而句号符才是通配符. 

      >>> from re_show import re_show, s
      >>> re_show(r'.a', s)
      {Ma}ry {ha}d{ a} little {la}mb
      And everywhere t{ha}t {Ma}ry
      went, the {la}mb {wa}s sure
      to go

  -*-

  A regular expression can have literal characters in it and also
  zero-width positional patterns. Each literal character or positional
  pattern is an atom in a regular expression. One may also group
  several atoms together into a small regular expression that is
  part of a larger regular expression. One might be inclined to
  call such a grouping a "molecule," but normally it is also
  called an atom.
  正则表达式可以包含文本字母以及零长度位置模式. 每个文本字母或者
  位置模式在正则表达式中都是原子(意即不可分割的, 译者注). 也许读者还想把几个
  原子组合成一个小的正则表达式, 然后放进一个大的正则表达式里面. 
  你也许想要叫这种组合"分子", 不过通常来说, 我们还是称呼它为原子. 

  In older Unix-oriented tools like grep, subexpressions must be
  grouped with escaped parentheses, for example, '\(Mary\)'. In
  Python (as with most more recent tools), grouping is done with
  bare parentheses, but matching a literal parenthesis requires
  escaping it in the pattern.
  一些基于Unix的较老的工具, 比如grep, 子表达式需要用转义后的括号
  来组合, '\(Mary\)'就是一个例子. 在Python以及绝大部分比较新的工具中, 
  只需要括号本身就可以完成组合, 不过如果要匹配括号本身,  那就需要在模式中
  转义了(即使用 '\(' 和 '\)'). 

      >>> from re_show import re_show, s
      >>> re_show(r'(Mary)( )(had)', s)
      {Mary had} a little lamb
      And everywhere that Mary
      went, the lamb was sure
      to go

      >>> re_show(r'\(.*\)', 'spam (and eggs)')
      spam {(and eggs)}

  -*-

  Rather than name only a single character, a pattern in a
  regular expression can match any of a set of characters.
  正则表达式中的模式可以匹配任意集合的字符, 而不是仅仅一个字母. 

  A set of characters can be given as a simple list inside square
  brackets, for example, '[aeiou]' will match any single lowercase
  vowel. For letter or number ranges it may also have the first and
  last letter of a range, with a dash in the middle; for example,
  '[A-Ma-m]' will match any lowercase or uppercase letter in the
  first half of the alphabet.
  字符集合可以用方括号括住的字符表来表示, 比如说 '[aeiou]' 就匹配
  任意单独一个小写元音字母. 如果有连续的字母或数字, 还可以使用破折号
  连接第一个和最后一个字母/数字来表示, 比如 '[A-Ma-m]' 就可以表示
  字母表中前半部分任意一个大写或者小写字母. 

  Python (as with many tools) provides escape-style shortcuts to
  the most commonly used character class, such as '\s' for a
  whitespace character and '\d' for a digit. One could always
  define these character classes with square brackets, but the
  shortcuts can make regular expressions more compact and more
  readable.
  Python和其他工具都为经常使用的字符类提供了转义风格的缩写. 
  例如 '\s' 表示空白字符,  '\d' 表示阿拉伯数字. 当然你也可以
  使用方括号来定义这些字符集合, 但使用这些缩写可以让正则表达式
  显得简洁, 更易读. 

      >>> from re_show import re_show, s
      >>> re_show(r'[a-z]a', s)
      Mary {ha}d a little {la}mb
      And everywhere t{ha}t Mary
      went, the {la}mb {wa}s sure
      to go

  -*-

  The caret symbol can actually have two different meanings in regular
  expressions. Most of the time, it means to match the zero-length
  pattern for line beginnings. But if it is used at the beginning of a
  character class, it reverses the meaning of the character class.
  Everything not included in the listed character set is matched.
  在正则表达式中脱字符实际上有两种不同的含义. 大部分时间它代表了
  "行首"这样一个零长度模式；但是如果被用在一个字符集合的开头, 
  它表示取这个字符集合的补集, 意即任何一个没有被列于本集合的字符都会被匹配到. 

      >>> from re_show import re_show, s
      >>> re_show(r'[^a-z]a', s)
      {Ma}ry had{ a} little lamb
      And everywhere that {Ma}ry
      went, the lamb was sure
      to go

  -*-

  Using character classes is a way of indicating that either one
  thing or another thing can occur in a particular spot. But
  what if you want to specify that either of two whole
  subexpressions occur in a position in the regular expression?
  For that, you use the alternation operator, the vertical bar
  ("|"). This is the symbol that is also used to indicate a pipe
  in Unix/DOS shells and is sometimes called the pipe character.
  使用字符集合可以用来表示在某个特殊地点, 总有这样一个或者那样一个字符
  出现. 如果你想用正则表达式表示两个子表达式中有任意一个出现在该地点, 
  那该怎么办呢？你可以使用轮换操作符, 垂线标志 ("|") .  这个符号在
  Unix/DOS 中用来表示管道, 所以有时候被称作管道符. 

  The pipe character in a regular expression indicates an
  alternation between everything in the group enclosing it. What
  this means is that even if there are several groups to the left
  and right of a pipe character, the alternation greedily asks
  for everything on both sides. To select the scope of the
  alternation, you must define a group that encompasses the
  patterns that may match. The example illustrates this:
  管道符在正则表达式中用来选择在它左边或者右边的所有组合中的内容. 
  也就是说, 在管道符的左边和右边如果有若干组合, 它将会贪婪匹配两边所有
  的组合. 你必须使用组合来限定这个模式可能匹配的范围. 下面的例子可以解释一下：

      >>> from re_show import re_show
      >>> s2 = 'The pet store sold cats, dogs, and birds.'
      >>> re_show(r'cat|dog|bird', s2)
      The pet store sold {cat}s, {dog}s, and {bird}s.

      >>> s3 = '=first first= # =second second= # =first= # =second='
      >>> re_show(r'=first|second=', s3)
      {=first} first= # =second {second=} # {=first}= # ={second=}

      >>> re_show(r'(=)(first)|(second)(=)', s3)
      {=first} first= # =second {second=} # {=first}= # ={second=}

      >>> re_show(r'=(first|second)=', s3)
      =first first= # =second second= # {=first=} # {=second=}

  -*-

  One of the most powerful and common things you can do with
  regular expressions is to specify how many times an atom occurs
  in a complete regular expression. Sometimes you want to
  specify something about the occurrence of a single character,
  but very often you are interested in specifying the occurrence
  of a character class or a grouped subexpression.
  在正则表达式中一个常见而又强大的事情就是你可以指定某个原子
  在整个正则表达式中出现的次数. 有时候你想限定某个字符, 更多时候
  你想要限定一个字符集合或者组合成的子表达式. 

  There is only one quantifier included with "basic" regular
  expression syntax, the asterisk ("*"); in English this has the
  meaning "some or none" or "zero or more."  If you want to
  specify that any number of an atom may occur as part of a
  pattern, follow the atom by an asterisk.
  在"基础"的正则表达式中只有一个量词, 那就是星号  ("*")； 在英语中
  它表示 "一些或者没有" 或者 "没有或者更多".  如果你想在表达式中
  表示一个原子在某个地方出现任意次数, 只需要在该原子后面跟上一个星号即可. 

  Without quantifiers, grouping expressions doesn't really serve
  as much purpose, but once we can add a quantifier to a
  subexpression we can say something about the occurrence of the
  subexpression as a whole. Take a look at the example:
  没有量词, 组合表达式并不能真正顺利表达意图. 一旦我们对某个子表达式
  加上量词, 我们就可以从整体上来把握该子表达式的存在了. 让我们来看一个例子：

      >>> from re_show import re_show
      >>> s = '''Match with zero in the middle: @@
      ... Subexpression occurs, but...: @=!=ABC@
      ... Lots of occurrences: @=!==!==!==!==!=@
      ... Must repeat entire pattern: @=!==!=!==!=@'''
      >>> re_show(r'@(=!=)*@', s)
      Match with zero in the middle: {@@}
      Subexpression occurs, but...: @=!=ABC@
      Lots of occurrences: {@=!==!==!==!==!=@}
      Must repeat entire pattern: @=!==!=!==!=@

  TOPIC -- Matching Patterns in Text: Intermediate
  主题 -- 文本匹配模式： 中级
  --------------------------------------------------------------------

  In a certain way, the lack of any quantifier symbol after an atom
  quantifies the atom anyway: It says the atom occurs exactly once.
  Extended regular expressions add a few other useful numbers to
  "once exactly" and "zero or more times."  The plus sign ("+")
  means "one or more times" and the question mark ("?") means
  "zero or one times."  These quantifiers are by far the most
  common enumerations you wind up using.
  换一个角度看, 原子后面没有量词符仍能表示该原子发生的次数：
  刚好一次. 扩展后的正则表达式除了"刚好一次"和"没有或者更多"以外, 
  还增加了一些有用的表示. 加号 ("+") 表示"一次或者更多", 问号 ("?") 表示
  "零次或者一次". 很明显, 这些特性会让你激动不已. 

  If you think about it, you can see that the extended regular
  expressions do not actually let you "say" anything the basic
  ones do not. They just let you say it in a shorter and more
  readable way. For example, '(ABC)+' is equivalent to
  '(ABC)(ABC)*', and 'X(ABC)?Y' is equivalent to 'XABCY|XY'. If
  the atoms being quantified are themselves complicated grouped
  subexpressions, the question mark and plus sign can make things
  a lot shorter.
  认真思考一下, 你可以发现扩展后的正则表达式并没有让你"说出"基本
  正则表达式不能说出的. 它们只是让你用一种更短更易读的方法表达. 
  比方说, '(ABC)+' 就等同于 '(ABC)(ABC)*',  
  'X(ABC)?Y' 等同于 'XABCY|XY'. 如果只有原子本身被复杂的组合
  子表达式量化表示, 问号符和加号符可以让表达式更短. 

      >>> from re_show import re_show
      >>> s = '''AAAD
      ... ABBBBCD
      ... BBBCD
      ... ABCCD
      ... AAABBBC'''
      >>> re_show(r'A+B*C?D', s)
      {AAAD}
      {ABBBBCD}
      BBBCD
      ABCCD
      AAABBBC

  -*-

  Using extended regular expressions, you can specify arbitrary
  pattern occurrence counts using a more verbose syntax than the
  question mark, plus sign, and asterisk quantifiers. The curly
  braces ("{" and "}") can surround a precise count of how many
  occurrences you are looking for.
  在扩展的正则表达式中, 你可以指定任意的模式发生次数, 不过要使用
  比问号符, 加号符以及星号符更详细冗长的语法. 你可以使用花括号
  ("{" 和 "}") 括住你期望发生的准确次数. 

  The most general form of the curly-brace quantification uses two
  range arguments (the first must be no larger than the second, and
  both must be non-negative integers). The occurrence count is
  specified this way to fall between the minimum and maximum
  indicated (inclusive). As shorthand, either argument may be left
  empty: If so, the minimum/maximum is specified as zero/infinity,
  respectively. If only one argument is used (with no comma in
  there), exactly that number of occurrences are matched.
  花括号量词的更普通用法是使用两个范围参数: 第一个不能比第二个大, 
  而且两个都必须为非负整数. 发生次数就位于这两个数字指定的闭区间内. 
  简单讲, 两个参数之中任意一个都可以留空, 这样的话, 最小值/最大值
  就分别被指定为零/无穷大, 如果只有一个参数(旁边没有逗号), 那就是
  要精确匹配的次数. 
  

      >>> from re_show import re_show
      >>> s2 = '''aaaaa bbbbb ccccc
      ... aaa bbb ccc
      ... aaaaa bbbbbbbbbbbbbb ccccc'''
      >>> re_show(r'a{5} b{,6} c{4,8}', s2)
      {aaaaa bbbbb ccccc}
      aaa bbb ccc
      aaaaa bbbbbbbbbbbbbb ccccc

      >>> re_show(r'a+ b{3,} c?', s2)
      {aaaaa bbbbb c}cccc
      {aaa bbb c}cc
      {aaaaa bbbbbbbbbbbbbb c}cccc

      >>> re_show(r'a{5} b{6,} c{4,8}', s2)
      aaaaa bbbbb ccccc
      aaa bbb ccc
      {aaaaa bbbbbbbbbbbbbb ccccc}

  -*-

  One powerful option in creating search patterns is specifying
  that a subexpression that was matched earlier in a regular
  expression is matched again later in the expression. We do
  this using backreferences. Backreferences are named by the
  numbers 1 through 99, preceded by the backslash/escape
  character when used in this manner. These backreferences refer
  to each successive group in the match pattern, as in
  '(one)(two)(three) \1\2\3'. Each numbered backreference refers
  to the group that, in this example, has the word corresponding
  to the number.
  在创造搜索模式的时候, 有个很有用的选项是在正则表达式中再次匹配一个
  已经匹配过的子表达式. 我们称之为向回引用(backreference). 
  向回引用的命名是反斜杠/转义符后跟数字1到99. 向回引用表示了匹配模式
  中连续的组合, 比如'(one)(two)(three) \1\2\3'. 在这里, 
  每个编号的向回引用就是对应英文单词的组合, 即'\1'等于'(one)'. 

  It is important to note something the example illustrates. What
  gets matched by a backreference is the same literal string
  matched the first time, even if the pattern that matched the
  string could have matched other strings. Simply repeating the
  same grouped subexpression later in the regular expression does
  not match the same targets as using a backreference (but you have
  to decide what it is you actually want to match in either case).
  这个例子还展示了一些重要的特性. 向回引用匹配的是前一次对应匹配实际
  命中的字面字符串, 尽管它也可能匹配其他的字符串. 如果不用向回引用, 
  而是再次使用一样的子表达式, 将可以匹配更多的结果. 但也许这不是你想要的. 
  所以你要自己确定哪种情况是你实际需要的. 

  Backreferences refer back to whatever occurred in the previous
  grouped expressions, in the order those grouped expressions
  occurred. Up to 99 numbered backreferences may be used. However,
  Python also allows naming backreferences, which can make it much
  clearer what the backreferences are pointing to. The initial
  pattern group must begin with '?P<name>', and the corresponding
  backreference must contain '(?P=name)'.
  顾名思义, 向回引用按顺序引用的是在前面已经匹配的组合表达式. 一共
  可以使用99个向回引用. Python还提供了命名向回引用 (naming backreferences) , 
  这样可以清楚地找到向回引用指向的具体位置.  前面的模式组合必须以 '?P<name>'开头, 
  后面对应的向回引用必须包括 '(?P=name)'. 

      >>> from re_show import re_show
      >>> s2 = '''jkl abc xyz
      ... jkl xyz abc
      ... jkl abc abc
      ... jkl xyz xyz
      ... '''
      >>> re_show(r'(abc|xyz) \1', s2)
      jkl abc xyz
      jkl xyz abc
      jkl {abc abc}
      jkl {xyz xyz}

      >>> re_show(r'(abc|xyz) (abc|xyz)', s2)
      jkl {abc xyz}
      jkl {xyz abc}
      jkl {abc abc}
      jkl {xyz xyz}

      >>> re_show(r'(?P<let3>abc|xyz) (?P=let3)', s2)
      jkl abc xyz
      jkl xyz abc
      jkl {abc abc}
      jkl {xyz xyz}

  -*-

  Quantifiers in regular expressions are greedy. That is, they
  match as much as they possibly can.
  正则表达式中的量词是贪婪性的, 也就是说, 它们匹配得越多越好. 

  Probably the easiest mistake to make in composing regular
  expressions is to match too much. When you use a quantifier,
  you want it to match everything (of the right sort) up to the
  point where you want to finish your match. But when using the
  '*', '+', or numeric quantifiers, it is easy to forget that the
  last bit you are looking for might occur later in a line than
  the one you are interested in.
  在编写正则表达式时, 一个最常见的错误是命中过多. 
  当你使用量词的时候, 你希望它能匹配所有正确的内容, 
  直到你想要它停止的地方. 但是使用'*',  '+'以及数字量词会轻易让你
  找到的内容远远超过你想要的. 

      >>> from re_show import re_show
      >>> s2 = '''-- I want to match the words that start
      ... -- with 'th' and end with 's'.
      ... this
      ... thus
      ... thistle
      ... this line matches too much
      ... '''
      >>> re_show(r'th.*s', s2)
      -- I want to match {the words that s}tart
      -- wi{th 'th' and end with 's}'.
      {this}
      {thus}
      {this}tle
      {this line matches} too much

  -*-

  Often if you find that regular expressions are matching too much,
  a useful procedure is to reformulate the problem in your mind.
  Rather than thinking about, "What am I trying to match later in
  the expression?" ask yourself, "What do I need to avoid matching
  in the next part?" This often leads to more parsimonious pattern
  matches. Often the way to avoid a pattern is to use the
  complement operator and a character class. Look at the example,
  and think about how it works.
  如果你经常发现正则表达式匹配了太多内容, 你可以在脑中重新盘算你的问题. 
  与其问你自己:"在表达式里我随后要匹配什么？", 不如要问:"我也如何才能
  避免匹配下一段". 不过这样会带来更多过度节省的模式匹配. 我们经常用
  求补算符和字符集合来避免某种模式. 请看例子, 并且想想它是如何工作的. 

  The trick here is that there are two different ways of
  formulating almost the same sequence. Either you can think you
  want to keep matching -until- you get to XYZ, or you can think you
  want to keep matching -unless- you get to XYZ. These are subtly
  differen
  这里的窍门是有两种不同的方法来叙述几乎一样的序列. 
  一个是:"你想要一直匹配,  -直到- 你得到XYZ", 
  另一个是:"你想要一直匹配,  -除非- 你得到XYZ". 
  这两者有微妙的区别. 

  For people who have thought about basic probability, the same
  pattern occurs. The chance of rolling a 6 on a die in one roll is
  1/6. What is the chance of rolling a 6 in six rolls? A naive
  calculation puts the odds at 1/6+1/6+1/6+1/6+1/6+1/6, or 100
  percent. This is wrong, of course (after all, the chance after
  twelve rolls isn't 200 percent). The correct calculation is, "How
  do I avoid rolling a 6 for six rolls?" (i.e.,
  5/6*5/6*5/6*5/6*5/6*5/6, or about 33 percent). The chance of
  getting a 6 is the same chance as not avoiding it (or about 66
  percent). In fact, if you imagine transcribing a series of die
  rolls, you could apply a regular expression to the written
  record, and similar thinking applies.
  对于了解基本的概率学的人来说, 同样的事情也会发生在概率学里. 
  掷一个骰子一下得到6的可能性是1/6, 那么掷一个骰子六下得到
  超过一个6的可能性是多少？(<+讨论+>原文表述有问题, 
  问的是只有一个6的概率, 实际应该是一个或者以上)
  一种幼稚的做法是把所有的数字加起来
  1/6+1/6+1/6+1/6+1/6+1/6, 即100%. 这个很明显是错误的, 
  掷12次岂不是得到200%？这个正确的计算应该是:"如何才能
  避免六掷得到一个或者更多6？". 这样可以得到5/6*5/6*5/6*5/6*5/6*5/6, 
  即33%左右. 回到刚才的问题, 得到一个或者更多6的可能性是33%的补集, 
  也就是67%左右. 想象一下你记录一连串掷骰子所得结果的过程, 
  实际上, 你可以将其用于正则表达式. 

      >>> from re_show import re_show
      >>> s2 = '''-- I want to match the words that start
      ... -- with 'th' and end with 's'.
      ... this
      ... thus
      ... thistle
      ... this line matches too much
      ... '''
      >>> re_show(r'th[^s]*.', s2)
      -- I want to match {the words} {that s}tart
      -- wi{th 'th' and end with 's}'.
      {this}
      {thus}
      {this}tle
      {this} line matches too much

  -*-

  Not all tools that use regular expressions allow you to modify
  target strings. Some simply locate the matched pattern; the
  mostly widely used regular expression tool is probably grep,
  which is a tool for searching only. Text editors, for example,
  may or may not allow replacement in their regular expression
  search facility.
  并不是所有的正则表达式都允许你修改得到的字符串. 有些仅仅定位命中的模式, 
  例如使用最广的正则表达式很可能是grep, 它仅仅用来搜索而已. 
  再或者, 文字编辑器也不一定允许在正则表达式的搜索功能中进行替换. 

  Python, being a general programming language, allows
  sophisticated replacement patterns to accompany matches. Since
  Python strings are immutable, [re] functions do not modify string
  objects in place, but instead return the modified versions. But
  as with functions in the [string] module, one can always rebind a
  particular variable to the new string object that results from
  [re] modification.
  Python作为一个通用编程语言, 可以在匹配时进行复杂的替换. 
  因为Python中的字符串是不可改变的,  [re] 函数并不在原处修改字符串对象, 
  而是返回一个替换过的对象, 也就是说[re]修改后返回的是一个新的对象. 
  不过读者总是可以使用 [string] 模块中的函数把返回结果绑定到指定的变量上. 

  Replacement examples in this tutorial will call a function
  're_new()' that is a wrapper for the module function `re.sub()`.
  Original strings will be defined above the call, and the modified
  results will appear below the call and with the same style of
  additional markup of changed areas as 're_show()' used. Be
  careful to notice that the curly braces in the results displayed
  will not be returned by standard [re] functions, but are only
  added here for emphasis (as is the typography). Simply import the
  following function in the examples below:
  本教程中的替换例子会呼叫 're_new()' 函数, 该函数实际上重新包装了 [re] 模块函数
  `re.sub()`. 原来的字符串需要在调用函数之前定义, 修改过的结果在调用函数以后打印出来, 
  并使用和 're_show()' 同样的标志把修改过的地方显示出来. 
  需要注意的是标准 [re] 函数不会返回花括号, 这里打印出来只是为了强调和排版式样. 
  你可以轻松地把以下函数导入:

      #---------- re_new.py ----------#
      import re
      def re_new(pat, rep, s):
          print re.sub(pat, '{'+rep+'}', s)

  -*-

  Let us take a look at a couple of modification examples that
  build on what we have already covered. This one simply
  substitutes some literal text for some other literal text. Notice
  that `string.replace()` can achieve the same result and will be
  faster in doing so.
  让我们看一下几个修改的例子. 这一个只是简单地把文本文字替换成另外的文本文字. 
  请记住 `string.replace()` 可以用更快的速度作同样的事情. 

      >>> from re_new import re_new
      >>> s = 'The zoo had wild dogs, bobcats, lions, and other wild cats.'
      >>> re_new('cat','dog',s)
      The zoo had wild dogs, bob{dog}s, lions, and other wild {dog}s.

  -*-

  Most of the time, if you are using regular expressions to modify a
  target text, you will want to match more general patterns than just
  literal strings. Whatever is matched is what gets replaced (even if it
  is several different strings in the target):
  大部分时间, 当你在用正则表达式修改目标文本的时候, 你想匹配的内容
  不止于文本字符串. 就算命中目标中有几个不同的字符串, 只要被匹配, 都将被替换, 

      >>> from re_new import re_new
      >>> s = 'The zoo had wild dogs, bobcats, lions, and other wild cats.'
      >>> re_new('cat|dog','snake',s)
      The zoo had wild {snake}s, bob{snake}s, lions, and other wild {snake}s.
      >>> re_new(r'[a-z]+i[a-z]*','nice',s)
      The zoo had {nice} dogs, bobcats, {nice}, and other {nice} cats.

  -*-

  It is nice to be able to insert a fixed string everywhere a
  pattern occurs in a target text. But frankly, doing that is
  not very context sensitive. A lot of times, we do not want
  just to insert fixed strings, but rather to insert something
  that bears much more relation to the matched patterns.
  Fortunately, backreferences come to our rescue here. One can
  use backreferences in the pattern matches themselves, but it is
  even more useful to be able to use them in replacement
  patterns. By using replacement backreferences, one can pick
  and choose from the matched patterns to use just the parts of
  interest.
  想象一下在某模式每个发生的地方都插入一个固定的字符串, 这样做
  并不是和上下文相关的. 很多时候, 我们需要插入的字符串和命中的模式
  有一定关系. 幸运的是, 这次向回引用现身拯救了我们. 我们不光可以在匹配模式中
  使用向回引用来表示匹配本身, 还可以在替换模式中使用它. 通过
  向回引用, 我们可以从模式命中的内容中精心挑选自己想要的内容. 

  As well as backreferencing, the examples below illustrate the
  importance of whitespace in regular expressions.  In most
  programming code, whitespace is merely aesthetic.  But the
  examples differ solely in an extra space within the arguments
  to the second call--and the return value is importantly
  different.
  下面的例子展示了向回引用和空白在正则表达式中的重要性. 
  大部分编程语言使用空白仅仅为了让代码美观. 下面代码中的两个例子的
  唯一区别是第二个调用中多了一个空格, 但是结果就很不一样了. 

      >>> from re_new import re_new
      >>> s = 'A37 B4 C107 D54112 E1103 XXX'
      >>> re_new(r'([A-Z])([0-9]{2,4})',r'\2:\1',s)
      {37:A} B4 {107:C} {5411:D}2 {1103:E} XXX
      >>> re_new(r'([A-Z])([0-9]{2,4}) ',r'\2:\1 ',s)
      {37:A }B4 {107:C }D54112 {1103:E }XXX

  -*-

  This tutorial has already warned about the danger of matching
  too much with regular expression patterns. But the danger is
  so much more serious when one does modifications, that it is
  worth repeating. If you replace a pattern that matches a
  larger string than you thought of when you composed the
  pattern, you have potentially deleted some important data from
  your target.
  本教程已经警告过正则表达式会匹配过多的的危险. 当你在进行修改的时候, 
  匹配过多会引发更为严重的问题, 所以现在再次提出这个问题并不为过. 
  当你发现并改正一个命中过多的模式的时候, 你可能已经删除了目标文本中的
  某些重要内容. 
  

  It is always a good idea to try out regular expressions on
  diverse target data that is representative of production usage.
  Make sure you are matching what you think you are matching. A
  stray quantifier or wildcard can make a surprisingly wide
  variety of texts match what you thought was a specific pattern.
  And sometimes you just have to stare at your pattern for a
  while, or find another set of eyes, to figure out what is
  really going on even after you see what matches. Familiarity
  might breed contempt, but it also instills competence.
  一个总是有效的方法是从最终目标中挑出有代表性的数据进行试验,  
  并确信你可以命中你想要的.  你预期的一个模式可能会因为
  错乱的量词或者通配符而疯狂命中极多的文本. 很多时候你只要全神贯注
  于你的模式, 或者请别的伙计来检查一下, 就可以找到问题所在. 
  熟悉正则表达式也许会导致对问题的轻视, 但是它也会让你获得足够能力. 

  TOPIC -- Advanced Regular Expression Extensions
  主题 -- 高级正则表达式扩展
  --------------------------------------------------------------------

  Some very useful enhancements to basic regular expressions are
  included with Python (and with many other tools). Many of
  these do not strictly increase the power of Python's regular
  expressions, but they -do- manage to make expressing them far
  more concise and clear.
  Python和其他许多工具都对基本的正则表达式进行了非常有用的增强. 
  严格来说, 并非所有增强都提高了正则表达式的能力, 但 -至少- 它们可以
  使得表达式显得干净简洁. 

  Earlier in the tutorial, the problems of matching too much were
  discussed, and some workarounds were suggested. Python is nice
  enough to make this easier by providing optional "non-greedy"
  quantifiers.  These quantifiers grab as little as possible
  while still matching whatever comes next in the pattern
  (instead of as much as possible).
  在本教程的前面, 我们讨论了匹配过多的问题, 还给出一些如何才能避免的
  建议. Python 有个非常好的地方在于提供了一个可选的 "非贪婪性"
  量词. 在保证能命中剩余的模式的情况下, 这些量词将会尽可能少的匹配, 
  和贪婪匹配正好相反. 

  Non-greedy quantifiers have the same syntax as regular greedy
  ones, except with the quantifier followed by a question mark.
  For example, a non-greedy pattern might look like:
  'A[A-Z]*?B'. In English, this means "match an A, followed by
  only as many capital letters as are needed to find a B."
  非贪婪性量词和普通的贪婪性量词拥有一样的语法, 只是非贪婪性
  量词后面多跟了一个问号符. 比方说, 一个非贪婪性的模式'A[A-Z]*?B', 
  换成口语就翻译成:"匹配一个A, 后面只跟着必要的若干个大写字母
  直到找到第一个B. "

  One little thing to look out for is the fact that the pattern
  '[A-Z]*?.' will always match zero capital letters. No longer
  matches are ever needed to find the following "any character"
  pattern. If you use non-greedy quantifiers, watch out for
  matching too little, which is a symmetric danger.
  需要注意的是 '[A-Z]*?.' 这个模式总是匹配零个大写字母, 
  后面甚至不需要额外的模式来匹配"任何字符". 当你使用非贪婪性
  量词的时候, 要小心命中过少, 这和命中过多是危险的两个极端. 

      >>> from re_show import re_show
      >>> s = '''-- I want to match the words that start
      ... -- with 'th' and end with 's'.
      ... this line matches just right
      ... this # thus # thistle'''
      >>> re_show(r'th.*s',s)
      -- I want to match {the words that s}tart
      -- wi{th 'th' and end with 's}'.
      {this line matches jus}t right
      {this # thus # this}tle

      >>> re_show(r'th.*?s',s)
      -- I want to match {the words} {that s}tart
      -- wi{th 'th' and end with 's}'.
      {this} line matches just right
      {this} # {thus} # {this}tle

      >>> re_show(r'th.*?s ',s)
      -- I want to match {the words }that start
      -- with 'th' and end with 's'.
      {this }line matches just right
      {this }# {thus }# thistle

  -*-

  Modifiers can be used in regular expressions or as arguments to
  many of the functions in [re]. A modifier affects, in one way
  or another, the interpretation of a regular expression pattern.
  A modifier, unlike an atom, is global to the particular
  match--in itself, a modifier doesn't match anything, it instead
  constrains or directs what the atoms match.
  修饰语可以用在正则表达式中, 也可以作为参数传入许多[re] 模块中的函数. 
  修饰语总是可以或此或彼地影响一个正则表达式的译码. 
  和原子不同, 修饰语对于某个特定的匹配(内部)而言是全局的, 
  而且修饰语并不会匹配任何东西, 相反用来指导和限制原子如何匹配. 


  When used directly within a regular expression pattern, one or
  more modifiers begin the whole pattern, as in '(?Limsux)'. For
  example, to match the word 'cat' without regard to the case of
  the letters, one could use '(?i)cat'. The same modifiers may
  be passed in as the last argument as bitmasks (i.e., with a '|'
  between each modifier), but only to some functions in the [re]
  module, not to all. For example, the two calls below are
  equivalent:
  当在正则表达式中直接使用修饰语的时候, 可以在整个模式的开始使用
  一个或者更多修饰语作为前缀, 比如 '(?Limsux)' 中就有六个修饰语. 
  再举个例子, 为了匹配一个单词 'cat' , 不考虑大小写, 你可以使用
  '(?i)cat' . [re]中的一部分函数接受相同的修饰语作为输入参数, 
  不过要以比特掩码的形式(比如, 在各修饰语之间用'|'分隔. ),
  写法略有不同, 但是效果是一样的. 以下两个调用结果是完全一样的:

      >>> import re
      >>> re.search(r'(?Li)cat','The Cat in the Hat').start()
      4
      >>> re.search(r'cat','The Cat in the Hat',re.L|re.I).start()
      4

  However, some function calls in [re] have no argument for
  modifiers. In such cases, you should either use the modifier
  prefix pseudo-group or pre-compile the regular expression
  rather than use it in string form. For example:
  但是有的函数调用不接受修饰语作为参数, 这种情况下你要么使用
  前缀形式, 要么就把正则表达式预编译, 这样避免直接使用字符串形式. 
  请看例子:

      >>> import re
      >>> re.split(r'(?i)th','Brillig and The Slithy Toves')
      ['Brillig and ', 'e Sli', 'y Toves']
      >>> re.split(re.compile('th',re.I),'Brillig and the Slithy Toves')
      ['Brillig and ', 'e Sli', 'y Toves']

  See the [re] module documentation for details on which
  functions take which arguments.
  参见 [re] 模块的文档以查看具体函数的参数接受说明. 

  -*-

  The listed modifiers below are used in [re] expressions. Users
  of other regular expression tools may be accustomed to a 'g'
  option for "global" matching. These other tools take a line of
  text as their default unit, and "global" means to match
  multiple lines. Python takes the actual passed string as its
  unit, so "global" is simply the default. To operate on a
  single line, either the regular expressions have to be tailored
  to look for appropriate begin-line and end-line characters, or
  the strings being operated on should be split first using
  `string.split()` or other means.
  下列修饰语可以在 [re] 表达式中使用. 其他正则表达式工具的用户
  可能还习惯用于 "全局" 匹配的 'g' 选项. 它们默认只接受一行文本, 
   "全局"表示可以匹配多行文本. Python使用实际输入的文本, 
   默认就是"全局"选项. 如果想要对每行进行操作, 或者对正则表达式
   进行修改, 使其能寻找表示行首和行尾的字符, 需预先使用
   `string.split()` 或其他方法将字符串分解, 然后进行处理. 

      #*--------- Regular expression modifiers ---------------#
      * L (re.L) - Locale customization of \w, \W, \b, \B
      * i (re.I) - Case-insensitive match
      * m (re.M) - Treat string as multiple lines
      * s (re.S) - Treat string as single line
      * u (re.U) - Unicode customization of \w, \W, \b, \B
      * x (re.X) - Enable verbose regular expressions
      #*--------- 正则表达式修饰语 ---------------#
      * L (re.L) - 本地化版本的 \w, \W, \b, \B
      * i (re.I) - 忽略大小写匹配
      * m (re.M) - 将字符串作为多行文本处理
      * s (re.S) - 将字符串作为单行文本处理
      * u (re.U) - Unicode版本的 \w, \W, \b, \B
	  * x (re.X) - 使用详尽的正则表达式 <+此处要加详细解释+>

  The single-line option ("s") allows the wildcard to match a
  newline character (it won't otherwise). The multiple-line
  option ("m") causes "^" and "$" to match the beginning and end
  of each line in the target, not just the begin/end of the
  target as a whole (the default).  The insensitive option ("i")
  ignores differences between the case of letters.  The Locale
  and Unicode options ("L" and "u") give different
  interpretations to the word-boundary ("\b") and alphanumeric
  ("\w") escaped patterns--and their inverse forms ("\B" and
  "\W").
  只有使用单行选项 ("s") 了才能使通配符匹配到新行符. 多行选项
   ("m") 将使目标中每行的行首和行尾能被 "^" 和 "$" 匹配,  
  而默认只是匹配整个目标文本的开头和结尾. 忽略大小写选项 ("i") 
  将认为大写字母和与其对应的小写字母是相同的. 本地化选项和
  Unicode选项 ("L" 和 "u") 对单词边界 ("\b") , 字母和数字 ("\w") 
  以及它们的补集 ("\B" 和 "\W")有相应的解释. 

  The verbose option ("x") is somewhat different from the others.
  Verbose regular expressions may contain nonsignificant
  whitespace and inline comments. In a sense, this is also just
  a different interpretation of regular expression patterns, but
  it allows you to produce far more easily readable complex
  patterns. Some examples follow in the sections below.
  详述选项 ("x") 和以上选项略有不同. 详述正则表达式可以包含
  无关紧要的空白和行内注释. 从另外一种角度来看, 这只是常规
  正则表达式的另外一种写法, 不过它允许你创造更具可读性的复杂
  模式. 下面有一些例子供参考:

  -*-

  Let's take a look first at how case-insensitive and single-line
  options change the match behavior.
  首先让我们看一下忽略大小写模式选项和单行选项是如何改变匹配行为的. 

      >>> from re_show import re_show
      >>> s = '''MAINE # Massachusetts # Colorado #
      ... mississippi # Missouri # Minnesota #'''
      >>> re_show(r'M.*[ise] ', s)
      {MAINE # Massachusetts }# Colorado #
      mississippi # {Missouri }# Minnesota #

      >>> re_show(r'(?i)M.*[ise] ', s)
      {MAINE # Massachusetts }# Colorado #
      {mississippi # Missouri }# Minnesota #

      >>> re_show(r'(?si)M.*[ise] ', s)
      {MAINE # Massachusetts # Colorado #
      mississippi # Missouri }# Minnesota #

  Looking back to the definition of 're_show()', we can see it
  was defined to explicitly use the multiline option.  So
  patterns displayed with 're_show()' will always be multiline.
  Let us look at a couple of examples that use `re.findall()`
  instead.
  回顾一下 're_show()' 的定义, 我们可以看到它是显式使用多行选项的, 
  所以使用 're_show()' 显示的模式总是多行的. 下面我们看看一些
  使用 `re.findall()` 的例子. 

      >>> from re_show import re_show
      >>> s = '''MAINE # Massachusetts # Colorado #
      ... mississippi # Missouri # Minnesota #'''
      >>> re_show(r'(?im)^M.*[ise] ', s)
      {MAINE # Massachusetts }# Colorado #
      {mississippi # Missouri }# Minnesota #

      >>> import re
      >>> re.findall(r'(?i)^M.*[ise] ', s)
      ['MAINE # Massachusetts ']
      >>> re.findall(r'(?im)^M.*[ise] ', s)
      ['MAINE # Massachusetts ', 'mississippi # Missouri ']

  -*-

  Matching word characters and word boundaries depends on exactly
  what gets counted as being alphanumeric. Character codepages
  for letters outside the (US-English) ASCII range differ among
  national alphabets. Python versions are configured to a
  particular locale, and regular expressions can optionally use
  the current one to match words.
  匹配单词中的字符和单词的边界则严格依赖于字母和数字的定义. ASCII(美国英语)
  以外的字符代码页对于各个国家/地区是不一样的. Python的各版本都被
  配置成使用某特别的本地化语言, 正则表达式可以选择是否使用
  当前的语言来匹配单词. 

  Of greater long-term significance is the [re] module's ability
  (after Python 2.0) to look at the Unicode categories of
  characters, and decide whether a character is alphabetic based on
  that category. Locale settings work OK for European diacritics,
  but for non-Roman sets, Unicode is clearer and less error prone.
  The "u" modifier controls whether Unicode alphabetic characters
  are recognized or merely ASCII ones:
  从Python 2.0开始, [re]模块在处理Unicode类的字符方面有了长足的进步, 
  可以确定某字符是否属于Unicode类的字母表. 对于欧洲变音符号, 
  本地化设置可以正常工作, 但是对于非罗马字集, Unicode更加清楚, 不易引起
  错误, "u" 修饰语被用来识别是否Unicode字符或者只是ASCII字符. 

      >>> import re
      >>> alef, omega = unichr(1488), unichr(969)
      >>> u = alef +' A b C d '+omega+' X y Z'
      >>> u, len(u.split()), len(u)
      (u'\u05d0 A b C d \u03c9 X y Z', 9, 17)
      >>> ':'.join(re.findall(ur'\b\w\b', u))
      u'A:b:C:d:X:y:Z'
      >>> ':'.join(re.findall(ur'(?u)\b\w\b', u))
      u'\u05d0:A:b:C:d:\u03c9:X:y:Z'

  -*-

  Backreferencing in replacement patterns is very powerful, but
  it is easy to use many groups in a complex regular expression,
  which can be confusing to identify. It is often more legible
  to refer to the parts of a replacement pattern in sequential
  order. To handle this issue, Python's [re] patterns allow
  "grouping without backreferencing."
  替换模式中的向回引用非常强大, 为了易读性, 通常按顺序引用替换模式的各部分.
  不过如果在复杂的正则表达式中用了太多组合, 会导致识别困难.  
  为了解决这个问题, Python的[re]模式允许"没有向回引用的组合". 
## lwl:颠倒了前两句的顺序, 更加通畅.

  A group that should not also be treated as a backreference has
  a question mark colon at the beginning of the group, as in
  '(?:pattern)'. In fact, you can use this syntax even when your
  backreferences are in the search pattern itself:
  如果一个组合的开头有一个问号和分号, 例如'(?:pattern)', 
  将不会计入向回引用. 事实上, 甚至在搜索模式里面你的向回引用
  都可以使用这一语法. 

      >>> from re_new import re_new
      >>> s = 'A-xyz-37 # B:abcd:142 # C-wxy-66 # D-qrs-93'
      >>> re_new(r'([A-Z])(?:-[a-z]{3}-)([0-9]*)', r'\1\2', s)
      {A37} # B:abcd:142 # {C66} # {D93}
      >>> # Groups that are not of interest excluded from backref
      ...
      >>> re_new(r'([A-Z])(-[a-z]{3}-)([0-9]*)', r'\1\2', s)
      {A-xyz-} # B:abcd:142 # {C-wxy-} # {D-qrs-}
      >>> # One could lose track of groups in a complex pattern
      ...

  -*-

  Python offers a particularly handy syntax for really complex
  pattern backreferences. Rather than just play with the
  numbering of matched groups, you can give them a name. Above
  we pointed out the syntax for named backreferences in the
  pattern space; for example, '(?P=name)'. However, a bit different
  syntax is necessary in replacement patterns. For that, we use
  the '\g' operator along with angle brackets and a name. For
  example:
  Python为真正复杂的向回引用提供了一个特别便利的语法. 
  不用再费力点数匹配的组合, 相反的, 你可以给每个组合起一个名字. 
  刚才我们指出了模式空间内的命名向回引用语法, 比如, '(?P=name)'. 
  在替换模式中, 语法略有不同. 在这里, 我们使用'\g'操作符, 
  加上尖括号和想要的名字. 例子如下:

      >>> from re_new import re_new
      >>> s = "A-xyz-37 # B:abcd:142 # C-wxy-66 # D-qrs-93"
      >>> re_new(r'(?P<prefix>[A-Z])(-[a-z]{3}-)(?P<id>[0-9]*)',
      ...        r'\g<prefix>\g<id>',s)
      {A37} # B:abcd:142 # {C66} # {D93}

  -*-

  Another trick of advanced regular expression tools is
  "lookahead assertions."  These are similar to regular grouped
  subexpression, except they do not actually grab what they
  match. There are two advantages to using lookahead assertions.
  On the one hand, a lookahead assertion can function in a
  similar way to a group that is not backreferenced; that is, you
  can match something without counting it in backreferences.
  More significantly, however, a lookahead assertion can specify
  that the next chunk of a pattern has a certain form, but let a
  different (more general) subexpression actually grab it
  (usually for purposes of backreferencing that other
  subexpression).
  高级正则表达式还有一个小窍门就是"向前断言". 它们和普通
  组合的子表达式类似, 只是不真正提取它们匹配的内容. 
  使用向前断言有两大优势, 一方面, 向前断言可以和没有
  向回引用的组合相似地工作, 也就是说, 你可以匹配一些内容
  而不需要计入向回引用. 另一方面更加显著, 向前断言可以指定
  下一块模式拥有某特定形式, 但是让另外一个不同(更普通)的子表达式
  实际提取出(通常是给其他子表达式用来向回引用). 

  There are two kinds of lookahead assertions:  positive and
  negative. As you would expect, a positive assertion specifies
  that something does come next, and a negative one specifies
  that something does not come next. Emphasizing their
  connection with non-backreferenced groups, the syntax for
  lookahead assertions is similar:  '(?=pattern)' for positive
  assertions, and '(?!pattern)' for negative assertions.
  向前断言有两种:肯定性和否定性. 正如你可以预测到, 一个肯定性断言
  说明下面会发生一些事情, 否定性断言则说明下面不会发生某些事情. 
  需要强调一下它们和非向回引用组合的联系, 因为三者的语法比较接近:
  肯定性向前断言是'(?=pattern)', 否定性向前断言是'(?!pattern)', 
  非向回引用是'(?:pattern)'. 

      >>> from re_new import re_new
      >>> s = 'A-xyz37 # B-ab6142 # C-Wxy66 # D-qrs93'
      >>> # Assert that three lowercase letters occur after CAP-DASH
      ...
      >>> re_new(r'([A-Z]-)(?=[a-z]{3})([\w\d]*)', r'\2\1', s)
      {xyz37A-} # B-ab6142 # C-Wxy66 # {qrs93D-}
      >>> # Assert three lowercase letts do NOT occur after CAP-DASH
      ...
      >>> re_new(r'([A-Z]-)(?![a-z]{3})([\w\d]*)', r'\2\1', s)
      A-xyz37 # {ab6142B-} # {Wxy66C-} # D-qrs93

  -*-

  Along with lookahead assertions, Python 2.0+ adds "lookbehind
  assertions."  The idea is similar--a pattern is of interest
  only if it is (or is not) preceded by some other pattern.
  Lookbehind assertions are somewhat more restricted than
  lookahead assertions because they may only look backwards by a
  fixed number of character positions.  In other words, no
  general quantifiers are allowed in lookbehind assertions.
  Still, some patterns are most easily expressed using lookbehind
  assertions.
  和向前断言一起, Python 2.0以后还增加了一个"向后断言". 
  这个主意是很相似的--感兴趣的模式前面一定有(或者没有)某些其他模式. 
  向后断言比向前断言有更多限制, 因为它们只能回退一定数目的字符. 
  另一方面, 向后断言不允许通用的量词. 尽管如此, 使用向后断言仍然
  可以轻松表达某些模式. 

  As with lookahead assertions, lookbehind assertions come in a
  negative and a positive flavor. The former assures that a certain
  pattern does -not- precede the match, the latter assures that
  the pattern -does- precede the match.
  和向前断言一样, 向后断言也有肯定性和否定性之分. 前者确定在匹配前面
  -有-特定模式,  而后者则确定-无-特定模式. 

      >>> from re_new import re_new
      >>> re_show('Man', 'Manhandled by The Man')
      {Man}handled by The {Man}

      >>> re_show('(?<=The )Man', 'Manhandled by The Man')
      Manhandled by The {Man}

      >>> re_show('(?<!The )Man', 'Manhandled by The Man')
      {Man}handled by The Man

  -*-

  In the later examples we have started to see just how
  complicated regular expressions can get. These examples are
  not the half of it. It is possible to do some almost absurdly
  difficult-to-understand things with regular expression (but
  ones that are nonetheless useful).
  新的例子已经向我们小小地展示了正则表达式可能有多复杂, 不过
  距离真正的难度还有距离. 有些正则表达式可能难到无法理解,让人
  觉得荒谬, 尽管如此, 它们还是有用途的. 

  There are two basic facilities that Python's "verbose" modifier
  ("x") uses in clarifying expressions. One is allowing regular
  expressions to continue over multiple lines (by ignoring
  whitespace like trailing spaces and newlines). The second is
  allowing comments within regular expressions. When patterns
  get complicated, do both!
  Python的"详述"修饰语("x")有两种基本功能来使表达式更清晰. 
  一个是忽略空白, 比如说每行结尾的空行, 以及新行符. 这样
  可以允许正则表达式持续几行. 另外一个是在正则表达式中允许注释. 
  当你遇到复杂的模式的时候, 两个都用上!

  The example given is a fairly typical example of a complicated,
  but well-structured and well-commented, regular expression:
  下面给出的典型例子是一个复杂, 但是结构良好, 拥有详细注释的正则表达式:

      >>> from re_show import re_show
      >>> s = '''The URL for my site is: http://mysite.com/mydoc.html. You
      ... might also enjoy ftp://yoursite.com/index.html for a good
      ... place to download files.'''
      >>> pat = r'''  (?x)( # verbose identify URLs within text
      ... (http|ftp|gopher) # make sure we find a resource type
      ...               :// # ...needs to be followed by colon-slash-slash
      ...         [^ \n\r]+ # some stuff then space, newline, tab is URL
      ...                \w # URL always ends in alphanumeric char
      ...       (?=[\s\.,]) # assert: followed by whitespace/period/comma
      ...                 ) # end of match group'''
      >>> re_show(pat, s)
      The URL for my site is: {http://mysite.com/mydoc.html}. You
      might also enjoy {ftp://yoursite.com/index.html} for a good
      place to download files.

      >>> from re_show import re_show
      >>> s = '''The URL for my site is: http://mysite.com/mydoc.html. You
      ... might also enjoy ftp://yoursite.com/index.html for a good
      ... place to download files.'''
      >>> pat = r'''  (?x)( # 详述文本中URL的标识 
      ... (http|ftp|gopher) # 确保我们找到一个资源类型
      ...               :// # ...后面跟着冒号-斜线-斜线
      ...         [^ \n\r]+ # 除了空格, 新行符, 制表符(<+代码中没有+>)之外的就是URL了
      ...                \w # URL总是以字母或者数字结束
      ...       (?=[\s\.,]) # 断言: 后面跟着空白/句号/分号
      ...                 ) # 匹配组合结束'''
      >>> re_show(pat, s)
      The URL for my site is: {http://mysite.com/mydoc.html}. You
      might also enjoy {ftp://yoursite.com/index.html} for a good
      place to download files.
      我的网址是: {http://mysite.com/mydoc.html}. 你也许喜欢从
      {ftp://yoursite.com/index.html} 下载东西.

SECTION 1 -- Some Common Tasks
第一节 -- 一些常见任务
------------------------------------------------------------------------

  PROBLEM: Making a text block flush left
  问题: 使一个文字块左对齐
  --------------------------------------------------------------------

  For visual clarity or to identify the role of text, blocks of
  text are often indented--especially in prose-oriented documents
  (but log files, configuration files, and the like might also
  have unused initial fields).  For downstream purposes,
  indentation is often irrelevant, or even outright
  incorrect, since the indentation is not part of the text itself
  but only a decoration of the text.  However, it often makes
  matters even worse to perform the very most naive
  transformation of indented text--simply remove leading
  whitespace from every line.  While block indentation may be
  decoration, the relative indentations of lines within blocks
  may serve important or essential functions (for example, the
  blocks of text might be Python source code).、
  为了视觉清晰或者标志文字的作用, 文字块通常被缩进 -- 特别是类似散文
  的文档(但是日志文件, 配置文件以及类似的都有不用的初始域). 
  为了下游<+??+>目的, 缩进通常是不相关的, 甚至完全不正确, 
  只是因为缩进只是文字的修饰, 而不是文字的一部分. 
  甚至如果试图将每行开头的空白移除这样一个极端简单任务都
  可能让事情变得更糟糕. 块缩进也许只是装饰, 但是块内行与行的
  相对缩进也可能是重要甚至必须的(比如说, 文字块是Python的源代码). 

  The general procedure you need to take in maximally unindenting
  a block of text is fairly simple. But it is easy to throw more
  code at it than is needed, and arrive at some inelegant and
  slow nested loops of `string.find()` and `string.replace()`
  operations. A bit of cleverness in the use of regular
  expressions--combined with the conciseness of a functional
  programming (FP) style--can give you a quick, short, and direct
  transformation.
  通常用来对某块文字最大程度取消缩进的过程是非常简单的. 程序员
  编写的代码很轻易就超过真正需要, 陷入一些使用`string.find()`和
  `string.replace()` 的嵌入循环, 速度缓慢而且不优雅. 

      #---------- flush_left.py ----------#
      # Remove as many leading spaces as possible from whole block
	  # 对整个块删除尽量多的空格
      from re import findall,sub
      # What is the minimum line indentation of a block?
      # 本块的最小行缩进是多少？
      indent = lambda s: reduce(min,map(len,findall('(?m)^ *(?=\S)',s)))
      # Remove the block-minimum indentation from each line?
	  # 在每行移除块最小缩进？
      flush_left = lambda s: sub('(?m)^ {%d}' % indent(s),'',s)

      if __name__ == '__main__':
          import sys
          print flush_left(sys.stdin.read())

  The 'flush_left()' function assumes that blocks are indented
  with spaces.  If tabs are used--or used combined with
  spaces--an initial pass through the utility 'untabify.py' (which
  can be found at '$PYTHONPATH/tools/scripts/') can convert
  blocks to space-only indentation.
  'flush_left()'函数默认文字块都是使用空格来缩进. 如果使用的是制表符
  --或者制表符和空格混合使用--可以预先使用工具'untabify.py'过滤
  (可以在'$PYTHONPATH/tools/scripts/'中找到), 这样文字块将只用空格来
  缩进了. 

  A helpful adjunct to 'flush_left()' is likely to be the
  'reformat_para()' function that was presented in Chapter 2,
  Problem 2. Between the two of these, you could get a good part of
  the way towards a "batch-oriented word processor." (What other
  capabilities would be most useful?)
  'flush_left()'的一个有用搭档是第二章第二个问题讲述的'reformat_para()'
  函数. 从这两个函数出发, 你可以得到一个"基于批处理的文字处理器". 
  (还有其他有用的处理能力吗？)


  PROBLEM: Summarizing command-line option documentation
  问题: 命令行选项文档汇总
  --------------------------------------------------------------------

  Documentation of command-line options to programs is usually
  in semi-standard formats in places like manpages, docstrings,
  READMEs and the like.  In general, within documentation you
  expect to see command-line options indented a bit, followed by
  a bit more indentation, followed by one or more lines of
  description, and usually ended by a blank line.  This style is
  readable for users browsing documentation, but is of
  sufficiently complexity and variability that regular
  expressions are well suited to finding the right descriptions
  (simple string methods fall short).
  众多程序的命令行选项文档通常以半标准的格式存在于manpage, docstrings, 
  README以及类似地方. 通常来说, 你可以预期文档中的命令行选项被略微缩进. 
  后面还有另外的一些缩进. 再后面跟着一行或者更多的描述, 最后通常以空白行
  结束. 这种风格可以帮助提高文档的可读性, 但是对于搜索想要的描述时, 
  简单的搜索功能无法适应它的复杂性和多样性, 这时候正则表达式
  就有用武之地了. 

  A specific scenario where you might want a summary of
  command-line options is as an aid to understanding
  configuration files that call multiple child commands.  The
  file '/etc/inetd.conf' on Unix-like systems is a good example
  of such a configuration file.  Moreover, configuration files
  themselves often have enough complexity and variability within
  them that simple string methods have difficulty parsing them.
  想象这样一个情景:你遇到一些配置文件, 它们调用了多个子命令, 
  为了帮助理解, 你需要汇总命令行选项. 类Unix系统上的文件'/etc/inetd.conf'
  就是类似配置文件的一个很好例子. 可是, 这些配置文件通常都过分复杂, 
  变化较多, 普通的字符串处理很难解些它们. 

  The utility below will look for every service launched by
  '/etc/inetd.conf' and present to STDOUT summary documentation
  of all the options used when the services are started.
  以下工具将寻找'/etc/inetd.conf'启动的每个服务, 并在标准输出(STDOUT)
  上打印出这些服务启动时使用的选项的文档摘要. 


      #---------- show_services.py ----------#
      import re, os, string, sys

      def show_opts(cmdline):
          args = string.split(cmdline)
          cmd = args[0]
          if len(args) > 1:
              opts = args[1:]
          # might want to check error output, so use popen3()
          (in_, out_, err) = os.popen3('man %s | col -b' % cmd)
          manpage = out_.read()
          if len(manpage) > 2:      # found actual documentation
              print '\n%s' % cmd
              for opt in opts:
                  pat_opt = r'(?sm)^\s*'+opt+r'.*?(?=\n\n)'
                  opt_doc = re.search(pat_opt, manpage)
                  if opt_doc is not None:
                      print opt_doc.group()
                  else:             # try harder for something relevant
                      mentions = []
                      for para in string.split(manpage,'\n\n'):
                         if re.search(opt, para):
                             mentions.append('\n%s' % para)
                      if not mentions:
                         print '\n    ',opt,' '*9,'Option docs not found'
                      else:
                         print '\n    ',opt,' '*9,'Mentioned in below para:'
                         print '\n'.join(mentions)
          else:                     # no manpage available
              print cmdline
              print '    No documentation available'

      def services(fname):
          conf = open(fname).read()
          pat_srv = r'''(?xm)(?=^[^#])       # lns that are not commented out
                        (?:(?:[\w/]+\s+){6}) # first six fields ignored
                        (.*$)                # to end of ln is servc launch'''
          return re.findall(pat_srv, conf)

      if __name__ == '__main__':
          for service in services(sys.argv[1]):
              show_opts(service)

  The particular tasks performed by 'show_opts()' and 'services()'
  are somewhat specific to Unix-like systems, but the general
  techniques are more broadly applicable. For example, the
  particular comment character and number of fields in
  '/etc/inetd.conf' might be different for other launch scripts,
  but the use of regular expressions to find the launch commands
  would apply elsewhere. If the 'man' and 'col' utilities are not
  on the relevant system, you might do something equivalent, such
  as reading in the docstrings from Python modules with similar
  option descriptions (most of the samples in '$PYTHONPATH/tools/'
  use compatible documentation, for example).
  'show_opts()' 和 'services()'执行的任务只能用在类Unix系统上, 
  但是它们表达的技术具有广泛的适用性. 比如说, '/etc/inetd.conf'中
  的注释符和域的数量都是特有的, 其他启动脚本可能不一样, 但是用来
  寻找启动命令的正则表达式仍然可以使用. 如果'man'和'col'在相关
  系统上并不存在, 你需要自己做一些相同的工作, 例如从Python模块中
  使用相似选项描述, 读入docstrings(比如说大部分'$PYTHONPATH/tools/'中的例子
  都使用兼容文档). <+最后一句不太懂+>

  Another thing worth noting is that even where regular expressions
  are used in parsing some data, you need not do everything with
  regular expressions. The simple `string.split()` operation to
  identify paragraphs in 'show_opts()' is still the quickest and
  easiest technique, even though `re.split()` could do the same
  thing.
  另外一件需要注意的事情是, 尽管正则表达式被用来解析数据, 但你没必要
  用正则表达式来解决任何事情. `string.split()`操作可以用来
  标识'show_opts()'中的段落, 尽管`re.split()`可以做同样的事情, 
  但前者是最快最简单的技术. 

  Note: Along the lines of paragraph splitting, here is a thought
  problem. What is a regular expression that matches every whole
  paragraph that contains within it some smaller pattern 'pat'? For
  purposes of the puzzle, assume that a paragraph is some text that
  both starts and ends with doubled newlines ("\n\n").
  注意: 除了段落的行分割, 这里还有思维问题. 如果要匹配一整个段落, 
  其中包含了一些小的模式'pat', 该用怎样的正则表达式？为了解决这个
  问题, 可以假设每个段落的开头和结尾都是两个新行符 ("\n\n") . 


  PROBLEM:  Detecting duplicate words
  问题： 探测多余的单词
  --------------------------------------------------------------------

  A common typo in prose texts is doubled words (hopefully they
  have been edited out of this book except in those few cases
  where they are intended).  The same error occurs to a lesser
  extent in programming language code, configuration files, or
  data feeds.  Regular expressions are well-suited to detecting
  this occurrence, which just amounts to a backreference to a
  word pattern.  It's easy to wrap the regex in a small utility
  with a few extra features:
  一个散文写作中常见的打字错误是重复的单词 (希望本书已经
  将这些错误排除, 当然, 有些地方是故意的) . 同样的错误在程序代码, 
  配置文件或者数据馈送中发生概率较小. 正则表达式很合适探测这种
  情况, 它只需要向回引用单词模式即可. 很容易就可以将这个
  正则表达式以及一些额外特性包装在小工具里面. 

      #---------- dupwords.py ----------#
      # Detect doubled words and display with context
      # Include words doubled across lines but within paras
	  # 检测重复单词并且和上下文一起显示
	  # 包括段落内分居两行的重复单词

      import sys, re, glob
      for pat in sys.argv[1:]:
          for file in glob.glob(pat):
              newfile = 1
              for para in open(file).read().split('\n\n'):
                  dups = re.findall(r'(?m)(^.*(\b\w+\b)\s*\b\2\b.*$)', para)
                  if dups:
                      if newfile:
                          print '%s\n%s\n' % ('-'*70,file)
                          newfile = 0
                      for dup in dups:
                          print '[%s] -->' % dup[1], dup[0]

  This particular version grabs the line or lines on which
  duplicates occur and prints them for context (along with a prompt
  for the duplicate itself). Variations are straightforward. The
  assumption made by 'dupwords.py' is that a doubled word that
  spans a line (from the end of one to the beginning of another,
  ignoring whitespace) is a real doubling; but a duplicate that
  spans paragraphs is not likewise noteworthy.
  本版本寻找重复单词所在的一行或几行, 并作为上下文打印出来, 当然
  也有对重复单词的提示. 变化会很直观<+此句啥么意思?+>. 'dupwords.py'的假设
  是重复的单词可以跨行 (从某行的结尾到下一行的开头, 中间的空白被忽略) , 
  但是跨段的就不值得同样的注意了. 


  PROBLEM: Checking for server errors:
  问题： 检查系统错误
  --------------------------------------------------------------------

  Web servers are a ubiquitous source of information nowadays.
  But finding URLs that lead to real documents is largely
  hit-or-miss.  Every Web maintainer seems to reorganize her site
  every month or two, thereby breaking bookmarks and hyperlinks.
  As bad as the chaos is for plain Web surfers, it is worse for
  robots faced with the difficult task of recognizing the
  difference between content and errors.  By-the-by, it is easy
  to accumulate downloaded Web pages that consist of error
  messages rather than desired content.
  现在网络服务器是无处不在的信息资源. 但是寻找指向真正文档的URL
  却需要碰碰运气. 每个网络维护者看起来每隔一两个月都要重新组织一下
  他们/她们的网站. 不过这样也损害了书签和超链接. 这给上网浏览者
  带来了混乱, 不过对于机器人而言, 情况更糟, 它们无法识别真正的内容
  和错误. 顺便说一下, 下载的网页中错误内容很容易多过真正想要的内容. 

  In principle, Web servers can and should return error codes
  indicating server errors.  But in practice, Web servers almost
  always return dynamically generated results pages for erroneous
  requests.  Such pages are basically perfectly normal HTML pages
  that just happen to contain text like "Error 404:  File not
  found!"  Most of the time these pages are a bit fancier than
  this, containing custom graphics and layout, links to site
  homepages, JavaScript code, cookies, meta tags, and all sorts
  of other stuff.  It is actually quite amazing just how much
  many Web servers send in response to requests for nonexistent
  URLs.
  原则上说, 网络服务器也应该通过返回错误代码来显示服务出错. 
  在实际运用中, 对于错误请求, 大部分网络服务器总是返回动态生成
  的结果页面. 这些页面也是普通的html页面, 其中包含了
  "Error 404:  File not found!". 大部分时间, 这些页面要比这个好看
  一点, 链接到站点主页, 使用JavaScript代码, cookies, 元标签 (meta tags) , 
  以及其他各类内容. 事实上对于不存在网址的请求, 网络服务器返回页面的
  数目是让人咋舌的. 

  Below is a very simple Python script to examine just what Web
  servers return on valid or invalid requests.  Getting an error
  page is usually as simple as asking for a page called
  'http://somewebsite.com/phony-url' or the like (anything that
  doesn't really exist).  [urllib] is discussed in Chapter 5, but
  its details are not important here.
  下面是个非常简单的Python脚本, 可以用来检查网络服务器返回的内容
  是否有效. 要想得到一个错误页面, 只需请求一个叫
  'http://somewebsite.com/phony-url'的页面, 或者其他不存在的网址. 
  [urllib] 将在第五章中讨论, 这里不用考虑它的细节. 

      #---------- url_examine.py ----------#
      import sys
      from urllib import urlopen

      if len(sys.argv) > 1:
          fpin = urlopen(sys.argv[1])
          print fpin.geturl()
          print fpin.info()
          print fpin.read()
      else:
          print "No specified URL"

  Given the diversity of error pages you might receive, it is
  difficult or impossible to create a regular expression (or any
  program) that determines with certainty whether a given HTML
  document is an error page.  Furthermore, some sites choose to
  generate pages that are not really quite errors, but not
  really quite content either (e.g, generic directories of site
  information with suggestions on how to get to content).  But
  some heuristics come quite close to separating content from
  errors.  One noteworthy heuristic is that the interesting
  errors are almost always 404 or 403 (not a sure thing, but good
  enough to make smart guesses).  Below is a utility to rate the
  "error probability" of HTML documents:
  考虑到你接收到的错误页面的差异, 要写出一个正则表达式 (或其他程序) 
  来确定某html文档是否错误页, 基本上是不可能的. 特别某些网站返回的
  页面并不包含错误, 但是也没有需要的内容, 比如说网站目录信息以及
  关于如何获得信息的建议. 不过有一些直观推断可以分辨真正的内容和错误. 
  一个值得注意的是我们感兴趣的错误总是404或者403 (不确定, 但是足够
  信息以供猜测) . 下面工具可以用来评估HTML文档的"错误可能性":

      #---------- error_page.py ----------#
      import re, sys
      page = sys.stdin.read()

      # Mapping from patterns to probability contribution of pattern
      err_pats = {r'(?is)<TITLE>.*?(404|403).*?ERROR.*?</TITLE>': 0.95,
                  r'(?is)<TITLE>.*?ERROR.*?(404|403).*?</TITLE>': 0.95,
                  r'(?is)<TITLE>ERROR</TITLE>': 0.30,
                  r'(?is)<TITLE>.*?ERROR.*?</TITLE>': 0.10,
                  r'(?is)<META .*?(404|403).*?ERROR.*?>': 0.80,
                  r'(?is)<META .*?ERROR.*?(404|403).*?>': 0.80,
                  r'(?is)<TITLE>.*?File Not Found.*?</TITLE>': 0.80,
                  r'(?is)<TITLE>.*?Not Found.*?</TITLE>': 0.40,
                  r'(?is)<BODY.*(404|403).*</BODY>': 0.10,
                  r'(?is)<H1>.*?(404|403).*?</H1>': 0.15,
                  r'(?is)<BODY.*not found.*</BODY>': 0.10,
                  r'(?is)<H1>.*?not found.*?</H1>': 0.15,
                  r'(?is)<BODY.*the requested URL.*</BODY>': 0.10,
                  r'(?is)<BODY.*the page you requested.*</BODY>': 0.10,
                  r'(?is)<BODY.*page.{1,50}unavailable.*</BODY>': 0.10,
                  r'(?is)<BODY.*request.{1,50}unavailable.*</BODY>': 0.10,
                  r'(?i)does not exist': 0.10,
                 }
      err_score = 0
      for pat, prob in err_pats.items():
          if err_score > 0.9: break
          if re.search(pat, page):
              # print pat, prob
              err_score += prob

      if err_score > 0.90:   print 'Page is almost surely an error report'
      elif err_score > 0.75: print 'It is highly likely page is an error report'
      elif err_score > 0.50: print 'Better-than-even odds page is error report'
      elif err_score > 0.25: print 'Fair indication page is an error report'
      else:                 print 'Page is probably real content'

  Tested against a fair number of sites, a collection like this of
  regular expression searches and threshold confidences works
  quite well.  Within the author's own judgment of just what is
  really an error page, 'erro_page.py' has gotten no false
  positives and always arrived at at least the lowest warning
  level for every true error page.
  我们已经测试了很多网站, 返回的结果证明正则表达式搜索和可靠性阈值
  可以很好地工作. 根据作者自己对于什么是错误页面的判断, 'erro_page.py'
  已经完全正确了, 对于返回的每个真正错误页面, 至少能达到最低的警告水平. 

  The patterns chosen are all fairly simple, and both the
  patterns and their weightings were determined entirely
  subjectively by the author.  But something like this weighted
  hit-or-miss technique can be used to solve many "fuzzy logic"
  matching problems (most having nothing to do with Web server
  errors).
  选中的模式都很简单, 而且模式和它们的权重完全都是作者
  主观决定的. 不过类似的加权随机算法可以用来解决许多"模糊逻辑"
  匹配问题 (绝大部分和网络服务器错误无关) . 

  Code like that above can form a general approach to more
  complete applications.  But for what it is worth, the scripts
  'url_examine.py' and 'error_page.py' may be used directly
  together by piping from the first to the second.  For example:
  以上代码已经接近一个更完备的程序. 其中有价值的地方是, 
  两个脚本'url_examine.py'和'error_page.py'可以直接一起使用, 
  只要通过管道把第一个的结果通向第二个. 比如：

      #*------ Using ex_error_page.py -----#
      % python urlopen.py http://gnosis.cx/nonesuch | python ex_error_page.py
      Page is almost surely an error report


  PROBLEM: Reading lines with continuation characters
  问题：读入有续行符的多行文本
  --------------------------------------------------------------------

  Many configuration files and other types of computer code are
  line oriented, but also have a facility to treat multiple lines
  as if they were a single logical line.  In processing such a
  file it is usually desirable as a first step to turn all these
  logical lines into actual newline-delimited lines (or more
  likely, to transform both single and continued lines as
  homogeneous list elements to iterate through later).  A
  continuation character is generally required to be the -last-
  thing on a line before a newline, or possibly the last thing
  other than some whitespace.  A small (and very partial) table
  of continuation characters used by some common and uncommon
  formats is listed below:
  许多配置文件以及其他电脑代码都是一行一行处理的, 不过也提供
  方便来把多行作为一个逻辑行处理. 处理这样一个文件通常希望第一步
  所有的逻辑行变换到物理行, 后者使用新行符划分界限, 更确切地说, 
  是把单行和连续行统一成列表元素以供将来迭代使用. 通常
  在一行的 -最后- 位置, 下一新行之前需要放置一个续行符,也就是说
  以续行符(除了某些空白符)结尾. 下面的这个不长但是很实用的表格
  列举了一些常见或者不常见的格式使用的续行符. 
  

      #*----- Common continuation characters -----#
      \  	Python, JavaScript, C/C++, Bash, TCL, Unix config
      _  	Visual Basic, PAW
      &  	Lyris, COBOL, IBIS
      ;  	Clipper, TOP
      -  	XSPEC, NetREXX
      =  	Oracle Express

  Most of the formats listed are programming languages, and
  parsing them takes quite a bit more than just identifying the
  lines.  More often, it is configuration files of various sorts
  that are of interest in simple parsing, and most of the time
  these files use a common Unix-style convention of using
  trailing backslashes for continuation lines.
  列出的大部分格式都是编程语言, 解析它们只是比鉴别出使用续行符
  的那些行复杂一点点. 更常见的是想要对多种配置文件进行简单解析, 而
  绝大部分情况这些文件都是通过使用Unix风格的反斜杠结尾来
  表示连续的行. 

  One -could- manage to parse logical lines with a [string]
  module approach that looped through lines and performed
  concatenations when needed.  But a greater elegance is served
  by reducing the problem to a single regular expression.  The
  module below provides this:
  也许有人-能-成功地使用[string]模块来解析逻辑行, 循环检查
  每行, 当需要的时候进行拼接. 不过如果使用正则表达式来
  精简问题可以更加优雅. 该模块提供如下：

      #---------- logical_lines.py ----------#
      # Determine the logical lines in a file that might have
      # continuation characters.  'logical_lines()' returns a
      # list.  The self-test prints the logical lines as
      # physical lines (for all specified files and options).
	  # 确定文件中可能有续行符的逻辑行. 
	  # 'logical_lines()'返回一个列表. 自测试部分将会
	  # (对于所有指定的文件和选项) 把逻辑行和物理行一样打印出来. 

      import re

      def logical_lines(s, continuation='\\', strip_trailing_space=0):
          c = continuation
          if strip_trailing_space:
              s = re.sub(r'(?m)(%s)(\s+)$'%[c], r'\1', s)
          pat_log = r'(?sm)^.*?$(?<!%s)'%[c]  # e.g. (?sm)^.*?$(?<!\\)
          return [t.replace(c+'\n','') for t in re.findall(pat_log, s)]

      if __name__ == '__main__':
          import sys
          files, strip, contin = ([], 0, '\\')
          for arg in sys.argv[1:]:
              if arg[:-1] == '--continue=': contin = arg[-1]
              elif arg[:-1] == '-c': contin = arg[-1]
              elif arg in ('--string','-s'): strip = 1
              else: files.append(arg)
          if not files: files.append(sys.stdin)
          for file in files:
              s = open(sys.argv[1]).read()
              print '\n'.join(logical_lines(s, contin, strip))

  The comment in the 'pat_log' definition shows a bit just how
  cryptic regular expressions can be at times.  The comment is
  the pattern that is used for the default value of
  'continuation'.  But as dense as it is with symbols, you can
  still read it by proceeding slowly, left to right.  Let us try
  a version of the same line with the verbose modifier and
  comments:
  'pat_log'定义中的注释展示了正则表达式某些时候的神秘形象. 
  该注释被用来表示'连续行标志'的默认值. 尽管和符号一样难解, 
  你仍然可以慢慢地从左到右来阅读理解它. 
  让我们看看另一个使用冗余修饰符和注释的版本:

      >>> pat = r'''
      ... (?x)    # This is the verbose version
      ... (?s)    # In the pattern, let "." match newlines, if needed
      ... (?m)    # Allow ^ and $ to match every begin- and end-of-line
      ... ^       # Start the match at the beginning of a line
      ... .*?     # Non-greedily grab everything until the first place
      ...         # where the rest of the pattern matches (if possible)
      ... $       # End the match at an end-of-line
      ... (?<!    # Only count as a match if the enclosed pattern was not
      ...         # the immediately last thing seen (negative lookbehind)
      ... \\)     # It wasn't an (escaped) backslash'''
      >>> pat = r'''
      ... (?x)    # 这是冗余模式的版本
      ... (?s)    # 在本模式中, 如果需要的话, "."可以匹配新行. 
      ... (?m)    # 允许 ^ 和 $ 来匹配每个行首和行尾
      ... ^       # 开始. 匹配行首
      ... .*?     # 非贪婪性搜取所有内容, 直到第一个
      ...         # 剩余模式匹配到的地方 (如果能够的话) . 
      ... $       # 在行尾结束匹配. 
      ... (?<!    # 只有包含的模式不是最后一个看见的内容(否定性回顾), 
      ...         # 才算作匹配命中. 
      ... \\)     # 这不是 (转义的) 反斜杠'''


  PROBLEM: Identifying URLs and email addresses in texts
  问题： 识别文本中URL和电子邮件地址
  --------------------------------------------------------------------

  A neat feature of many Internet and news clients is their
  automatic identification of resources that the applications can
  act upon. For URL resources, this usually means making the links
  "clickable"; for an email address it usually means launching a
  new letter to the person at the address. Depending on the nature
  of an application, you could perform other sorts of actions for
  each identified resource. For a text processing application, the
  use of a resource is likely to be something more batch-oriented:
  extraction, transformation, indexing, or the like.
  许多互联网和新闻客户端提供了一个省时省力的特性, 那就是自动识别
  其他程序可以操作的资源类型. 对于URL资源, 这个通常意味着该链接是
  “可以点击”的；而对于电子邮件地址而言, 它通常意味着开始一封新
  电子邮件, 收信人即为该地址. 你可以对每个识别的资源进行其他操作. 
  不过要依赖于被调用程序本身的支持. 对于文本处理程序, 对资源的处理
  基本上都是基于批处理的, 不外乎提取, 变形, 索引等等. 
  
  Fully and precisely implementing RFC1822 (for email addresses)
  or RFC1738 (for URLs) is possible within regular expressions.
  But doing so is probably even more work than is really needed
  to identify 99% of resources.  Moreover, a significant number
  of resources in the "real world" are not strictly compliant
  with the relevant RFCs--most applications give a certain leeway
  to "almost correct" resource identifiers.  The utility below
  tries to strike approximately the same balance of other
  well-implemented and practical applications:  get -almost-
  everything that was intended to look like a resource, and
  -almost- nothing that was intended not to:
  全面而精确的实现RFC1822 (对于电子邮件地址) 或者RFC1738 (对于URL) 
  在正则表达式内是可能的. 但是这样做很可能比识别99%的资源所需要
  的工作多得多. 还有一个原因是, 在“现实生活”中, 有相当数量的资源
  并没有严格遵循相关的RFC--大部分程序对资源识别“基本正确”的目标
  还留有一些余地. 一些其他应用程序实现了很好而且很实用的平衡, 
  下面的工具试图取得同样效果：得到-基本上-所有想要用做资源的内容, 
  而那些不想用做资源的内容则-基本上-不会提取出. 
  

      #---------- find_urls.py ----------#
      # Functions to identify and extract URLs and email addresses
	  # 识别和提取URL以及电子邮件地址的程序

      import re, fileinput

      pat_url = re.compile(  r'''
                       (?x)( # verbose identify URLs within text
           (http|ftp|gopher) # make sure we find a resource type
                         :// # ...needs to be followed by colon-slash-slash
              (\w+[:.]?){2,} # at least two domain groups, e.g. (gnosis.)(cx)
                        (/?| # could be just the domain name (maybe w/ slash)
                  [^ \n\r"]+ # or stuff then space, newline, tab, quote
                      [\w/]) # resource name ends in alphanumeric or slash
           (?=[\s\.,>)'"\]]) # assert: followed by white or clause ending
                           ) # end of match group
                             ''')
      pat_email = re.compile(r'''
                      (?xm)  # verbose identify URLs in text (and multiline)
                   (?=^.{11} # Mail header matcher
           (?<!Message-ID:|  # rule out Message-ID's as best possible
               In-Reply-To)) # ...and also In-Reply-To
                      (.*?)( # must grab to email to allow prior lookbehind
          ([A-Za-z0-9-]+\.)? # maybe an initial part: DAVID.mertz@gnosis.cx
               [A-Za-z0-9-]+ # definitely some local user: MERTZ@gnosis.cx
                           @ # ...needs an at sign in the middle
                (\w+\.?){2,} # at least two domain groups, e.g. (gnosis.)(cx)
           (?=[\s\.,>)'"\]]) # assert: followed by white or clause ending
                           ) # end of match group
                             ''')
      extract_urls = lambda s: [u[0] for u in re.findall(pat_url, s)]
      extract_email = lambda s: [(e[1]) for e in re.findall(pat_email, s)]

      if __name__ == '__main__':
          for line in fileinput.input():
              urls = extract_urls(line)
              if urls:
                  for url in urls:
                      print fileinput.filename(),'=>',url
              emails = extract_email(line)
              if emails:
                  for email in emails:
                      print fileinput.filename(),'->',email

      #---------- find_urls.py ----------#
	  # 识别和提取URL以及电子邮件地址的程序

      import re, fileinput

      pat_url = re.compile(  r'''
                       (?x)( # 使用冗余模式(verbose)识别文中的URL
           (http|ftp|gopher) # 确定我们找到一种资源类别
                         :// # ...后面需要跟着“冒号-斜杠-斜杠”
              (\w+[:.]?){2,} # 至少两个域名组合, 比如说 (gnosis.)(cx)
                        (/?| # 可以正好是域名  (也许 w/ slash)  <+不懂+>
                  [^ \n\r"]+ # 或者一些内容, 后面跟着空格, 换行符, 制表符, 引号
                      [\w/]) # 资源名字以字母数字或者斜杠结尾
           (?=[\s\.,>)'"\]]) # 断言：后面跟着空白或者分句结束符
                           ) # 匹配组合结束
                             ''')
      pat_email = re.compile(r''' 
                      (?xm)  # 使用冗余模式(verbose)识别文中的URL,以及打开多行匹配模式
                   (?=^.{11} # 匹配邮件头(mail head)
           (?<!Message-ID:|  # 排除Message-ID的内容
               In-Reply-To)) # ……以及 In-Reply-To 中的
                      (.*?)( # 为了进行比较重要的的回看 (lookbehind) , 必须抓进email中
          ([A-Za-z0-9-]+\.)? # 可能是个开始的部分: DAVID.mertz@gnosis.cx
               [A-Za-z0-9-]+ # 这次肯定是本地用户: MERTZ@gnosis.cx
                           @ # ……中间需要一个at符
                (\w+\.?){2,} # 至少两个域名组合, 比如说 (gnosis.)(cx)
           (?=[\s\.,>)'"\]]) # 断言：后面跟着空白或者分句结束符
                           ) # 匹配组合结束
                             ''')
      extract_urls = lambda s: [u[0] for u in re.findall(pat_url, s)]
      extract_email = lambda s: [(e[1]) for e in re.findall(pat_email, s)]

      if __name__ == '__main__':
          for line in fileinput.input():
              urls = extract_urls(line)
              if urls:
                  for url in urls:
                      print fileinput.filename(),'=>',url
              emails = extract_email(line)
              if emails:
                  for email in emails:
                      print fileinput.filename(),'->',email

  A number of features are notable in the utility above. One point
  is that everything interesting is done within the regular
  expressions themselves. The actual functions 'extract_urls()' and
  'extract_email()' are each a single line, using the conciseness
  of functional-style programming, especially list comprehensions
  (four or five lines of more procedural code could be used, but
  this style helps emphasize where the work is done). The utility
  itself prints located resources to STDOUT, but you could do
  something else with them just as easily.
  在上面的小工具里面, 有一些特性值得注意. 其中一点是所有有趣的事情都被
  正则表达式自己解决了. 实际的函数“extract_urls()”和“extract_email()”
  都是仅仅一行, 它们充分利用了函数式编程的简洁明了, 特别是
  list comprehensions (也可以使用过程代码, 那样会多出4到5行, 
  而现在这种风格可以强调工作在哪儿完成) . 本工具只是把找到的资源在STDOUT上
  打印出来, 你也可以轻松扩充以完成更多的事情. 

  A bit of testing of preliminary versions of the regular
  expressions led me to add a few complications to them. In part
  this lets readers see some more exotic features in action; but in
  greater part, this helps weed out what I would consider "false
  positives." For URLs we demand at least two domain groups--this
  rules out LOCALHOST addresses, if present. However, by allowing a
  colon to end a domain group, we allow for specified ports such as
  'http://gnosis.cx:8080/resource/'.
  我对这些正则表达式的初始版本进行了一些测试, 结果我感觉到需要给它们
  再增加一些复杂度. 小的方面来说, 这样读者可以在操作中看到更多的奇特特性. 
  大的方面来说, 可以铲除那些我认为“假阳性”的情况. 对于URL, 我们要求
  至少两个域名组合, 但是这个会排除我们的本地地址 (LOCALHOST) , 如果有
  的话. 另外, 如果允许域名组合用冒号结尾, URL中就可以指定端口了, 例如
  “http://gnosis.cx:8080/resource/”. 

  Email addresses have one particular special consideration.  If
  the files you are scanning for email addresses happen to be
  actual mail archives, you will also find Message-ID strings.
  The form of these headers is very similar to that of email
  addresses ('In-Reply-To:' headers also contain Message-IDs).
  By combining a negative lookbehind assertion with some
  throwaway groups, we can make sure that everything that gets
  extracted is not a 'Message-ID:' header line.  It gets a little
  complicated to combine these things correctly, but the power of
  it is quite remarkable.
  电子邮件需要特别考虑一种情况. 如果你用来寻找电子邮件地址的文件恰好
  是邮件归档, 你将会发现Message-ID字符串, 这些电子文件头的格式和电子
  邮件地址非常相近 ('In-Reply-To:'也包含了Message-ID) . 把一个
  否定性向后断言和一些要扔掉的组合合并起来, 我们可以确定不会从
  “Message-ID”中提取出任何内容. 把所有东西结合在一起确实有一点
  难度, 但是它所具备的能力却是出类拔萃的. 


  PROBLEM: Pretty-printing numbers
  --------------------------------------------------------------------
  问题: 如何打印出优美的数字？

  In producing human-readable documents, Python's default string
  representation of numbers leaves something to be desired.
  Specifically, the delimiters that normally occur between powers
  of 1,000 in written large numerals are not produced by the
  `str()` or `repr()` functions--which makes reading large
  numbers difficult.  For example:
  如果要生成供人阅读的文档, Python中默认对数字的字符串表示会留下小小遗憾. 
  具体来说, 一般书写大数字的时候, 在1,000的幂数之间会有定界符, 而`str()`
  或`repr()`函数并未提供如此功能, 这样使得大数字不容易阅读. 请看例子：

      >>> budget = 12345678.90
      >>> print 'The company budget is $%s' % str(budget)
      The company budget is $12345678.9
      >>> print 'The company budget is %10.2f' % budget
      The company budget is 12345678.90

  Regular expressions can be used to transform numbers that are
  already "stringified" (an alternative would be to process
  numeric values by repeated division/remainder operations,
  stringifying the chunks).  A few basic utility functions are
  contained in the module below.
  正则表达式可以用来转换已经被"字符串化"的数字(另外一种做法是通过
  反复"相除/余数"操作来处理, 每次把余数转换成字符串).  下面的模块
  包含了一些基本的工具函数.

      #---------- pretty_nums.py ----------#
      # Create/manipulate grouped string versions of numbers
      # 创造/控制数字转换成的字符串, 并且分割成几个小组.   

      import re

      def commify(f, digits=2, maxgroups=5, european=0):
          template = '%%1.%df' % digits
          s = template % f
          pat = re.compile(r'(\d+)(\d{3})([.,]|$)([.,\d]*)')
          if european:
              repl = r'\1.\2\3\4'
          else:   # could also use locale.localeconv()['decimal_point']
              repl = r'\1,\2\3\4'
          for i in range(maxgroups):
              s = re.sub(pat,repl,s)
          return s

      def uncommify(s):
          return s.replace(',','')

      def eurify(s):
          s = s.replace('.','\000')   # place holder #占位符
          s = s.replace(',','.')      # change group delimiter #修改组定界符
          s = s.replace('\000',',')   # decimal delimiter #十进制的定界符
          return s

      def anglofy(s):
          s = s.replace(',','\000')   # place holder #占位符
          s = s.replace('.',',')      # change group delimiter #修改组定界符
          s = s.replace('\000','.')   # decimal delimiter #十进制的定界符
          return s

      vals = (12345678.90, 23456789.01, 34567890.12)
      sample = '''The company budget is $%s.
      Its debt is $%s, against assets
      of $%s'''

      if __name__ == '__main__':
          print sample % vals, '\n-----'
          print sample % tuple(map(commify, vals)), '\n-----'
          print eurify(sample % tuple(map(commify, vals))), '\n-----'

  The technique used in 'commify()' has virtues and vices.  It is
  quick, simple, and it works.  It is also slightly kludgey
  inasmuch as it loops through the substitution (and with the
  default 'maxgroups' argument, it is no good for numbers bigger
  than a quintillion; most numbers you encounter are smaller
  than this).  If purity is a goal--and it probably should not
  be--you could probably come up with a single regular expression
  to do the whole job.  Another quick and convenient technique is
  the "place holder" idea that was mentioned in the introductory
  discussion of the [string] module.
  'commify()'中的技术有优点也有缺点. 它简单快速, 而且确实能工作. 但
  当它进行循环替换的时候, 略有一些缺陷, 
  (还和默认的'maxgroups'参数有关. 
  它不适用于大于百万的三次方(quintillion, 10^18)的数字, 但是绝大部分你遇到
  的数字都比这个小). 如果你想要追求纯洁 (当然这个基本上不会是真的了) , 
  你很可能使用一个正则表达式来完成整个任务. 还有一个快捷方便的技术是"占位符", 
  我们曾经在介绍[string]模块时候提到过. 


SECTION 2 -- Standard Modules
第二节 —— 标准模块
------------------------------------------------------------------------

  TOPIC -- Versions and Optimizations
  主题 —— 版本和优化
  --------------------------------------------------------------------

    Rules of Optimization:
    Rule 1: Don't do it.
    Rule 2 (for experts only): Don't do it yet.
      -- M.A. Jackson
    优化规则：
    规则1：不要优化
    规则2 (仅对专家有效) ：现在不要优化
      —— M.A. Jackson

  Python has undergone several changes in its regular expression
  support. [regex] was superceded by [pre] in Python 1.5; [pre],
  in turn, by [sre] in Python 2.0. Although Python has continued
  to include the older modules in its standard library for
  backwards compatibility, the older ones are deprecated when the
  newer versions are included. From Python 1.5 forward, the
  module [re] has served as a wrapper to the underlying regular
  expression engine ([sre] or [pre]). But even though Python
  2.0+ has used [re] to wrap [sre], [pre] is still available (the
  latter along with its own underlying [pcre] C extension
  module that can technically be used directly).
  Python对正则表达式的支持已经经历了好几个变化. Python 1.5 中使用
  [pre]代替了[regex], 而[pre]又在Python 2.0 中被[sre]代替. 尽管
  为了向后兼容, Python在标准库中保留了旧模块, 当新版本出来之后, 
  旧版本即被废弃. 

  Each version has generally improved upon its predecessor, but
  with something as complicated as regular expressions there are
  always a few losses with each gain. For example, [sre] adds
  Unicode support and is faster for most operations--but [pre]
  has better optimization of case-insensitive searches. Subtle
  details of regular expression patterns might even let the
  quite-old [regex] module perform faster than the newer ones.
  Moreover, optimizing regular expressions can be extremely
  complicated and dependent upon specific small version
  differences.
  每个升级版本通常来说都有改进, 但是对正则表达式这样一个复杂的
  事物来说, 有得必有失. 比如说, [sre]添加了对Unicode的支持, 大
  部分操作速度更快了, 但是[pre]对大小写敏感的搜索优化更好. 
  正则表达式中一些微妙的修改甚至可以让很老的[regex]模块比
  新的模块执行更快. 而且, 正则表达式的优化是及其复杂的, 甚至和
  特定细微版本的区别有关. 
##small version如何翻译阿？

  Readers might start to feel their heads swim with these version
  details. Don't panic. Other than out of historic interest,
  you really do not need to worry about what implementations
  underlie regular expression support. The simple rule is just
  to use the module [re] and not think about what it wraps--the
  interface is compatible between versions.
  读者是否感到头脑在这些版本细节里面打转呢？不要惊慌. 如果不想
  考察历史, 你根本不需要担心对正则表达式的支持是如何实现的. 一个
  简单的规则是：只要使用[re]模块, 不需关心它包装的内容, 此模块的
  接口会兼容各版本的. 

  The real virtue of regular expressions is that they allow a
  concise and precise (albeit somewhat cryptic) description of
  complex patterns in text. Most of the time, regular expression
  operations are -fast enough-; there is rarely any point in
  optimizing an application past the point where it does what it
  needs to do fast enough that speed is not a problem. As Knuth
  famously remarks, "We should forget about small efficiencies, say
  about 97% of the time: Premature optimization is the root of all
  evil." ("Computer Programming as an Art" in _Literate
  Programming_, CSLI Lecture Notes Number 27, Stanford University
  Center for the Study of Languages and Information, 1992).
  正则表达式的真正美德在于它允许对文本中的复杂模式进行简洁而又精确
  的描述, 尽管有点晦涩难懂. 正则表达式的操作在绝大部分时间都
  -足够快-, 很少有地方需要百尺竿头更进一步. 正如高纳德那个著名的
  评论：“我们应该忘记一些小的效率改进, 大约97%的时间内, 
  过早优化都是邪恶的源泉. ”(_文学编程_中的“计算机编程的艺术一面”, 
  CSLI 讲课笔记第27号, 斯坦福大学语言和信息学习中心, 1992)

  In case regular expression operations prove to be a genuinely
  problematic performance bottleneck in an application, there are
  four steps you should take in speeding things up. Try these in
  order:
  如果正则表达式被证实在某程序中确实是性能的瓶颈, 你可以采取4步来
  加速. 请按顺序尝试：

  1.  Think about whether there is a way to simplify the regular
      expressions involved. Most especially, is it possible to
      reduce the likelihood of backtracking during pattern
      matching? You should always test your beliefs about such
      simplification, however; performance characteristics rarely
      turn out exactly as you expect.
  1.  考虑一下能否简化用到的正则表达式. 更具体的说, 能否减少
      模式匹配过程中的回溯的可能性？你应该每次都检查关于简化的
	  念头, 因为很少能一次就准确得到你希望的性能. 

  2.  Consider whether regular expressions are -really- needed
      for the problem at hand. With surprising frequency, faster
      and simpler operations in the [string] module (or,
      occasionally, in other modules) do what needs to be done.
      Actually, this step can often come earlier than the first
      one.
  2.  考虑一下取到的问题是否-真-需要正则表达式. [string]模块
      (有时候是其他模块), 有时候可以提供更快更简单的操作, 甚至
	  这种可能性高得让你惊讶. 实际工作中, 这一步经常比第一步要早. 

  3.  Write the search or transformation in a faster and
      lower-level engine, especially [mx.TextTools]. Low-level
      modules will inevitably involve more work and considerably
      more intense thinking about the problem. But
      order-of-magnitude speed gains are often possible for the
      work.
  3.  使用更快的底层引擎来编写搜索或转换, 具体点说就是
      [mx.TextTools]. 使用底层模块毫无疑问会需要更多工作, 需要
	  更专注地思考问题. 不过收获就是数量级的加速. 

  4.  Code the application (or the relevant parts of it) in a
      different programming language. If speed is the absolutely
      first consideration in an application, Assembly, C, or C++
      are going to win. Tools like swig--while outside the scope
      of this book--can help you create custom extension modules
      to perform bottleneck operations. There is a chance also
      that if the problem -really must- be solved with regular
      expressions that Perl's engine will be faster (but not
      always, by any means).
  4.  换用另外一种编程语言来编写程序, 或者其中相关的部分. 如果
      该程序考虑的第一要素是速度, 汇编、C或者C++都可以胜任. 有些
	  工具, 例如swig(这个已经超出本书范围了), 可以帮助你创建
	  扩展模块来执行瓶颈操作. 如果这个问题-一定要-使用正则表达式
	  解决, Perl的引擎也有可能会快一点, 当然也不可能总是了. 


  TOPIC -- Simple Pattern Matching
  --------------------------------------------------------------------
  主题 -- 简单模式匹配

  =================================================================
    MODULE -- fnmatch : Glob-style pattern matching
    模块 -- fnmatch ： Glob风格的模式匹配
  =================================================================

  The real purpose of the [fnmatch] module is to match filenames
  against a pattern. Most typically, [fnmatch] is used indirectly
  through the [glob] module, where the latter returns lists of
  matching files (for example to process each matching file). But
  [fnmatch] does not itself know anything about filesystems, it
  simply provides a way of checking patterns against strings. The
  pattern language used by [fnmatch] is much simpler than that used
  by [re], which can be either good or bad, depending on your
  needs. As a plus, most everyone who has used a DOS, Windows,
  OS/2, or Unix command line is already familiar with the [fnmatch]
  pattern language, which is simply shell-style expansions.
  [fnmatch]模块的真正目的是使用模式来匹配文件名. 最典型的情况是, 
  [glob]模块调用[fnmatch], 前者会返回一个匹配到的文件列表
  (这样可以处理每个匹配文件), 而非直接使用[fnmatch]. [fnmatch]本身
  并不知道文件系统的任何情况, 它只是提供一个方法来用模式检查字符串. 
  [fnmatch]使用的模式语言比[re]中使用的要简单得多, 这个很难讲是好是坏, 
  毕竟和你的需求有关. 补充一句, 如果你用过DOS、Windows、OS/2或者Unix上
  的命令行, 你应该比较熟悉[fnmatch]上的模式语言了, 因为它就是一个shell风格
  的扩展. 

  Four subpatterns are available in [fnmatch] patterns. In contrast
  to [re] patterns, there is no grouping and no quantifiers.
  Obviously, the discernment of matches is much less with [fnmatch]
  than with [re]. The subpatterns are as follows:
  [fnmatch]模式提供了四个子模式. 和[re]模式相比, 这里没有组(grouping), 
  也没有量词(quantifier). 显然, [fnmatch]匹配的识别能力要比[re]差会
  很多. 下面是各个子模式：

      #------------- Glob-style subpatterns --------------#
      *      Match everything that follows (non-greedy).
      ?      Match any single character.
      [set]  Match one character from a set.  A set generally
             follows the same rules as a regular expression
             character class.  It may include zero or more ranges
             and zero or more enumerated characters.
      [!set] Match any one character that is not in the set.
      #------------- Glob风格的子模式 --------------#
      *      匹配后面跟随的所有东西(非贪婪性). 
      ?      匹配任何一个字符. 
      [set]  匹配集合set中的任意一个字符. 集合使用的规则通常和正则表达式
             中字符类一样, 它可以包含零个或者更多范围, 也可以列举
             零个或者更多字符. 
      [!set] 匹配任何一个不在集合set中的字符

  A pattern is simply the concatenation of one or more
  subpatterns.
  模式就是一个或者更多子模式拼接起来. 

  FUNCTIONS:
  函数：

  fnmatch.fnmatch(s, pat)
      Test whether the pattern 'pat' matches the string 's'.  On
      case-insensitive filesystems, the match is case
      insensitive.  A cross-platform script should avoid
      `fnmatch.fnmatch()` except when used to match actual
      filenames.
      测试模式'pat'是否匹配字符串's'. 在大小写不敏感的文件系统上, 
      匹配也是大小写不敏感的. 跨平台的脚本应当避免
      `fnmatch.fnmatch()`, 除非是用来匹配实际的文件名. 

      >>> from fnmatch import fnmatch
      >>> fnmatch('this', '[T]?i*')  # On Unix-like system 在类Unix系统上
      0

      >>> fnmatch('this', '[T]?i*')  # On Win-like system 在Win系列系统上
      1

      SEE ALSO, `fnmatch.fnmatchcase()`
      参见`fnmatch.fnmatchcase()`

  fnmatch.fnmatchcase(s, pat)
      Test whether the pattern 'pat' matches the string 's'.
      The match is case-sensitive regardless of platform.
      测试模式'pat'是否匹配字符串's', 这里的匹配是大小写敏感的, 
      与系统无关. 

      >>> from fnmatch import fnmatchcase
      >>> fnmatchcase('this', '[T]?i*')
      0
      >>> from string import upper
      >>> fnmatchcase(upper('this'), upper('[T]?i*'))
      1

      SEE ALSO, `fnmatch.fnmatch()`
      参见`fnmatch.fnmatch()`

  fnmatch.filter(lst, pat)
      Return a new list containing those elements of 'lst' that
      match 'pat'.  The matching behaves like `fnmatch.fnmatch()`
      rather than like `fnmatch.fnmatchcase()`, so the results
      can be OS-dependent.  The example below shows a (slower)
      means of performing a case-sensitive match on all
      platforms.
      返回一个列表, 其中包含了'lst'中匹配'pat'的元素. 这个匹配和
      `fnmatch.fnmatch()`的行为很象, 而非`fnmatch.fnmatchcase()`,
      所以结果是和操作系统相关的. 下面的例子展现了一个(较慢的)
      在所有平台上实现大小写敏感的匹配. 

      >>> import fnmatch          # Assuming Unix-like system 假设类Unix系统
      >>> fnmatch.filter(['This','that','other','thing'], '[Tt]?i*')
      ['This', 'thing']
      >>> fnmatch.filter(['This','that','other','thing'], '[a-z]*')
      ['that', 'other', 'thing']
      >>> from fnmatch import fnmatchcase   # For all platforms 对于所有平台
      >>> mymatch = lambda s: fnmatchcase(s, '[a-z]*')
      >>> filter(mymatch, ['This','that','other','thing'])
      ['that', 'other', 'thing']

      For an explanation of the built-in function `filter()`
      function, see Appendix A.
      附录A有对内建函数`filter()`的解释. 

      SEE ALSO, `fnmatch.fnmatch()`, `fnmatch.fnmatchcase()`
      参见`fnmatch.fnmatch()`和`fnmatch.fnmatchcase()`

  SEE ALSO, [glob], [re]
  参见[glob]和[re]


  TOPIC -- Regular Expression Modules
  主题 -- 正则表达式模块
  --------------------------------------------------------------------

  =================================================================
    MODULE -- pre : Pre-sre module
    模块 -- pre : sre之前的模块

  =================================================================
    MODULE -- pcre : Underlying C module for pre
    模块 -- pcre : pre的底层C模块
  =================================================================

  The Python-written module [pre], and the C-written [pcre]
  module that implements the actual regular expression engine,
  are the regular expression modules for Python 1.5-1.6.  For
  complete backwards compatibility, they continue to be included
  in Python 2.0+.  Importing the symbol space of [pre] is
  intended to be equivalent to importing [re] (i.e.,  [sre] at one
  level of indirection) in Python 2.0+, with the exception of the
  handling of Unicode strings, which [pre] cannot do.  That is,
  the lines below are almost equivalent, other than potential
  performance differences in specific operations:
  Python编写的[pre]以及C编写的[pcre]模块是Python 1.5-1.6中
  的正则表达式模块, 它们是实际上的正则表达式引擎. 为了完全向后兼容, 
  Python 2.0以后版本仍然包含了它们, 导入[pre]的符号空间和导入[re]的
  是一样的 (也就是说, 间接导入[sre]), 
  除了[pre]不支持Unicode字符串. 所以, 下面的两行的效果几乎一样, 除了某些
  特定操作的性能有差异. 

      >>> import pre as re
      >>> import re

  However, there is very rarely any reason to use [pre] in Python
  2.0+.  Anyone deciding to import [pre] should know far more
  about the internals of regular expression engines than is
  contained in this book.  Of course, prior to Python 2.0,
  importing [re] simply imports [pcre] itself (and the Python
  wrappers later renamed [pre]).
  但是, Python 2.0以后极少需要使用[pre]了. 任何人如需导入[pre], 
  仅仅知道本书中讲述的正则表达式引擎的内部原理是不够的, 他们必须
  了解更多. 当然, Python 2.0之前导入[re]只是导入[pcre]本身, [re]是
  Python对[pcre]的包装, 后来被改名为[pre]. 

  SEE ALSO, [re]
  参见[re]

  =================================================================
    MODULE -- reconvert : Convert [regex] patterns to [re] patterns
    模块 -- reconvert : 转换 [regex]模式到[re]模式
  =================================================================

  This module exists solely for conversion of old regular
  expressions from scripts written for pre-1.5 versions of
  Python, or possibly from regular expression patterns used with
  tools such as sed, awk, or grep.  Conversions are not
  guaranteed to be entirely correct, but [reconvert] provides a
  starting point for a code update.
  本模块存在的唯一原因是用来转换Python 1.5之前脚本中, 
  或者一些工具例如sed, awk或者grep中使用的正则表达式. 转换无法保证
  完全正确, 但是[reconvert]至少为代码升级提供了一个起始点. 

  FUNCTIONS:
  功能：

  reconvert.convert(s)
      Return as a string the modern [re]-style pattern that
      corresponds to the [regex]-style pattern passed in argument
      's'.  For example:
      根据参数中传入的[regex]风格的模式's', 转换成现代[re]风格的模式
      并返回. 请看例子：

      >>> import reconvert
      >>> reconvert.convert(r'\<\(cat\|dog\)\>')
      '\\b(cat|dog)\\b'
      >>> import re
      >>> re.findall(r'\b(cat|dog)\b', "The dog chased a bobcat")
      ['dog']

  SEE ALSO, [regex]
  参见[regex]

  =================================================================
    MODULE -- regex : Deprecated regular expression module
    模块 -- regex : 废弃的正则表达式模块
  =================================================================

  The [regex] module is distributed with recent Python versions
  only to ensure strict backwards compatibility of scripts.
  Starting with Python 2.1, importing [regex] will produce a
  DeprecationWarning:
  最近的Python版本之所以会包括[regex]模块, 只是为了保证脚本的严格
  向后兼容性. 从Python 2.1开始, 导入[regex]将会产生一个
  DeprecationWarning：

      #*----------- Deprecation warning for regex --------------#
      #*----------- 对regex的废弃警告 --------------#
      % python -c "import regex"
      -c:1:  DeprecationWarning:  the regex module is deprecated;
      please use the re module

  For all users of Python 1.5+, [regex] should not be used in new
  code, and efforts should be made to convert its usage to [re]
  calls.
  对于Python 1.5以后的所有用户来说, 新的代码中不应该使用[regex], 
  而应该致力于把以前对它的调用转换到[re]上来. 

  SEE ALSO, [reconvert]
  参见[reconvert]

  =================================================================
    MODULE -- sre : Secret Labs Regular Expression Engine
    模块 -- sre : 秘密实验室(Secret Labs)正则表达式
  =================================================================

  Support for regular expressions in Python 2.0+ is provided by
  the module [sre].  The module [re] simply wraps [sre] in order
  to have a backwards- and forwards-compatible name.  There will
  almost never be any reason to import [sre] itself; some later
  version of Python might eventually deprecate [sre] also.  As
  with [pre], anyone deciding to import [sre] itself should know
  far more about the internals of regular expression engines than
  is contained in this book.
  Python 2.0之后对正则表达式的支持均为[sre]模块提供. [re]模块只是
  封装了[sre],以便有个向前向后均兼容的名字. 几乎没有必要来直接导入
  [sre]本身, 也许以后某个版本也会把[sre]废弃掉. 和[pre]一样, 如果
  有人想要导入[sre]本身, 需要知道的知识远胜比本书中讲述的正则表达式引擎的
  内部原理. 

  SEE ALSO, [re]
  参见[re]

  =================================================================
    MODULE -- re : Regular expression operations
    模块 -- re : 正则表达式操作
  =================================================================

  PATTERN SUMMARY:
  模式摘要：

  The chart below lists regular expression patterns; following
  that are explanations of each pattern.  For more detailed
  explanation of patterns in action, consult the tutorial and/or
  problems contained in this chapter.  The utility function
  're_show()' defined in the tutorial is used in some
  descriptions.
  下图列出了正则表达式模式, 后面跟着的是对每个模式的解释. 欲详细了解
  运行的模式, 请参考学习指南和/或本章包含的问题. 指南中定义的工具函数
  're_show()'有时候被用来描述得到的结果. 

  !!!

      #----- Regular expression patterns -----#
      #----- 正则表达式模式 -----#
      <<regex_patterns.eps>>


  ATOMIC OPERATORS:
  原子操作符：

  Plain symbol
  普通符号
      Any character not described below as having a special
      meaning simply represents itself in the target string.  An
      "A" matches exactly one "A" in the target, for example.
      任何符号, 如果下面没有描述到, 表示没有特殊意义, 它
      在目标字符串里面直接代表自己. 比如说, 一个"A"在目标中
      只精确表示一个"A". 
      
  Escape: "\"
  转义符："\"
      The escape character starts a special sequence.  The
      special characters listed in this pattern summary must be
      escaped to be treated as literal character values
      (including the escape character itself).  The letters "A",
      "b", "B", "d", "D", "s", "S", "w", "W", and "Z" specify
      special patterns if preceded by an escape.  The escape
      character may also introduce a backreference group with up
      to two decimal digits.  The escape is ignored if it
      precedes a character with no special escaped meaning.
      Since Python string escapes overlap regular expression
      escapes, it is usually better to use raw strings for
      regular expressions that potentially include escapes.  For
      example:
      转义符表示特殊序列的开始. 本摘要中所列出的特殊字符如果
      想要作为字面字符使用, 必须进行转义才行, 包括转义符本身. 
      而字母"A","b", "B", "d", "D", "s", "S", "w", "W", 和"Z"
      只有跟在转义符后面才能表示特殊模式. 转义符后面可以跟一个或者
      两个十进制数字, 用来表示向回引用组(backreference group). 
      若是后面跟的字符并没有特殊转义含义, 转义符将被忽略. 
      Python字符串会转义重叠的正则表达式转义符, 通常来说, 使用
      原始字符串来表示可能包含转义符的正则表达式比较好. 请看例子：

      >>> from re_show import re_show
      >>> re_show(r'\$ \\ \^', r'\$ \\ \^ $ \ ^')
      \$ \\ \^ {$ \ ^}

      >>> re_show(r'\d \w', '7 a 6 # ! C')
      {7 a} 6 # ! C

  Grouping operators: "(", ")"
  组操作符："(", ")"
  
      Parentheses surrounding any pattern turn that pattern into
      a group (possibly within a larger pattern).  Quantifiers
      refer to the immediately preceding group, if one is
      defined, otherwise to the preceding character or character
      class.  For example:
      使用圆括号来围住一个模式, 可以将该模式变成一个(位于更大模式中的)组. 
      如果定义了的话, 量词引用的将是紧靠其前面组, 否则引用的是前面的字符
      或者字符类. 请看例子：

      >>> from re_show import re_show
      >>> re_show(r'abc+', 'abcabc abc abccc')
      {abc}{abc} {abc} {abccc}

      >>> re_show(r'(abc)+', 'abcabc abc abccc')
      {abcabc} {abc} {abc}cc

  Backreference: "\d", "\dd"
  向回引用符："\d", "\dd"
      A backreference consists of the escape character followed
      by one or two decimal digits.  The first digit in a back
      reference may not be a zero.  A backreference refers to
      the same string matched by an earlier group, where
      the enumeration of previous groups starts with 1.  For
      example:
      向回引用符是由转义符跟上一个或两个十进制数字组成. 
      在向回引用中, 第一个数字不可以是零. 前面的组从1开始计数, 
      向回引用符引用的就是它的数字对应的那个组. 请看例子：

      >>> from re_show import re_show
      >>> re_show(r'([abc])(.*)\1', 'all the boys are coy')
      {all the boys a}re coy

      An attempt to reference an undefined group will raise an
      error.
      如果试图引用一个没有定义的组, 将会引发错误. 

  Character classes: "[", "]"
  字符类："[", "]"
      Specify a set of characters that may occur at a position.
      The list of allowable characters may be enumerated with no
      delimiter.  Predefined character classes, such as "\d", are
      allowed within custom character classes.  A range of
      characters may be indicated with a dash.  Multiple ranges
      are allowed within a class.  If a dash is meant to be
      included in the character class itself, it should occur as
      the first listed character.  A character class may be
      complemented by beginning it with a caret ("^").  If a
      caret is meant to be included in the character class
      itself, it should occur in a noninitial position.  Most
      special characters, such as "$", ".", and "(", lose their
      special meaning inside a character class and are merely
      treated as class members.  The characters "]", "\", and
      "'-'" should be escaped with a backslash, however.  For
      example:
      可以在某个地方指定一个字符集合, 该集合是一个列表, 列举了
      所有允许的字符, 中间没有定界符. 预定义的字符类, 例如"\d", 
      也可以在自定义字符类中使用. 也可以使用破折号来表示某个范围内
      的字符. 在一个类中, 允许有多个范围. 如果某个字符集合中, 破折号
      需要表示它自己, 它就必须是列表的第一个字符. 如果在字符集合的开头
      是一个脱字符("^"), 就表示对该字符类取反. 如果想要字符集合中
      想要包括脱字符本身, 那么它就不能出现在开头这个位置. 
      在字符集合中, 绝大部分特殊字符, 诸如"$",  ".",  和 "(", 
      都失去了它们的特殊意义, 只被作为集合成员处理. 但是字符
      "]",  "\", 和"'-'" 却应该使用反斜杠来转义. 请看例子：

      >>> from re_show import re_show
      >>> re_show(r'[a-fA-F]', 'A X c G')
      {A} X {c} G

      >>> re_show(r'[-A$BC\]]', r'A X - \ ] [ $')
      {A} X {-} \ {]} [ {$}

      >>> re_show(r'[^A-Fa-f]', r'A X c G')
      A{ }{X}{ }c{ }{G}

  Digit character class: "\d"
  数字字符类："\d"
      The set of decimal digits.  Same as "[0-9]".
      十进制数字的集合. 等同于"[0-9]". 

  Non-digit character class: "\D"
  非数字字符类："\D"
      The set of all characters -except- decimal digits.  Same as
      "[^0-9]".
      -除了-十进制数字外的所有字符的集合. 等同于"[^0-9]". 

  Alphanumeric character class: "\w"
  数字字母字符类："\w"
      The set of alphanumeric characters.  If re.LOCALE and
      re.UNICODE modifiers are -not- set, this is the same as
      [a-zA-Z0-9_].  Otherwise, the set includes any other
      alphanumeric characters appropriate to the locale or with
      an indicated Unicode character property of alphanumeric.
      数字字母的集合. 如果-没有-设置re.LOCALE和re.UNICODE修饰符, 
      该集合就等同于[a-zA-Z0-9_], 否则还包含其他对locale来说合适的
      数字字母, 或者Unicode版本的数字和字母. 

  Non-alphanumeric character class: "\W"
  非数字字母字符类："\W"
      The set of nonalphanumeric characters.  If re.LOCALE and
      re.UNICODE modifiers are -not- set, this is the same as
      [^a-zA-Z0-9_].  Otherwise, the set includes any other
      characters not indicated by the locale or Unicode character
      properties as alphanumeric.
      非数字字母的集合. 如果-没有-设置re.LOCALE和re.UNICODE修饰符, 
      该集合就等同于[^a-zA-Z0-9_]. 否则还包含其他对locale来说不是
      数字字母的字符, 或者Unicode版本的非数字字母字符. 

  Whitespace character class: "\s"
  空白字符类："\s"
      The set of whitespace characters.  Same as "[ \t\n\r\f\v]".
      空白字符的集合. 等同于"[ \t\n\r\f\v]". 

  Non-whitespace character class: "\S"
  非空白字符类："\S"
      The set of non-whitespace characters.  Same as
      "[^ \t\n\r\f\v]".
      非空白字符的集合. 等同于"[^ \t\n\r\f\v]". 

  Wildcard character: "."
  通配符："."
      The period matches any single character at a position.  If
      the re.DOTALL modifier is specified, "." will match a
      newline.  Otherwise, it will match anything other than a
      newline.
      (英文)句号可以在指定位置匹配任意一个字符. 如果指定了re.DOTALL, 
      "."将还可以匹配新行符, 否则, 它就只能匹配除此以外的任何东西. 

  Beginning of line: "^"
  行首符："^"
      The caret will match the beginning of the target string.
      If the re.MULTILINE modifier is specified, "^" will match
      the beginning of each line within the target string.
      脱字符可以匹配目标字符串的开头. 如果指定了re.MULTILINE, 
      脱字符将可以匹配目标字符串中的每一行的行首. 

  Beginning of string: "\A"
  字符串首符："\A"
      The "\A" will match the beginning of the target string.
      If the re.MULTILINE modifier is -not- specified, "\A"
      behaves the same as "^".  But even if the modifier is
      used, "\A" will match only the beginning of the entire
      target.
      不管-有没有-指定re.MULTILINE, "\A"只匹配整个目标字符串的开头,
      而不会匹配到每一行的行首.
 ## 原文真罗嗦, 修改一下

  End of line: "$"
  行尾符："$"
      The dollar sign will match the end of the target string.
      If the re.MULTILINE modifier is specified, "$" will match
      the end of each line within the target string.
      美元符可以匹配目标字符串的结尾. 如果指定了re.DOTALL, 
      "$"还可以匹配目标字符串中每一行的行末. 

  End of string: "\Z"
  字符串结尾符："\Z"
      The "\Z" will match the end of the target string.  If the
      re.MULTILINE modifier is -not- specified, "\Z" behaves the
      same as "$".  But even if the modifier is used, "\Z" will
      match only the end of the entire target.、
      不管-有没有-指定re.MULTILINE, "\Z"只匹配目标字符串的结尾,
      而不会匹配到每一行的行末.
 ## 上面的话真罗嗦. 一样精简
 ## end of string, 不知道怎么翻译才好

  Word boundary: "\b"
  单词边界符："\b"
      The "\b" will match the beginning or end of a word (where a
      word is defined as a sequence of alphanumeric characters
      according to the current modifiers).  Like "^" and "$",
      "\b" is a zero-width match.
      "\b"可以匹配单词的开头或者结尾, 此处单词根据当前修饰符被定义
      为一个数字字母字符序列. 和"^" 以及 "$"一样, "\b"是个零长度的
      匹配(zero-width match). 

  Non-word boundary: "\B"
  非单词边界符："\B"
      The "\B" will match any position that is -not- the
      beginning or end of a word (where a word is defined as a
      sequence of alphanumeric characters according to the
      current modifiers).  Like "^" and "$", "\B" is a zero-width
      match.
      "\B"可以匹配任何一个-不是-单词开头或结尾的位置, 此处单词根据当前
      修饰符被定义为一个数字字母字符序列. 和"^" 以及 "$"一样, 
      "\B"是个零长度的匹配(zero-width match). 

  Alternation operator: "|"
  轮换操作符(管道符)："|"
      The pipe symbol indicates a choice of multiple atoms in a
      position.  Any of the atoms (including groups) separated by
      a pipe will match.  For example:
      管道符表示某位置上有多个原子供选择. 任何一个被管道符分隔开的
      原子(包括组)都可以匹配. 请看例子：

      >>> from re_show import re_show
      >>> re_show(r'A|c|G', r'A X c G')
      {A} X {c} {G}

      >>> re_show(r'(abc)|(xyz)', 'abc efg xyz lmn')
      {abc} efg {xyz} lmn

  QUANTIFIERS:
  量词：

  Universal quantifier: "*"
  通用量词："*"
      Match zero or more occurrences of the preceding atom.  The
      "*" quantifier is happy to match an empty string.  For
      example:
      匹配前面原子的重复出现, 可以是零次或者更多次. "*"量词可以用来匹配
      一个空字符串. 请看例子：
 ## 翻译得不通畅. 下面还有

      >>> from re_show import re_show
      >>> re_show('a* ', ' a aa aaa aaaa b')
      { }{a }{aa }{aaa }{aaaa }b

  Non-greedy universal quantifier: "*?"
  非贪婪通用量词："*?"
      Match zero or more occurrences of the preceding atom, but
      try to match as few occurrences as allowable.  For example:
      匹配前面原子的重复出现, 可以是零次或者更多次, 但是要匹配尽可能少的次数. 
      请看例子：
 ## 前面有的翻译成非贪婪, 有的翻译成非贪心, 需要统一

      >>> from re_show import re_show
      >>> re_show('<.*>', '<> <tag>Text</tag>')
      {<> <tag>Text</tag>}

      >>> re_show('<.*?>', '<> <tag>Text</tag>')
      {<>} {<tag>}Text{</tag>}

  Existential quantifier: "+"
  存在量词："+"
      Match one or more occurrences of the preceding atom.  A
      pattern must actually occur in the target string to satisfy
      the "+" quantifier.  For example:
      匹配前面原子的重复出现, 可以是一次或者更多次. 为了满足"+"量词, 模式在
      字符串中必须实际出现才行. 请看例子：

      >>> from re_show import re_show
      >>> re_show('a+ ', ' a aa aaa aaaa b')
       {a }{aa }{aaa }{aaaa }b

  Non-greedy existential quantifier: "+?"
  非贪婪存在量词："+?"
      Match one or more occurrences of the preceding atom, but
      try to match as few occurrences as allowable.  For example:
      匹配前面原子的重复出现, 可以是一次或者更多次, 但是要匹配尽可能少的次数. 
      请看例子：

      >>> from re_show import re_show
      >>> re_show('<.+>', '<> <tag>Text</tag>')
      {<> <tag>Text</tag>}

      >>> re_show('<.+?>', '<> <tag>Text</tag>')
      {<> <tag>}Text{</tag>}

  Potentiality quantifier: "?"
  可能性量词："?"
      Match zero or one occurrence of the preceding atom.  The
      "?" quantifier is happy to match an empty string.  For
      example:
      匹配前面原子的重复出现, 可以是零次或者一次. "?"量词可以命中
      空字符串. 请看例子：

      >>> from re_show import re_show
      >>> re_show('a? ', ' a aa aaa aaaa b')
      { }{a }a{a }aa{a }aaa{a }b

  Non-greedy potentiality quantifier: "??"
  非贪婪可能性量词："??"
      Match zero or one occurrences of the preceding atom, but
      match zero if possible.  For example:
      匹配前面原子的出现, 可以是零次或者一次, 但是尽可能地匹配
      零次. 请看例子：

      >>> from re_show import re_show
      >>> re_show(' a?', ' a aa aaa aaaa b')
      { a}{ a}a{ a}aa{ a}aaa{ }b

      >>> re_show(' a??', ' a aa aaa aaaa b')
      { }a{ }aa{ }aaa{ }aaaa{ }b

  Exact numeric quantifier: "{num}"
  精确数目量词："{num}"
      Match exactly 'num' occurrences of the preceding atom.  For
      example:
      匹配前面原子的精确'num'次出现. 请看例子：

      >>> from re_show import re_show
      >>> re_show('a{3} ', ' a aa aaa aaaa b')
       a aa {aaa }a{aaa }b

  Lower-bound quantifier: "{min,}"
  下边界量词："{min,}"
      Match -at least- 'min' occurrences of the preceding atom.
      For example:
      匹配前面原子的-至少-'min'次出现. 请看例子：

      >>> from re_show import re_show
      >>> re_show('a{3,} ', ' a aa aaa aaaa b')
       a aa {aaa }{aaaa }b

  Bounded numeric quantifier: "{min,max}"
  边界数目量词："{min,max}"
      Match -at least- 'min' and -no more than- 'max' occurrences
      of the preceding atom.  For example:
      匹配前面原子-至少-'min'次, -至多-'max'次的出现. 请看例子：

      >>> from re_show import re_show
      >>> re_show('a{2,3} ', ' a aa aaa aaaa b')
       a {aa }{aaa }a{aaa }

  Non-greedy bounded quantifier: "{min,max}?"
  非贪婪边界数目量词："{min,max}?"
      Match -at least- 'min' and -no more than- 'max' occurrences
      of the preceding atom, but try to match as few occurrences
      as allowable.  Scanning is from the left, so a nonminimal
      match may be produced in terms of right-side groupings.
      For example:
      匹配前面原子-至少-'min'次, -至多-'max'次的出现, 但是匹配尽可能
      少的次数. 从左开始扫描, 所以对于遇到右侧组(right-side grouping)而言,
      可能会产生一个非最小匹配. <+此处不是很懂, 留待将来改正+>
      
      >>> from re_show import re_show
      >>> re_show(' a{2,4}?', ' a aa aaa aaaa b')
       a{ aa}{ aa}a{ aa}aa b

      >>> re_show('a{2,4}? ', ' a aa aaa aaaa b')
       a {aa }{aaa }{aaaa }b

  GROUP-LIKE PATTERNS:
  类组模式：

  Python regular expressions may contain a number of pseudo-group
  elements that condition matches in some manner.  With the
  exception of named groups, pseudo-groups are not counted in
  backreferencing.  All pseudo-group patterns have the form
  "(?...)".
  Python正则表达式可能还包含一些条件匹配的伪组. 除了命名组以外, 
  向回引用不会把伪组计算在内. 所有的伪组都有"(?...)"这样的形式. 

  Pattern modifiers: "(?Limsux)"
  模式修饰符："(?Limsux)"
      The pattern modifiers should occur at the very beginning of
      a regular expression pattern.  One or more letters in the
      set "Limsux" may be included.  If pattern modifiers are
      given, the interpretation of the pattern is changed
      globally.  See the discussion of modifier constants below
      or the tutorial for details.
      模式修饰符应该出现在正则表达式模式的最前端, 可以包括集合
      "Limsux"中的一个或多个字母. 如果给出了模式修饰符, 会全局改变
      对于该模式的解释. 预知详情, 请参见下面对修饰符常数的讨论, 
      或者Python学习指南. 

  Comments: "(?#...)"
  注释："(?#...)"
      Create a comment inside a pattern.  The comment is not
      enumerated in backreferences and has no effect on what is
      matched.  In most cases, use of the "(?x)" modifier allows
      for more clearly formatted comments than does "(?#...)".
      在模式内创建一个注释. 不会
      对匹配的内容有任何影响, 向回引用不会将其计算在内. 
      在绝大部分情况下, 指定"(?x)"修饰符
      来使用更清晰格式的注释. 

      >>> from re_show import re_show
      >>> re_show(r'The(?#words in caps) Cat', 'The Cat in the Hat')
      {The Cat} in the Hat

  Non-backreferenced atom: "(?:...)"
  非向回引用原子："(?:...)"
      Match the pattern "...", but do not include the matched
      string as a backreferencable group.  Moreover, methods like
      `re.match.group()` will not see the pattern inside
      non-backreferenced atom.
      匹配模式"...", 但是向回引用组里面不会包含这个匹配. 进一步说, 
      `re.match.group()`这样的方法将无法看到非向回引用原子中的模式. 

      >>> from re_show import re_show
      >>> re_show(r'(?:\w+) (\w+).* \1', 'abc xyz xyz abc')
      {abc xyz xyz} abc

      >>> re_show(r'(\w+) (\w+).* \1', 'abc xyz xyz abc')
      {abc xyz xyz abc}

  Positive Lookahead assertion: "(?=...)"
  肯定性向前断言(Positive Lookahead assertion)："(?=...)" <+需要统一+>
      Match the entire pattern only if the subpattern "..."
      occurs next.  But do not include the target substring
      matched by "..." as part of the match (however, some other
      subpattern may claim the same characters, or some of them).
      只有子模式"..."出现了, 整个模式才算命中. 但是最后命中结果并不包含
      "..."所匹配到的子字符串. (不过也许会有其他子模式会认领
      这个(全部或者部分的)子字符串. )

      >>> from re_show import re_show
      >>> re_show(r'\w+ (?=xyz)', 'abc xyz xyz abc')
      {abc }{xyz }xyz abc

  Negative Lookahead assertion: "(?!...)"
  否定性前瞻断言(Negative Lookahead assertion)："(?!...)" <+需要统一+>      
      Match the entire pattern only if the subpattern "..." does
      -not- occur next.
      只有接着出现的的-不是-子模式"...", 整个模式才算命中.

      >>> from re_show import re_show
      >>> re_show(r'\w+ (?!xyz)', 'abc xyz xyz abc')
      abc xyz {xyz }abc

  Positive Lookbehind assertion: "(?<=...)"
  肯定性向后断言(Positive Lookbehind assertion)： "(?<=...)" 
      Match the rest of the entire pattern only if the subpattern
      "..." occurs immediately prior to the current match point.
      But do not include the target substring matched by "..." as
      part of the match (the same characters may or may not be
      claimed by some prior group(s) in the entire pattern).  The
      pattern "..." must match a fixed number of characters and
      therefore not contain general quantifiers.
      只有子模式"..."就在当前匹配点前面出现了, 才算命中整个模式. 
      但是"..."所命中的子字符串不会包含于命中结果中. (也许整个模式中
      前面的某个/些组会认领这些子字符串). 模式 "..." 必须命中固定数目的
      字符, 因此不能包含常见量词. (译者: 感觉类似'^', 是个零长度匹配. 用于
      确认刚命中的子模式末尾有'...'出现. 下面的例子要匹配的是, 以大写
      字母结尾的单词(单个字母不算), 后跟一个空格.)

      >>> from re_show import re_show
      >>> re_show(r'\w+(?<=[A-Z]) ', 'Words THAT end in capS X')
      Words {THAT }end in {capS }X

  Negative Lookbehind assertion: "(?<!...)"
  否定性向后断言(Negative Lookbehind assertion)： "(?<!...)" <+待统一+>
      Match the rest of the entire pattern only if the subpattern
      "..." does -not- occur immediately prior to the current
      match point.  The same characters may or may not be claimed
      by some prior group(s) in the entire pattern.  The pattern
      "..." must match a fixed number of characters, and
      therefore not contain general quantifiers.
      就在当前匹配点前面, 只有子模式"..."-没有-发生, 才算命中整个
      模式. 但是"..."所匹配到的子字符串并不会包含于命中结果中. 
      模式"..."必须匹配固定数目的字符, 因此不能包含常见量词. 

      >>> from re_show import re_show
      >>> re_show(r'\w+(?<![A-Z]) ', 'Words THAT end in capS X')
      {Words }THAT {end }{in }capS X

  Named group identifier: "(?P<name>)"
  命名组标识符："(?P<name>)"
      Create a group that can be referred to by the name 'name'
      as well as in enumerated backreferences.  The forms below
      are equivalent.
      创建一个命名组, 除了使用计数向回引用以外, 还有使用名字'name'
      来引用. 下面几个形式是等价的. 

      >>> from re_show import re_show
      >>> re_show(r'(\w+) (\w+).* \1', 'abc xyz xyz abc')
      {abc xyz xyz abc}

      >>> re_show(r'(?P<first>\w+) (\w+).* (?P=first)', 'abc xyz xyz abc')
      {abc xyz xyz abc}

      >>> re_show(r'(?P<first>\w+) (\w+).* \1', 'abc xyz xyz abc')
      {abc xyz xyz abc}

  Named group backreference: "(?P=name)"
  命名组向回引用："(?P=name)"
      Backreference a group by the name 'name' rather than by
      escaped group number.  The group name must have been
      defined earlier by "(?P<name>)", or an error is raised.
      使用名字'name'来向回引用一个组, 而不是使用转义的组编号. 
      组名字'name'必须在前面已经用"(?P<name>)"定义过, 否则会
      引发一个错误. 

  CONSTANTS:
  常数：

  A number of constants are defined in the [re] modules that act
  as modifiers to many [re] functions.  These constants are
  independent bit-values, so that multiple modifiers may be
  selected by bitwise disjunction of modifiers.  For example:
  [re]模块定义了一些常数, 可以用作许多[re]函数的修饰符. 这些常数
  都是互相独立的比特值, 通过按位“或”运算, 可以选择多个修饰符. 
  请看例子：

    >>> import re
    >>> c = re.compile('cat|dog', re.IGNORECASE | re.UNICODE)

  re.I, re.IGNORECASE
      Modifier for case-insensitive matching.  Lowercase and
      uppercase letters are interchangeable in patterns modified
      with this modifier.  The prefix '(?i)' may also be used
      inside the pattern to achieve the same effect.
      该修饰符用于指示大小写不敏感匹配. 在被此修饰符修改过的模式中, 
      大写字母和小写字母是可以互换的. 也可以在模式中使用前缀'(?i)'
      来获取相同效果. 

  re.L, re.LOCALE
      Modifier for locale-specific matching of '\w', '\W', '\b',
      and '\B'.  The prefix '(?L)' may also be used inside the
      pattern to achieve the same effect.
      该修饰符指示选用本地化的对应于'\w', '\W', '\b'和 '\B'字符集. 
      也可以在模式中使用前缀'(?L)'来获取相同效果. 
## 此处意译, 感觉不是很通畅, 需要改进

  re.M, re.MULTILINE
      Modifier to make '^' and '$' match the beginning and end,
      respectively, of -each- line in the target string rather
      than the beginning and end of the entire target string.
      The prefix '(?m)' may also be used inside the pattern to
      achieve the same effect.
      本修饰符使得'^'和'$'分别匹配目标字符串中-每一行-的行首和行尾, 
      而不是仅仅目标字符串的开头和末尾. 也可以在模式中使用前缀'(?m)'
      来获取相同效果. 

  re.S, re.DOTALL
      Modifier to allow '.' to match a newline character.
      Otherwise, '.' matches every character -except- newline
      characters.  The prefix '(?s)' may also be used inside the
      pattern to achieve the same effect.
      默认'.'可以匹配除了新行符外的任何字符, 使用本修饰符以后, 
      新行符也是可以匹配的内容中了. 也可以在模式中使用前缀'(?s)'
      来获取相同效果. 

  re.U, re.UNICODE
      Modifier for Unicode-property matching of '\w', '\W', '\b',
      and '\B'.  Only relevant for Unicode targets.  The prefix
      '(?u)' may also be used inside the pattern to achieve the
      same effect.
      该修饰符指示选用Unicode版本的对应于'\w', '\W', '\b'和 '\B'
      字符集. 只和Unicode目标有关. 也可以在模式中使用前缀'(?u)'来
      获取相同效果. 

  re.X, re.VERBOSE
      Modifier to allow patterns to contain insignificant
      whitespace and end-of-line comments.  Can significantly
      improve readability of patterns.  The prefix '(?x)' may
      also be used inside the pattern to achieve the same effect.
      本修饰符允许模式包含无意义的空白以及行末的注释, 这样可以
      显著提高模式的可读性. 也可以在模式中使用前缀'(?x)'来
      获取相同效果. 

  re.engine
      The regular expression engine currently in use.  Only
      supported in Python 2.0+, where it normally is set to the
      string 'sre'.  The presence and value of this constant can
      be checked to make sure which underlying implementation is
      running, but this check is rarely necessary.
      记载了当前使用的正则表达式引擎. 只在Python 2.0 以后版本中支持, 
      通常被设置为字符串'sre'. 可以通过检查本常数是否存在以及它的值
      来确定底层模块是否正在运行, 不过这种检查几乎没有必要. 
      

  FUNCTIONS:
  函数：

  For all [re] functions, where a regular expression pattern
  'pattern' is an argument, 'pattern' may be either a compiled
  regular expression or a string.
  对于所有的[re]函数, 正则表达式模式'pattern'都是参数, 它可以是
  编译后的正则表达式, 也可以是字符串. 

  re.escape(s)
      Return a string with all non-alphanumeric characters
      escaped.  This (slightly scattershot) conversion makes an
      arbitrary string suitable for use in a regular expression
      pattern (matching all literals in original string).
      返回一个字符串, 其中所有的非数字字母字符都已经被转义了. 虽说
      有点毫无目标, 但是这个函数可以转换任意字符串以用于正则表达式
      模式(可用于匹配原来字符串中文本文字). 
## literals -> 书面文字, 不知道是否妥当

      >>> import re
      >>> print re.escape("(*@&^$@|")
      \(\*\@\&\^\$\@\|

  re.findall(pattern=..., string=...)
      Return a list of all nonoverlapping occurrences of
      'pattern' in 'string'.  If 'pattern' consists of several
      groups, return a list of tuples where each tuple contains a
      match for each group.  Length-zero matches are included in
      the returned list, if they occur.
      返回一个列表, 每个元素对应'pattern'在'string'中的一次出现, 
      注意是互不重叠的. 如果'pattern'包含了若干个子组, 将返回一个元组
      列表, 每个元组对应一次命中, 其中元组的每个元素表示一个命中的组. 
      如果出现的话, 零长度匹配也被包括在返回的列表中. 

      >>> import re
      >>> re.findall(r'\b[a-z]+\d+\b', 'abc123 xyz666 lmn-11 def77')
      ['abc123', 'xyz666', 'def77']
      >>> re.findall(r'\b([a-z]+)(\d+)\b', 'abc123 xyz666 lmn-11 def77')
      [('abc', '123'), ('xyz', '666'), ('def', '77')]

      SEE ALSO, `re.search()`, `mx.TextTools.findall()`
      参见`re.search()`, `mx.TextTools.findall()`

  re.purge()
      Clear the regular expression cache.  The [re] module keeps
      a cache of implicitly compiled regular expression patterns.
      The number of patterns cached differs between Python
      versions, with more recent versions generally keeping 100
      items in the cache.  When the cache space becomes full, it
      is flushed automatically.  You could use `re.purge()` to
      tune the timing of cache flushes.  However, such tuning is
      approximate at best:  patterns that are used repeatedly are
      much better off explicitly compiled with `re.compile()` and
      then used explicitly as named objects.
      清空正则表达式的缓存. [re]模块有个缓存可以用来保存隐式编译过的
      正则表达式模式. 不同Python版本缓存不同数目的模式, 例如最近的版本
      通常在缓存中保存100个. 当缓存空间满了之后, 它将会自动清空. 你
      可以使用`re.purge()`来改变清空缓存的时间间隔. 但是, 下面的做法
      比较接近最佳方案：需要重复使用的模式最好使用`re.compile()`
      显式编译, 然后作为命名对象显式使用. 

  re.split(pattern=..., string=... [,maxsplit=0])
      Return a list of substrings of the second argument 'string'.
      The first argument 'pattern' is a regular expression that
      delimits the substrings.  If 'pattern' contains groups, the
      groups are included in the resultant list.  Otherwise,
      those substrings that match 'pattern' are dropped, and only
      the substrings between occurrences of 'pattern' are
      returned.
      返回第二个参数'string'的子字符串列表. 第一个参数'pattern'是
      一个正则表达式, 作为子字符串之间的定界符. 如果'pattern'包含了
      子组, 后者将被包括在结果列表里, 否则, 'pattern'命中的子字符串
      将被丢弃, 只有出现在'pattern'之间的子字符串被返回. 

      If the third argument 'maxsplit' is specified as a positive
      integer, no more than 'maxsplit' items are parsed into the
      list, with any leftover contained in the final list
      element.
      如果指定了一个正整数作为第三个参数'maxsplit', 最多会解析'maxsplit'
      个元素, 剩下内容被置于最后一个列表元素内. 

      >>> import re
      >>> re.split(r'\s+', 'The Cat in the Hat')
      ['The', 'Cat', 'in', 'the', 'Hat']
      >>> re.split(r'\s+', 'The Cat in the Hat', maxsplit=3)
      ['The', 'Cat', 'in', 'the Hat']
      >>> re.split(r'(\s+)', 'The Cat in the Hat')
      ['The', ' ', 'Cat', ' ', 'in', ' ', 'the', ' ', 'Hat']
      >>> re.split(r'(a)(t)', 'The Cat in the Hat')
      ['The C', 'a', 't', ' in the H', 'a', 't', '']
      >>> re.split(r'a(t)', 'The Cat in the Hat')
      ['The C', 't', ' in the H', 't', '']

      SEE ALSO, `string.split()`
      参见`string.split()`

  re.sub(pattern=..., repl=..., string=... [,count=0])
      Return the string produced by replacing every
      nonoverlapping occurrence of the first argument 'pattern'
      with the second argument 'repl' in the third argument
      'string'.  If the fourth argument 'count' is specified, no
      more than 'count' replacements will be made.
      用'repl'替换'string'中所有不重叠的'pattern', 并返回替换后的
      结果. 如果指定了第四个参数'count', 替换次数不超过'count'. 

      The second argument 'repl' is most often a regular
      expression pattern as a string.  Backreferences to groups
      matched by 'pattern' may be referred to by enumerated
      backreferences using the usual escaped numbers.  If
      backreferences in 'pattern' are named, they may also be
      referred to using the form "\g<name>" (where 'name' is the
      name given the group in 'pat').  As well, enumerated
      backreferences may optionally be referred to using the
      form "\g<num>", where 'num' is an integer between 1 and 99.
      Some examples:
      第二个参数'repl'通常是一个正则表达式模式, 而非普通字符串. 
      如想向回引用'pattern'匹配的的子组, 可以使用转义后的数字, 也
      就是计数向回引用. 如果'pattern'中的是命名向回引用, 也可以
      使用格式"\g<name>"来引用它们, 这里'name'是'pattern'中给子组
      的名字. 另外, 计数向回引用也可以选用格式"\g<num>", 其中'num'
      是一个介于1到99之间的整数. 下面有一些例子：
      
## 原文'pat'应为'pattern', 不知是否作者有意为之. 

      >>> import re
      >>> s = 'abc123 xyz666 lmn-11 def77'
      >>> re.sub(r'\b([a-z]+)(\d+)', r'\2\1 :', s)
      '123abc : 666xyz : lmn-11 77def :'
      >>> re.sub(r'\b(?P<lets>[a-z]+)(?P<nums>\d+)', r'\g<nums>\g<1> :', s)
      '123abc : 666xyz : lmn-11 77def :'
      >>> re.sub('A', 'X', 'AAAAAAAAAA', count=4)
      'XXXXAAAAAA'

      A variant manner of calling `re.sub()` uses a function
      object as the second argument 'repl'.  Such a callback
      function should take a MatchObject as an argument and
      return a string.  The 'repl' function is invoked for each
      match of 'pattern', and the string it returns is
      substituted in the result for whatever 'pattern' matched.
      For example:
      另外一种调用`re.sub()`的方法是使用函数对象作为第二个参数
      'repl', 该回调函数应该接受一个MatchObject作为参数, 并返回
      一个字符串. 对于每个匹配的'pattern', 'repl'函数都将被调用, 
      返回的字符串用于替换结果中'pattern'匹配到的内容. 请看例子：
      

      >>> import re
      >>> sub_cb = lambda pat: '('+`len(pat.group())`+')'+pat.group()
      >>> re.sub(r'\w+', sub_cb, 'The length of each word')
      '(3)The (6)length (2)of (4)each (4)word'

      Of course, if 'repl' is a function object, you can take
      advantage of side effects rather than (or instead of)
      simply returning modified strings.  For example:
      当然, 如果'repl'是一个函数对象, 它会返回一个修改过的字符串, 
      除此之外, 你可以充分利用它的副作用. 请看例子：
## 意译

      >>> import re
      >>> def side_effects(match):
      ...     # Arbitrarily complicated behavior could go here...
      ...     print len(match.group()), match.group()
      ...     return match.group()  # unchanged match
      ...     # 此处可以进行任意复杂活动
      ...     return match.group()  # 匹配未改变
      ...
      >>> new = re.sub(r'\w+', side_effects, 'The length of each word')
      3 The
      6 length
      2 of
      4 each
      4 word
      >>> new
      'The length of each word'

      Variants on callbacks with side effects could be turned
      into complete string-driven programs (in principle, a
      parser and execution environment for a whole programming
      language could be contained in the callback function, for
      example).
      不同的回调函数加上副作用, 可以建立一个完备的字符串驱动的程序. 
      (比如说, 原则上, 回调函数可以包含一个解析器以及整个编程语言
      的执行环境. )

      SEE ALSO, `string.replace()`
      参见`string.replace()`

  re.subn(pattern=..., repl=..., string=... [,count=0])
      Identical to `re.sub()`, except return a 2-tuple with the
      new string and the number of replacements made.
      和`re.sub()`相同, 除了返回长度为二的元组, 其一为新的字符串, 
      其二为替换的次数. 

      >>> import re
      >>> s = 'abc123 xyz666 lmn-11 def77'
      >>> re.subn(r'\b([a-z]+)(\d+)', r'\2\1 :', s)
      ('123abc : 666xyz : lmn-11 77def :', 3)

      SEE ALSO, `re.sub()`
      参见`re.sub()`

  CLASS FACTORIES:
  类工厂：

  As with some other Python modules, primarily ones written in C,
  [re] does not contain true classes that can be specialized.
  Instead, [re] has several factory-functions that return
  instance objects.  The practical difference is small for most
  users, who will simply use the methods and attributes of
  returned instances in the same manner as those produced by
  true classes.
  和其他Python模块一样, 特别是那些使用C编写的模块, [re]并不包含
  真正的类以供用户衍生. 相反, [re]有几个工厂函数可以用来返回实例对象. 
  在实际运用中的区别对于绝大部分用户而言, 都是非常小的, 他们只需要
  使用返回实例的方法和属性, 就像这些实例是由真正的类一样. 
##specialized 如何翻译？应该衍生子类吧

  re.compile(pattern=... [,flags=...])
      Return a PatternObject based on pattern string 'pattern'. If
      the second argument 'flags' is specified, use the modifiers
      indicated by 'flags'.  A PatternObject is interchangeable
      with a pattern string as an argument to [re] functions.
      However, a pattern that will be used frequently within an
      application should be compiled in advance to assure that it
      will not need recompilation during execution.  Moreover, a
      compiled PatternObject has a number of methods and
      attributes that achieve effects equivalent to [re]
      functions, but which are somewhat more readable in some
      contexts.  For example:
      返回一个基于模式字符串'pattern'的PatternObject. 如果指定了
      第二个参数'flags', 则把它们用作修饰符. 作为[re]函数中的参数, 
      PatternObject和模式字符串是可以互换的. 但是, 如果需要
      频繁地使用同一个模式, 那么最好预先编译, 这样可以避免在执行中
      重新编译它. 而且, 编译过的PatternObject拥有一些和[re]函数
      同样效果的方法和属性, 而且它们在某些上下文中更具
      可读性. 请看例子：

      >>> import re
      >>> word = re.compile('[A-Za-z]+')
      >>> word.findall('The Cat in the Hat')
      ['The', 'Cat', 'in', 'the', 'Hat']
      >>> re.findall(word, 'The Cat in the Hat')
      ['The', 'Cat', 'in', 'the', 'Hat']

  re.match(pattern=..., string=... [,flags=...])
      Return a MatchObject if an initial substring of the second
      argument 'string' matches the pattern in the first argument
      'pattern'.  Otherwise return None.  A MatchObject, if
      returned, has a variety of methods and attributes to
      manipulate the matched pattern--but notably a MatchObject
      is -not- itself a string.
      如果'string'的最开头的子字符串匹配到了模式'pattern', 返回
      一个MatchObject, 否则返回None. MatchObject, 如果返回的话, 
      拥有一系列的方法和属性来操作匹配的模式--注意MatchObject本身
      并-不是-字符串. 

      Since `re.match()` only matches initial substrings,
      `re.search()` is more general.  `re.search()` can be
      constrained to itself match only initial substrings by
      prepending "\A" to the pattern matched.
      因为`re.match()`只匹配开始的子字符串, `re.search()`显得
      更通用. 当然也可以限制`re.search()`只匹配开始的子字符串, 
      只要在匹配模式开头加上"\A"即可. 

      SEE ALSO, `re.search()`, `re.compile.match()`
      参见`re.search()`, `re.compile.match()`

  re.search(pattern=..., string=... [,flags=...])
      Return a MatchObject corresponding to the leftmost
      substring of the second argument 'string' that matches the
      pattern in the first argument 'pattern'.  If no match is
      possible, return None.  A matched string can be of zero
      length if the pattern allows that (usually not what is
      actually desired).  A MatchObject, if returned, has a
      variety of methods and attributes to manipulate the matched
      pattern--but notably a MatchObject is -not- itself a
      string.
      根据'string'中匹配'pattern'的子字符串中最左边的那个, 返回
      一个MatchObject；如果没有任何匹配, 返回None. 匹配的字符串可以
      是零长度, 只要模式允许如此(通常来说不是实际想要的). 
      MatchObject拥有一系列的方法和属性来操作匹配的模式--注意
      MatchObject本身并-不是-字符串. 

      SEE ALSO, `re.match()`, `re.compile.search()`
      参见`re.match()`, `re.compile.search()`

  METHODS AND ATTRIBUTES:
  方法和属性:

  re.compile.findall(s)
      Return a list of nonoverlapping occurrences of the
      PatternObject in 's'.  Same as 're.findall()' called with
      the PatternObject.
      返回一个列表, 每个元素是PatternObject在's'中的一次出现, 
      注意彼此之间是互不重叠的. 等同于使用PatternObject调用
      're.findall()'. 

      SEE ALSO, `re.findall()`
      参见`re.findall()`

  re.compile.flags
      The numeric sum of the flags passed to `re.compile()`
      in creating the PatternObject.  No formal guarantee is
      given by Python as to the values assigned to modifier
      flags, however.  For example:
      在创建PatternObject时, 传递给`re.compile()`的标志(译注：即
      前面所说修饰符)之数值和. 但是, Python并未对赋给修饰符标识
      的值给出正式的保证. 例如:

      >>> import re
      >>> re.I,re.L,re.M,re.S,re.X
      (2, 4, 8, 16, 64)
      >>> c = re.compile('a', re.I | re.M)
      >>> c.flags
      10

  re.compile.groupindex
      A dictionary mapping group names to group numbers.  If no
      named groups are used in the pattern, the dictionary is
      empty.  For example:
      这是一个字典, 将子组名字映射到子组编号. 如果该模式中并未使用
      任何命名子组, 此字典将是空的. 请看例子：

      >>> import re
      >>> c = re.compile(r'(\d+)(\[A-Z]+)([a-z]+)')
      >>> c.groupindex
      {}
      >>> c =
      re.compile(r'(?P<nums>\d+)(?P<caps>\[A-Z]+)(?P<lowers>[a-z]+)')
      >>> c.groupindex
      {'nums': 1, 'caps': 2, 'lowers': 3}

  re.compile.match(s [,start [,end]])
      Return a MatchObject if an initial substring of the first
      argument 's' matches the PatternObject.  Otherwise, return
      None.  A MatchObject, if returned, has a variety of methods
      and attributes to manipulate the matched pattern--but
      notably a MatchObject is -not- itself a string.
      如果'string'的最开头的子字符串匹配到了模式'pattern', 返回
      一个MatchObject, 否则返回None. MatchObject, 如果返回的话, 
      拥有一系列的方法和属性来操作匹配的模式--注意MatchObject本身
      并-不是-字符串. 


      In contrast to the similar function `re.match()`, this
      method accepts optional second and third arguments 'start'
      and 'end' that limit the match to substring within 's'.
      In most respects specifying 'start' and 'end' is similar to
      taking a slice of 's' as the first argument.  But when
      'start' and 'end' are used, "^" will only match the true
      start of 's'.  For example:
      和相似的函数`re.match()`相比, 这个方法还接受可选的第二和第三
      个参数, 分别是'start'和'end', 用来限制对's'中子字符串的匹配. 
      具体一点说, 'start'和'end'就类似于取出's'中的一个切片作为第一个
      参数. 但是当使用这两个参数时, "^"将只匹配's'的真正开头. 例如：

      >>> import re
      >>> s = 'abcdefg'
      >>> c = re.compile('^b')
      >>> print c.match(s, 1)
      None
      >>> c.match(s[1:])
      <SRE_Match object at 0x10c440>
      >>> c = re.compile('.*f$')
      >>> c.match(s[:-1])
      <SRE_Match object at 0x116d80>
      >>> c.match(s,1,6)
      <SRE_Match object at 0x10c440>

      SEE ALSO, `re.match()`, `re.compile.search()`
      参见`re.match()`, `re.compile.search()`

  re.compile.pattern
      The pattern string underlying the compiled MatchObject.
      生成编译过的MatchObject的模式字符串. 

      >>> import re
      >>> c = re.compile('^abc$')
      >>> c.pattern
      '^abc$'

  re.compile.search(s [,start [,end]])
      Return a MatchObject corresponding to the leftmost
      substring of the first argument 'string' that matches the
      PatternObject.  If no match is possible, return None.  A
      matched string can be of zero length if the pattern allows
      that (usually not what is actually desired).  A
      MatchObject, if returned, has a variety of methods and
      attributes to manipulate the matched pattern--but notably a
      MatchObject is -not- itself a string.
      根据'string'中匹配'pattern'的子字符串中最左边的那个, 返回
      一个MatchObject；如果没有命中, 返回None. 匹配的字符串可以
      是零长度, 只要模式允许如此(通常来说不是实际想要的). 
      MatchObject拥有一系列的方法和属性来操作匹配的模式--注意
      MatchObject本身并-不是-字符串. 
## 这个解说和re.search是一样的, 作者是不是要骗稿费阿？哈哈
## 当然下面说了不同的地方

      In contrast to the similar function `re.search()`, this
      method accepts optional second and third arguments 'start'
      and 'end' that limit the match to a substring within 's'.
      In most respects specifying 'start' and 'end' is similar to
      taking a slice of 's' as the first argument.  But when
      'start' and 'end' are used, "^" will only match the true
      start of 's'.  For example:
      和相似的函数`re.search()`相比, 这个方法还接受可选的第二和第三
      个参数, 分别是'start'和'end', 用来限制对's'中子字符串的匹配. 
      具体一点说'start'和'end'就类似于取出's'中的一个切片作为第一个
      参数. 但是当使用这两个参数时, "^"将只匹配's'的真正开端. 例如：

      >>> import re
      >>> s = 'abcdefg'
      >>> c = re.compile('^b')
      >>> c = re.compile('^b')
      >>> print c.search(s, 1),c.search(s[1:])
      None <SRE_Match object at 0x117980>
      >>> c = re.compile('.*f$')
      >>> print c.search(s[:-1]),c.search(s,1,6)
      <SRE_Match object at 0x51040> <SRE_Match object at 0x51040>

      SEE ALSO, `re.search()`, `re.compile.match()`
      参见`re.search()`, `re.compile.match()`

  re.compile.split(s [,maxsplit])
      Return a list of substrings of the first argument 's'.  If
      thePatternObject contains groups, the groups are included
      in the resultant list.  Otherwise, those substrings that
      match PatternObject are dropped, and only the substrings
      between occurrences of 'pattern' are returned.
      返回's'的子字符串组成的一个列表. 如果PatternObject包含子组, 
      后者将被包含在结果列表中. 否则, 那些PatternObject命中的子
      字符串将被抛弃, 只有出现在'pattern'之间的子字符串才被返回. 

      If the second argument 'maxsplit' is specified as a
      positive integer, no more than 'maxsplit' items are parsed
      into the list, with any leftover contained in the final
      list element.
      如果指定了一个正整数作为第二个参数'maxsplit', 最多解析'maxsplit'
      个元素, 剩下内容被置于最后一个列表元素内. 

      `re.compile.split()` is identical in behavior to
      `re.split()`, simply spelled slightly differently.  See the
      documentation of the latter for examples of usage.
      `re.compile.split()`和`re.split()`的行为是完全一样的, 
      除了拼写稍有不同. 参见后面的文档以获得使用范例. 
## 本段前后与前面re.xx很多雷同, 所有很多翻译都是复制拷贝过来的的. 
## 校对时注意检查. 

      SEE ALSO, `re.split()`
      参见`re.split()`

  re.compile.sub(repl, s [,count=0])
      Return the string produced by replacing every
      nonoverlapping occurrence of the PatternObject with the
      first argument 'repl' in the second argument 'string'.  If
      the third argument 'count' is specified, no more than
      'count' replacements will be made.
      用'repl'替换'string'中每个不重叠的PatternObject, 并返回替换后的
      结果. 如果指定了第三个参数'count', 替换次数不超过'count'. 

      The first argument 'repl' may be either a regular
      expression pattern as a string or a callback function.
      Backreferences may be named or enumerated.
      第一个参数'repl'可以是字符串格式的正则表达式模式, 也可以
      是一个回调函数. 向回引用可以是命名的或者计数的. 

      `re.compile.sub()` is identical in behavior to `re.sub()`,
      simply spelled slightly differently.  See the documentation
      of the latter for a number of examples of usage.
      `re.compile.sub()`和`re.sub()`的行为是完全一样的, 除了
      拼写上略有不同. 参见后面的文档以获得使用范例. 

      SEE ALSO, `re.sub()`, `re.compile.subn()`
      参见`re.sub()`, `re.compile.subn()`

  re.compile.subn()
      Identical to `re.compile.sub()`, except return a 2-tuple
      with the new string and the number of replacements made.
      和`re.compile.sub()`完全一样, 除了返回长度为二的元组, 
      分别为新的字符串和替换的次数. 

      `re.compile.subn()` is identical in behavior to
      `re.subn()`, simply spelled slightly differently.  See the
      documentation of the latter for examples of usage.
      `re.compile.subn()`和`re.subn()`的行为是完全一样的, 除了
      拼写上略有不同. 参见后面的文档以获得使用范例. 

      SEE ALSO, `re.subn()`, `re.compile.sub()`
      参见`re.subn()`, `re.compile.sub()`

  Note:  The arguments to each "MatchObject" method are listed on
  the `re.match()` line, with ellipses given on the `re.search()`
  line.  All arguments are identical since `re.match()` and
  `re.search()` return the very same type of object.
  注意：每个"MatchObject"方法的参数都只在`re.match()`行列出, 而
  `re.search()`中以省略号代替. 这是因为`re.match()`和`re.search()`
  总是返回同样类型的对象, 因此所有参数都是相同的. 

  re.match.end([group])
  re.search.end(...)
      The index of the end of the target substring matched by the
      MatchObject.  If the argument 'group' is specified, return
      the ending index of that specific enumerated group.
      Otherwise, return the ending index of group 0 (i.e., the
      whole match). If 'group' exists but is the part of an
      alternation operator that is not used in the current
      match, return -1. If `re.search.end()` returns the same
      non-negative value as `re.search.start()`, then 'group' is
      a zero-width substring.
      MatchObject命中的目标子字符串的末尾所处下标. 如果指定参数
      'group', 返回指定计数子组的结尾下标；否则返回第0子组的结尾
      下标(也就是说, 整个匹配). 如果'group'存在, 但是属于某个当前
      匹配并未命中的管道符, 将返回-1. 如果`re.search.end()`和
      `re.search.start()`返回同样的非负值, 那么'group'是个零长度
      的子字符串.  

      >>> import re
      >>> m = re.search('(\w+)((\d*)| )(\w+)','The Cat in the Hat')
      >>> m.groups()
      ('The', ' ', None, 'Cat')
      >>> m.end(0), m.end(1), m.end(2), m.end(3), m.end(4)
      (7, 3, 4, -1, 7)

  re.match.endpos, re.search.endpos
      The end position of the search.  If `re.compile.search()`
      specified an 'end' argument, this is the value, otherwise
      it is the length of the target string.  If `re.search()` or
      `re.match()` are used for the search, the value is always
      the length of the target string.
      搜索结束的位置. 如果`re.compile.search()`指定了'end'参数, 
      则返回该参数值, 否则就返回目标字符串的长度. 如果搜索使用了
      `re.search()`或`re.match()`, 总是返回目标字符串的长度. 

      SEE ALSO, `re.compile.search()`, `re.search()`, `re.match()`
      参见`re.compile.search()`, `re.search()`, `re.match()`

  re.match.expand(template)
  re.search.expand(...)
      Expand backreferences and escapes in the argument 'template'
      based on the patterns matched by the MatchObject.  The
      expansion rules are the same as for the 'repl' argument to
      `re.sub()`.  Any nonescaped characters may also be
      included as part of the resultant string.  For example:
      根据MatchObject命中的模式, 扩展参数'template'中的向回引用, 
      以及转义字符. 扩展规则和`re.sub()`中的'repl'参数一样. 'template'
      中任何没有被转义的字符仍将出现在结果字符串中. 例子如下：

      >>> import re
      >>> m = re.search('(\w+) (\w+)','The Cat in the Hat')
      >>> m.expand(r'\g<2> : \1')
      'Cat : The'

  re.match.group([group [,...]])
  re.search.group(...)
      Return a group or groups from the MatchObject.  If no
      arguments are specified, return the entire matched
      substring.  If one argument 'group' is specified, return
      the corresponding substring of the target string.  If
      multiple arguments 'group1, group2, ...' are specified,
      return a tuple of corresponding substrings of the target.
      从MatchObject中返回一个或多个子组. 如果没有指定任何参数, 
      返回整个命中的子字符串. 如果只指定了一个参数'group', 返回
      目标字符串中对应的子字符串. 如果指定了多个参数
      'group1, group2, ...', 返回一个元组, 其中包含了对应的那些
      子字符串. 

      >>> import re
      >>> m = re.search(r'(\w+)(/)(\d+)','abc/123')
      >>> m.group()
      'abc/123'
      >>> m.group(1)
      'abc'
      >>> m.group(1,3)
      ('abc', '123')

      SEE ALSO, `re.search.groups()`, `re.search.groupdict()`
      参见`re.search.groups()`, `re.search.groupdict()`

  re.match.groupdict([defval])
  re.search.groupdict(...)
      Return a dictionary whose keys are the named groups in the
      pattern used for the match.  Enumerated but unnamed groups
      are not included in the returned dictionary.  The values of
      the dictionary are the substrings matched by each group in
      the MatchObject.  If a named group is part of an
      alternation operator that is not used in the current match,
      the value corresponding to that key is None, or 'defval' if
      an argument is specified.
      返回一个字典, 它的键(key)是匹配模式中的命名子组. 没有命名的
      计数子组不会被包含在此字典中. 字典的值(value)是MatchObject中的每个
      子组命中的子字符串. 如果某命名子组属于某个当前匹配并未命中
      的管道符, 对应其键的值是None, 或者'defval', 如果给出参数'defval'的话.

      >>> import re
      >>> m = re.search(r'(?P<one>\w+)((?P<tab>\t)|( ))(?P<two>\d+)','abc 123')
      >>> m.groupdict()
      {'one': 'abc', 'tab': None, 'two': '123'}
      >>> m.groupdict('---')
      {'one': 'abc', 'tab': '---', 'two': '123'}

      SEE ALSO, `re.search.groups()`
      参见`re.search.groups()`

  re.match.groups([defval])
  re.search.groups(...)
      Return a tuple of the substrings matched by groups in the
      MatchObject.  If a group is part of an alternation operator
      that is not used in the current match, the tuple element at
      that index is None, or 'defval' if an argument is
      specified.
      返回一个元组, 包含了MatchObject中各子组命中的子字符串. 
      如果某子组属于某个当前匹配并未命中的管道符, 对应其下标的
      元组元素的值是None, 或者'defval', 如果给出参数'defval'的话. 

      >>> import re
      >>> m = re.search(r'(\w+)((\t)|(/))(\d+)','abc/123')
      >>> m.groups()
      ('abc', '/', None, '/', '123')
      >>> m.groups('---')
      ('abc', '/', '---', '/', '123')

      SEE ALSO, `re.search.group()`, `re.search.groupdict()`
      参见`re.search.group()`, `re.search.groupdict()`

  re.match.lastgroup, re.search.lastgroup
      The name of the last matching group, or None if the last
      group is not named or if no groups compose the match.
      返回最后命中子组的名字, 如果最后一个子组并未命名, 或者该匹配
      并无任何子组, 则返回None. 

  re.match.lastindex, re.search.lastindex
      The index of the last matching group, or None if no groups
      compose the match.
      返回最后一次匹配到的子组的下标, 如果该匹配并无任何子组, 
      则返回None. 

  re.match.pos, re.search.pos
      The start position of the search.  If `re.compile.search()`
      specified a 'start' argument, this is the value, otherwise
      it is 0.  If `re.search()` or `re.match()` are used for the
      search, the value is always 0.
      返回本次搜索的起始位置. 如果`re.compile.search()`指定了一个
      'start'参数, 返回其值, 否则返回0. 如果搜索使用了
      `re.search()`或`re.match()`, 总是返回0. 

      SEE ALSO, `re.compile.search()`, `re.search()`, `re.match()`
      参见`re.compile.search()`, `re.search()`, `re.match()`

  re.match.re, re.search.re
      The PatternObject used to produce the match.  The actual
      regular expression pattern string must be retrieved from
      the PatternObject's 'pattern' method:
      返回用于进行匹配的PatternObject. 实际的正则表达式模式字符串
      必须使用PatternObject的'pattern'方法来获取. 

      >>> import re
      >>> m = re.search('a','The Cat in the Hat')
      >>> m.re.pattern
      'a'

  re.match.span([group])
  re.search.span(...)
      Return the tuple composed of the return values of
      're.search.start(group)' and 're.search.end(group)'.  If
      the argument 'group' is not specified, it defaults to 0.
      返回一个元组, 由're.search.start(group)'和
      're.search.end(group)'返回的值组成. 如果未指定参数'group',
      使用默认值0. 

      >>> import re
      >>> m = re.search('(\w+)((\d*)| )(\w+)','The Cat in the Hat')
      >>> m.groups()
      ('The', ' ', None, 'Cat')
      >>> m.span(0), m.span(1), m.span(2), m.span(3), m.span(4)
      ((0, 7), (0, 3), (3, 4), (-1, -1), (4, 7))

  re.match.start([group])
  re.search.start(...)
      The index of the end of the target substring matched by the
      MatchObject.  If the argument 'group' is specified, return
      the ending index of that specific enumerated group.
      Otherwise, return the ending index of group 0 (i.e., the
      whole match). If 'group' exists but is the part of an
      alternation operator that is not used in the current
      match, return -1. If `re.search.end()` returns the same
      non-negative value as `re.search.start()`, then 'group' is
      a zero-width substring.
      MatchObject匹配的目标子字符串的末尾所处下标. 如果指定参数
      'group', 返回指定计数子组的结尾下标；否则返回第0子组的结尾
      下标(也就是说, 整个匹配). 如果'group'存在, 但是属于某个当前
      匹配并未使用到的管道符, 将返回-1. 如果`re.search.end()`和
      `re.search.start()`返回同样的非负值, 那么'group'是个零长度
      的子字符串. 
## 原文疑为作者拷贝粘贴后没有修改. 不知如何修改

      >>> import re
      >>> m = re.search('(\w+)((\d*)| )(\w+)','The Cat in the Hat')
      >>> m.groups()
      ('The', ' ', None, 'Cat')
      >>> m.start(0), m.start(1), m.start(2), m.start(3), m.start(4)
      (0, 0, 3, -1, 4)

  re.match.string, re.search.string
      The target string in which the match occurs.
      匹配所处的目标字符串. 

      >>> import re
      >>> m = re.search('a','The Cat in the Hat')
      >>> m.string
      'The Cat in the Hat'

  EXCEPTIONS:
  异常：

  re.error
      Exception raised when an invalid regular expression string
      is passed to a function that would produce a compiled
      regular expression (including implicitly).
      当一个无效的正则表达式传入函数后, 后者将会生成(包括隐性编译)
      一个编译好的正则表达式, 此时会引发一个异常. 

