APPENDIX -- A DATA COMPRESSION PRIMER
附录 -- 数据压缩入门
-------------------------------------------------------------------

SECTION -- Introduction
节 -- 介绍
-------------------------------------------------------------------

  See Section 2.2.5 for details on compression capabilities
  included in the Python standard library. This appendix is
  intended to provide readers who are unfamiliar with data
  compression a basic background on its techniques and theory. The
  final section of this appendix provides a practical
  example--accompanied by some demonstration code--of a
  Huffman-inspired custom encoding.
  查看2.2.5小节，以获取Python标准库中包含的压缩能力。
  本附录试图向不熟悉数据压缩的读者，提供关于数据压缩技术和理论
  的基础背景知识。本附录的最后一节提供了一个实践例子--还有一些
  展示代码--此例子使用的自定义编码是从霍夫曼编码获得的灵感。
  

  Data compression is widely used in a variety of programming
  contexts.  All popular operating systems and programming
  languages have numerous tools and libraries for dealing with
  data compression of various sorts.  The right choice of
  compression tools and libraries for a particular application
  depends on the characteristics of the data and application in
  question:  streaming versus file; expected patterns and
  regularities in the data; relative importance of CPU usage,
  memory usage, channel demands, and storage requirements; and
  other factors.
  在多种编程环境中都广泛使用了数据压缩。所有流行操作系统和编程语言
  都有众多的工具和库来处理各种各样的数据压缩。对于特定程序来说，
  如何才能正确选择压缩工具和库，依赖于正在讨论的数据和程序的特性：
  流还是文件；期待的模式和数据的规则性；彼此相关的价值，比如,CPU使用率，
  内存使用率，信道需求，以及存储要求；还有其他因素。

  Just what is data compression, anyway?  The short answer is
  that data compression removes -redundancy- from data; in
  information-theoretic terms, compression increases the
  -entropy- of the compressed text.  But those statements are
  essentially just true by definition.  Redundancy can come in a
  lot of different forms.  Repeated bit sequences ('11111111')
  are one type.  Repeated byte sequences are another
  ('XXXXXXXX').  But more often redundancies tend to come on a
  larger scale, either regularities of the data set taken as a
  whole, or sequences of varying lengths that are relatively
  common.  Basically, what data compression aims at is finding
  algorithmic transformations of data representations that will
  produce more compact representations given "typical" data sets.
  If this description seems a bit complex to unpack, read on to
  find some more practical illustrations.
  那啥是数据压缩呢？简短一点说就是，数据压缩从数据中移除-冗余信息-；
  根据信息理论，压缩增加了压缩过文本的-熵-。这些声明在定义上都对。
  冗余信息有很多种不同形式。重复的比特序列（‘11111111’）就是其中一种。
  重复的字节序列是（‘XXXXXXXX’）是另外一种。但是，冗余更通常倾向于
  出现在更大的范围，要么是整个数据集合的规则性，要么是一些变动长度
  的序列，它们相对而言比较常见。基本上，数据压缩的目标是，
  寻找数据表示在算法上的转换，以产生给定“典型”数据集合的更紧凑的表现。
  如果这样的说明看上去有点太复杂，无法理解，那就继续读下去，下面
  还有更多实用展示。

SECTION -- Lossless and Lossy Compression
节 -- 无损和有损压缩
-------------------------------------------------------------------

  There are actually two fundamentally different "styles" of data
  compression: lossless and lossy. This appendix is generally about
  lossless compression techniques, but the reader would be served
  to understand the distinction first. Lossless compression
  involves a transformation of the representation of a data set
  such that it is possible to reproduce -exactly- the original data
  set by performing a decompression transformation. Lossy
  compression is a representation that allows you to reproduce
  something "pretty much like" the original data set. As a plus for
  the lossy techniques, they can frequently produce far more
  compact data representations than lossless compression techniques
  can. Most often lossy compression techniques are used for images,
  sound files, and video. Lossy compression may be appropriate in
  these areas insofar as human observers do not perceive the
  literal bit-pattern of a digital image/sound, but rather more
  general "gestalt" features of the underlying image/sound.
  实际上有两种“风格”本质不同的数据压缩：无损和有损。本附录基本都是
  将无损压缩技术，但是首先会让读者明白两者的区别。无损压缩包括了一个
  对数据集合表现的转换，只要进行一个解压转换，
  后者可以可以重新生成和原来-完全一样-的数据集合。有损压缩是一种表现方式，
  它允许你再次生成和原来数据结构“相当相似”的结果。有损技术的一个优点就是，
  它们可以经常产生比无损压缩技术远更紧凑的表现形式。绝大部分时刻，
  有损压缩技术都是用于图像，声音文件，以及视频。有损压缩在这些领域范围内
  可以是恰当地，因为人类的观察力不能感觉到数字图片/声音的字面的比特模式，
  而只能感觉到下面的图像/声音的“完全心态”特性。

  From the point of view of "normal" data, lossy compression is
  not an option.  We do not want a program that does "about the
  same" thing as the one we wrote.  We do not want a database
  that contains "about the same" kind of information as what we
  put into it.  At least not for most purposes (and the writer
  knows of few practical uses of lossy compression outside of
  what are already approximate mimetic representations of the
  real world, likes images and sounds).
  从“普通”数据这一点看，有损压缩不是正确选择。我们不希望某程序
  的行为和编写时所希望的“大概一样”。我们也不希望某个数据库包含的
  信息与其输入时“大概一样”。至少不能用于绝大部分用途（而除了那些
  已经存在的，对于真实世界的大约模拟表现形式，例如图像和声音，作者
  几乎不知道有损压缩还有什么实用价值）。

SECTION -- A Data Set Example
节 -- 一个数据集合的例子
-------------------------------------------------------------------

  For purposes of this appendix, let us start with a specific
  hypothetical data representation.  Here is an
  easy-to-understand example.  In the town of Greenfield, MA, the
  telephone prefixes are '772-', '773-', and '774-'.  (For non-USA
  readers:  In the USA, local telephone numbers are seven digits
  and are conventionally represented in the form ###-####; prefixes
  are assigned in geographic blocks.)  Suppose also that the
  first prefix is the mostly widely assigned of the three.  The
  suffix portions might be any other digits, in fairly equal
  distribution.  The data set we are interested in is "the list
  of all the telephone numbers currently in active use."  One can
  imagine various reasons why this might be interesting for
  programmatic purposes, but we need not specify that herein.
  为了说明本附录的目的，让我们开始从一个假设的特定数据表述开始。
  这儿有个容易理解的例子。在美国麻塞诸塞州的Greenfield镇，电话号码
  前缀是‘772-’，‘773-’以及‘774-’。（致美国以外的读者：在美国，本地电话
  号码是七位数字，按照惯例，以###-####的形式出现；前缀是根据地理上的
  划分的块落分配的。）再假设第一个前缀是三个中分配最多的。后缀部分可以
  是任何数字，分布相当均匀。在此数据集合中，我们感兴趣的是“目前所有处于活动
  状态的电话号码的清单”。对于为什么这个对于编程用途是有趣的，你可以想出
  若干个理由来，但是我们不需要在此指出。
  ## lwl：概率 equal distribution。 这是什么分布的？

  Initially, the data set we are interested in comes in a
  particular data representation:  a multicolumn report (perhaps
  generated as output of some query or compilation process).  The
  first few lines of this report might look like:
  首先，我们感兴趣的数据集合生来就有特殊的数据表现形式：
  它是一个多栏报告（也许是一些查询或者汇编处理产生的输出）。
  此报告的开始几行看起来也许是这样的：

      #*----------- Telephone Number Report ----------------------#
      #*----------- 电话号码报告 ----------------------#
      =============================================================
      772-7628     772-8601     772-0113     773-3429     774-9833
      773-4319     774-3920     772-0893     772-9934     773-8923
      773-1134     772-4930     772-9390     774-9992     772-2314
      [...]

SECTION -- Whitespace Compression
节 -- 空白压缩
-------------------------------------------------------------------

  Whitespace compression can be characterized most generally as
  "removing what we are not interested in."  Even though this
  technique is technically a lossy-compression technique, it is
  still useful for many types of data representations we find in
  the real world.  For example, even though HTML is far more
  readable in a text editor if indentation and vertical spacing
  is added, none of this "whitespace" should make any difference
  to how the HTML document is rendered by a Web browser.  If you
  happen to know that an HTML document is destined only for a
  Web browser (or for a robot/spider) then it might be a good
  idea to take out all the whitespace to make it transmit faster
  and occupy less space in storage.  What we remove in whitespace
  compression never really had any functional purpose to start
  with.
  空白压缩可以被通俗的描述为“删除我们不感兴趣的东西”。尽管技术上
  来说这是个有损压缩技术，对于我们在实际世界中遇到的许多类型的数据
  表现形式而言，它还是有用的。例如，在文本编辑器中，如果增加了
  缩进和垂直空格，HTML会易读得多，尽管如此，这些“空白”没有一个
  会让浏览器对这个HTML文档的渲染有变化。如果你正好知道某个HTML文档
  是专门为浏览器（或者为搜索引擎的机器人/爬虫）而写，将其中所有的
  空白取出，可以取得更快的传输，更小的存储空间。在空白压缩中移除的
  内容从未真正拥有功能性用途。

  In the case of our example in this article, it is possible to
  remove quite a bit from the described report. The row of "="
  across the top adds nothing functional, nor do the "-" within
  numbers, nor the spaces between them. These are all useful for a
  person reading the original report, but do not matter once we
  think of it as data. What we remove is not precisely whitespace
  in traditional terms, but the intent is the same.
  在这篇文章的例子中，从所描述的报告中移除相当多的内容，是可能的。
  横越顶部的“=”行并未增加任何功能，数字内的“-”也是如此。这些东西
  对于阅读原始报告的人士是有用的，但是当我们将其作为数据考虑时，
  就无所谓了。我们移除的内容并不完全是传统所说的空白，但是意图是一样的。

  Whitespace compression is extremely "cheap" to perform.  It is
  just a matter of reading a stream of data and excluding a few
  specific values from the output stream.  In many cases, no
  "decompression" step is involved at all.  But even where we
  would wish to re-create something close to the original
  somewhere down the data stream, it should require little in
  terms of CPU or memory.  What we reproduce may or may not be
  exactly what we started with, depending on just what rules and
  constraints were involved in the original.  An HTML page typed
  by a human in a text editor will probably have spacing that is
  idiosyncratic.  Then again, automated tools often produce
  "reasonable" indentation and spacing of HTML.  In the case of
  the rigid report format in our example, there is no reason that
  the original representation could not be precisely produced by
  a "decompressing formatter" down the data stream.
  空白压缩实现起来极其“便宜”。它只是读入一个数据流，把一些特定值
  屏弃在输出流之外。在很多情况中，根本没有“解压”步骤。但是就算
  我们想创建和原来数据流相近的内容，也不需要多少CPU或内存。
  我们重新产生的内容不一定和开始的完全一样，这要依赖于原始文件
  中的规则和限制。人类在文本编辑器中输入的HTML页面，很可能
  拥有特殊的间隔。再次说明，自动化工具通常产生具有“合理的”缩进和间隔
  的HTML。我们这个例子拥有严格的报告格式，没有理由使得“解压格式器”
  从数据流中无法精确产生原始的表现形式。

SECTION -- Run-Length Encoding
节 -- 游程编码
-------------------------------------------------------------------

  Run-length encoding (RLE) is the simplest widely used
  lossless-compression technique. Like whitespace compression, it
  is "cheap"--especially to decode. The idea behind it is that many
  data representations consist largely of strings of repeated
  bytes. Our example report is one such data representation. It
  begins with a string of repeated "=", and has strings of spaces
  scattered through it. Rather than represent each character with
  its own byte, RLE will (sometimes or always) have an iteration
  count followed by the character to be repeated.
  游程编码（RLE， Run-Length Encoding）是广泛使用的无损压缩技术中最
  简单的。类似于空白压缩，它很“便宜”--特别是对于解压而言。它后面的
  思想是许多数据表现形式都由很多重复字节的字符串构成。我们的报告例子
  就是这样一种。它从一个重复“=”的字符串开始，并有空格字符串点缀左右。
  没有使用原来的字节表示每个字符，RLE（有时或者总是）有一个重复计数
  跟在要重复的字符后面。

  If repeated bytes are predominant within the expected data
  representation, it might be adequate and efficient to always have
  the algorithm specify one or more bytes of iteration count,
  followed by one character. However, if one-length character
  strings occur, these strings will require two (or more) bytes to
  encode them, that is, '00000001 01011000' might be the output
  bit stream required for just one ASCII "X" of the input stream.
  Then again, a hundred "X" in a row would be output as '01100100
  01011000', which is quite good.
  如果重复的字节在预期的数据表现中占据主要地位，让算法总是
  有一个或更多字节的重复计数，将是充分而言高效的。但是，如果出现长度为一
  的字符串，这些字符串将需要两个（或更多）直接来对其编码，也就是说，
  ‘00000001 01011000’可能是仅一个ASCII字符“X”输入后需要的输出比特流。
  但是再来，一行中有一百个“X”，它的输出可能是‘01100100 01011000’，
  这个就比较好。

  What is frequently done in RLE variants is to selectively use
  bytes to indicate iterator counts and otherwise just have bytes
  represent themselves. At least one byte-value has to be reserved
  to do this, but that can be escaped in the output, if needed. For
  example, in our example telephone-number report, we know that
  everything in the input stream is plain ASCII characters.
  Specifically, they all have bit one of their ASCII value as 0. We
  could use this first ASCII bit to indicate that an iterator count
  was being represented rather than representing a regular
  character. The next seven bits of the iterator byte could be used
  for the iterator count, and the next byte could represent the
  character to be repeated. So, for example, we could represent the
  string "YXXXXXXXX" as:
  在RLE变种中经常做的事情是，选择性地使用字节来显示重复计数，要不然
  那些字节就表示它们自己。至少需要保留一个字节值来完成这个，但是需要
  的话，可以在输出中转义它们。举例而言，在我们的电话号码报告中，
  我们知道输入流中的所有东西都是普通ASCII字符。特别的地方在于，
  它们的ASCII值都有一个比特值是0。我们可以使用这第一个ASCII比特来表明
  这是一个重复计数，而不是一个普通字符。下面七个比特可以表示重复计数的
  数量，再下一字节代表要重复的字符。
  所以，作为例子，我们可以把‘YXXXXXXXX’重新表示为：

      #*----- Run length encoding example -----#
      #*----- 游程编码例子 -----#
      "Y"      Iter(8)  "X"
      01001111 10001000 01011000

  This example does not show how to escape iterator byte-values,
  nor does it allow iteration of more than 127 occurrences of a
  character.  Variations on RLE deal with issues such as these,
  if needed.
  本例并未展示如何转义重复的字节值，它也不允许某一字符重复超过127次。
  如果需要的话，RLE的变种就处理此类问题。

SECTION -- Huffman Encoding
节 -- 霍夫曼编码
-------------------------------------------------------------------

  Huffman encoding looks at the symbol table of a whole data set.
  The compression is achieved by finding the "weights" of each
  symbol in the data set. Some symbols occur more frequently than
  others, so Huffman encoding suggests that the frequent symbols
  need not be encoded using as many bits as the less-frequent
  symbols. There are variations on Huffman-style encoding, but the
  original (and frequent) variation involves looking for the most
  common symbol and encoding it using just one bit, say 1. If you
  encounter a 0, you know you're on the way to encoding a longer
  variable length symbol.
  霍夫曼编码检查整个数据集合的符号表。完成压缩时通过寻找数据集合中
  每个符号的“比重”。一些符号比其他符号出现得更加频繁，所以霍夫曼编码
  建议对经常使用的符号编码长度要比不经常使用的短。霍夫曼编码有多个变种，
  但是原始的（也是常用的）变种包括了寻找最常见的符号，只用一比特，就是1，
  来对其进行编码。如果你遇到0，你就知道你正在对一个更长的可变长度符号
  进行编码。

  Let's imagine we apply Huffman encoding to our local phone-book
  example (assume we have already whitespace-compressed the
  report). We might get:
  让我们想象一下，我们把霍夫曼编码实施到我们的本地电话本例子上
  （假设我们已经对此报告进行空白压缩）。我们可能获得：

      #*----- Huffman encoding example -----#
      #*----- 霍夫曼编码例子 -----#
      Encoding   Symbol
      编码       例子
       1           7
       010         2
       011         3
       00000       4
       00001       5
       00010       6
       00011       8
       00100       9
       00101       0
       00111       1

  Our initial symbol set of digits could already be
  straightforwardly encoded (with no-compression) as 4-bit
  sequences (nibbles).  The Huffman encoding given will use up to
  5-bits for the worst-case symbols, which is obviously worse
  than the nibble encoding.  However, our best case will use only
  -1- bit, and we know that our best case is also the most
  frequent case, by having scanned the data set.  So we might
  encode a particular phone number like:
  我们初始的数字符号集合可以被直接编码（没有压缩）成4比特的序列（半字节）。
  对于最坏情况的符号，给出的霍夫曼编码会使用长至5比特的序列，这个很明显
  比半字节的情况还要坏。但是，我们最好的情况只要使用-1-比特，通过扫描数据集合，
  我们也知道我们最好的情况就是最常见的情况。所以，我们可以编码某个
  特定电话号码如下：

      #*----- Huffman translation example -----#
      #*----- 霍夫曼翻译例子 -----#
      772 7628 --> 1 1 010 1 00010 010 00011

  The nibble encoding would take 28-bits to represent a phone
  number; in this particular case, our encoding takes 19-bits. I
  introduced spaces into the example above for clarity; you can see
  that they are not necessary to unpack the encoding, since the
  encoding table will determine whether we have reached the end of
  an encoded symbol (but you have to keep track of your place in
  the bits).
  半字节编码需要28比特来表示一个电话号码；在此特定例子中，
  我们的编码只要19比特。为了清晰起见，在上面例子中引入了空格；
  你可以看出没有必要去解开此编码，因为编码表会确定我们是否已经
  到达某个编码后符号的末尾（但你仍然要明了自己在比特中的位置）。

  Huffman encoding is still fairly cheap to decode, cycle-wise. But
  it requires a table lookup, so it cannot be quite as cheap as
  RLE, however. The encoding side of Huffman is fairly expensive,
  though; the whole data set has to be scanned and a frequency
  table built up. In some cases a "shortcut" is appropriate with
  Huffman coding. Standard Huffman coding applies to a particular
  data set being encoded, with the set-specific symbol table
  prepended to the output data stream. However, if the whole type
  of data encoded--not just the single data set--has the same
  regularities, we can opt for a global Huffman table. If we have
  such a global Huffman table, we can hard-code the lookups into
  our executables, which makes both compression and decompression
  quite a bit cheaper (except for the initial global sampling and
  hard-coding). For example, if we know our data set would be
  English-language prose, letter-frequency tables are well known
  and quite consistent across data sets.
  解压霍夫曼编码还是相当便宜的。但它需要进行表查询，所以它不可能
  和RLE一样便宜。而霍夫曼的编码是相当昂贵；需要扫描整个数据集合，
  建立一个使用频率表。有些时候，“捷径”和霍夫曼编码是合适的。
  将标准霍夫曼编码应用到要编码的数据集合上，在输出数据流之前，
  预先准备好与集合相关的符号表。但是，如果整个编码的数据类型
  --不只是单个数据集合--拥有同样的规则性，我们可以选择一个全局
  霍夫曼表。如果我们有这样一个全局霍夫曼表，我们可以将查询硬编码
  到我们的执行代码中，这样使得压缩和解压缩便宜一点点（除了开始的
  全局采样和硬编码）。例如，如果我们知道我们的数据集合是英文散文，
  字母频率表是人所皆知的，在各个数据集合中都比较一致。

SECTION -- Lempel-Ziv Compression
节 -- Lempel-Ziv压缩
-------------------------------------------------------------------

  Probably the most significant lossless-compression technique is
  Lempel-Ziv. What is explained here is "LZ78," but LZ77 and other
  variants work in a similar fashion. The idea in LZ78 is to encode
  a streaming byte sequence using a dynamic table. At the start of
  compressing a bit stream, the LZ table is filled with the actual
  symbol set, along with some blank slots. Various size tables are
  used, but for our (whitespace-compressed) telephone number
  example above, let's suppose that we use a 32-entry table (this
  should be OK for our example, although much too small for most
  other types of data). First thing, we fill the first ten slots
  with our alphabet (digits). As new bytes come in, we first output
  an existing entry that grabs the longest sequence possible, then
  fill the next available slot with the N+1 length sequence. In the
  worst case, we are using 5-bits instead of 4-bits for a single
  symbol, but we'll wind up getting to use 5-bits for multiple
  symbols in a lot of cases. For example, the machine might do this
  (a table slot is noted with square brackets):
  可能最重要的无损压缩技术就是Lempel-Ziv。这儿将解释“LZ78”，但是
  LZ77和其他变种以一种相似的方式工作。LZ78中的思想是使用一个动态
  表来对流字节序列进行编码。在开始压缩比特流时，使用实际的符号集
  以及一些空白缝隙来填充LZ表。使用了不同尺寸的表，但是对于我们上面的
  （空白压缩过的）电话号码例子，让我们假设使用一个32个条目的表格
  （这个对于我们的例子应该没有问题，但是对于其他大部分数据类型，
  就显得太小了）。首先，我们用我们的字母表（数字）填充前面十个位置。
  当新的字节进来的时候，我们首先输出一个已存在条目，它里面是最长
  的序列，然后用这个N+1长度的序列填充下一个可用位置。在最坏情况，
  我们使用5比特而非4比特来表示单个符号，但我们结束时，
  很多情况将以5比特来表示多个符号。例如，该机器可能会这样做
  （表格位置用方括号表示）：

      #*----------- LZ77 Algorithm --------------#
      #*----------- LZ77算法 --------------#
      7 --> Lookup: 7 found       --> nothing to add    --> keep looking
      7 --> Lookup: 77 not found  --> add '77' to [11]  --> output [7]=00111
      2 --> Lookup: 72 not found  --> add '72' to [12]  --> output [7]=00111
      7 --> Lookup: 27 not found  --> add '27' to [13]  --> output [2]=00010
      6 --> Lookup: 76 not found  --> add '76' to [14]  --> output [7]=00111
      2 --> Lookup: 62 not found  --> add '62' to [15]  --> output [6]=00110
      8 --> Lookup: 28 not found  --> add '28' to [16]  --> output [2]=00010

  So far, we've got nothing out of it, but let's continue with
  the next phone number:
  到目前为止，我们还没有任何收获，但是让我们继续下面的电话号码：

      #*----------- LZ77 Algorithm (cont) -------#
      #*----------- LZ77算法 (续) -------#
      7 --> Lookup: 87 not found  --> add '87' to [17]  --> output [8]=00100
      7 --> Lookup: 77 found      --> nothing to add    --> keep looking
      2 --> Lookup: 772 not found --> add '772' to [18] --> output [11]=01011
      8 --> Lookup: 28 found      --> nothing to add    --> keep looking
      6 --> Lookup: 286 not found --> add '286' to [19] --> output [16]=10000
      ...

  The steps should suffice to see the pattern. We have not achieved
  any net compression yet, but notice that we've already managed to
  use slot 11 and slot 16, thereby getting two symbols with one
  output in each case. We've also accumulated the very useful byte
  sequence '772' in slot 18, which would prove useful later in the
  stream.
  这些步骤足够让人看出模式。我们还没有取得任何纯压缩，但要注意到
  我们已经成功的使用了位置11和16，因此两种情况都是一个输出代表
  两个符号。我们还将非常有用的比特序列‘772’累计于位置18，后面的流
  将证明这个是很有用的。

  What LZ78 does is fill up one symbol table with (hopefully)
  helpful entries, then write it, clear it, and start a new one. In
  this regard, 32 entries is still probably too small a symbol
  table, since that will get cleared before a lot of reuse of '772'
  and the like is achieved. But the small symbol table is easy to
  illustrate.
  LZ78所做的就是用（很有希望的）有用的条目填充一个符号表，然后将其写入，
  清理，最后开始一个新的。在这一点上，32个条目对于符号表来说大概有点小，
  因为在大量复用‘772’之前，这个表会被清空，取得了相似性。## 不懂
  但是小的符号表容易用来展示。

  In typical data sets, Lempel-Ziv variants achieve much better
  compression rates than Huffman or RLE. On the other hand,
  Lempel-Ziv variants are very pricey cycle-wise and can use large
  tables in memory. Most real-life compression tools and libraries
  use a combination of Lempel-Ziv and Huffman techniques.
  在典型的数据集合中，Lempel-Ziv变种 比霍夫曼编码或RLE获得更好的
  压缩率。另一方面，Lempel-Ziv变种进行昂贵的cycle-wise，内存中会
  被巨大的表格占据。大部分真实生活中的压缩工具和库都是混合使用
  Lempel-Ziv和霍夫曼技术。
  

SECTION -- Solving the Right Problem
节 -- 解决正确的问题
-------------------------------------------------------------------

  Just as choosing the right algorithm can often create
  orders-of-magnitude improvements over even heavily optimized
  wrong algorithms, choosing the right data representation is often
  even more important than compression methods (which are always a
  sort of post hoc optimization of desired features). The simple
  data set example used in this appendix is a perfect case where
  reconceptualizing the problem would actually be a much better
  approach than using -any- of the compression techniques
  illustrated.
  正如选择正确的算法经常比高度优化的错误算法有指数级别的改进，
  选择正确的数据表示方式通常比压缩方法更为重要（后者通常是某种
  前者之后的对想要特性的优化）。本附录中使用的简单数据集合例子，
  就是一个完美的案例，其中问题的一致性和-任何-展示的压缩技术相比，
  是个更好的方案。

  Think again about what our data represents.  It is not a very
  general collection of data, and the rigid a priori constraints
  allow us to reformulate our whole problem.  What we have is a
  maximum of 30,000 telephone numbers (7720000 through 7749999),
  some of which are active, and others of which are not.  We do
  not have a "duty," as it were, to produce a full representation
  of each telephone number that is active, but simply to indicate
  the binary fact that it -is- active.  Thinking of the problem
  this way, we can simply allocate 30,000 bits of memory and
  storage, and have each bit say "yes" or "no" to the presence of
  one telephone number.  The ordering of the bits in the
  bit-array can be simple ascending order from the lowest to the
  highest telephone number in the range.
  再次想想我们的数据表现形式。它不是一个非常通用的数据集合，
  它的严格的优先级限制允许我们重新阐述我们的整个问题。
  我们所有的是最多30,000个电话号码（7720000到7749999），
  它们中间的一部分是激活的，其他的没有。我们不像它那样有“责任”来
  为每个激活的电话号码产生一个完整的表现，只需要简单地显示
  它-是-激活的，这个二元事实。以这种方法思考问题，我们只需要分配
  30,000比特的内存和存储空间，其中每个比特说“是”或者“否”，以表示
  一个电话号码。此比特数组中的比特顺序可以是电话号码范围从最低到最高
  进行升序。
  # priori？

  This bit-array solution is the best in almost every respect.
  It allocates exactly 3750 bytes to represent the data set; the
  various compression techniques will use a varying amount of
  storage depending both on the number of telephone numbers in
  the set and the efficiency of the compression.  But if 10,000
  of the 30,000 possible telephone numbers are active, and even a
  very efficient compression technique requires several bytes per
  telephone number, then the bit-array is an order-of-magnitude
  better.  In terms of CPU demands, the bit-array is not only
  better than any of the discussed compression methods, it is
  also quite likely to be better than the naive noncompression
  method of listing all the numbers as strings.  Stepping through
  a bit-array and incrementing a "current-telephone-number"
  counter can be done quite efficiently and mostly within the
  on-chip cache of a modern CPU.
  这个比特数组解决方案几乎在所有方面都是最佳的。
  它分配正好3750字节来表示这个数据集合；而那些不同的压缩技术
  将使用变化数目的存储空间，它们同时依赖于集合中电话号码的个数，
  以及压缩的效率。但是如果30,000个中有10,000电话号码是激活的，
  就算非常有效率的压缩技术都会要求一个电话号码几个字节，而比特
  数组就是指数级别的优秀。在CPU需求方面，比特数组不光比任何讨论的
  压缩方法强，它以字符串列举所有号码时，还比原始未压缩方法更棒。
  穿过一个比特数组，对一个“当前电话号码”进行增量，可以完成得非常
  有效率，基本上在现代CPU的片上缓存内解决。

  The lesson to be learned from this very simple example is
  certainly not that every problem has some magic shortcut (like
  this one does). A lot of problems genuinely require significant
  memory, bandwidth, storage, and CPU resources, and in many of
  those cases compression techniques can help ease--or shift--those
  burdens. But a more moderate lesson could be suggested: Before
  compression techniques are employed, it is a good idea to make
  sure that one's starting conceptualization of the data
  representation is a good one.
  从这个非常简单的例子可以学到，肯定不是每个问题都有一些魔术般
  的快捷方式（就如这个一样）。许多问题真的需要很多内存，带宽，
  存储空间以及CPU资源，在很多情况中，压缩技术可以帮助减轻--或者转移--
  这些重负。但是一个更缓和的教训是：在使用压缩技术之前，有个好点子是
  确保你对数据表现形式的概念化从开始就是很好的。


SECTION -- A Custom Text Compressor
节 -- 一个自定义文本压缩器
-------------------------------------------------------------------

  Most styles of compression require a decompression pass before
  one is able to do something useful with a source document. Many
  (de)compressors can operate as a stream, producing only the
  needed bytes of a compressed or decompressed stream in sequence.
  In some cases, formats even insert recovery or bookkeeping bytes
  that allow streams to begin within documents (rather than from
  the very beginning). Programmatic wrappers can make compressed
  documents or strings look like plaintext ones at the appropriate
  API layer. Nonetheless, even streaming decompressors require a
  computational overhead to get at the plaintext content of a
  compressed document.
  大部分压缩风格在它能对源文档做一些有用的事情之前，都需要一个
  解压通行证。许多（解）压缩软件可以像流一样操作，只顺次产生
  压缩或解压缩流中需要的字节。在某些情况中，格式会要求插入
  恢复或薄记字节，它们允许流从文档中开始（，而不是从最开头）。
  纲领性的包装能让压缩文档或字符串在合适的API层看上去像普通文本。
  尽管如此，即使流解压缩也需要计算能力上的开支，以取得压缩过文档
  的普通文本内容。

  An excellent example of a streaming (de)compressor with an API
  wrapper is `gzip.GzipFile()`. Although not entirely transparent,
  you can compress and decompress documents without any explicit
  call to a (de)compression function using this wrapper.
  `gzip.GzipFile()` provides a file-like interface, but it is also
  easy to operate on a purely in-memory file using the support of
  `cStringIO.StringIO()`. For example:
  一个拥有API包装的流（解）压缩器的优良例子是`gzip.GzipFile()`。
  尽管不是完全透明，你不需要用此包装显式调用（解）压缩函数，
  就可以压缩和解压文档。`gzip.GzipFile()`提供了一个类似于文件的界面，
  但它也能够利用`cStringIO.StringIO()`的支持来操作一个完全位于内存内
  的文件。请看例子：

      >>> from gzip import GzipFile
      >>> from cStringIO import StringIO
      >>> sio = StringIO()
      >>> writer = GzipFile(None, 'wb', 9, sio)
      >>> writer.write('Mary had a little lamb\n')
      >>> writer.write('its fleece as white as snow\n')
      >>> writer.close()
      >>> sio.getvalue()[:20]
      '\x1f\x8b\x08\x00k\xc1\x9c<\x02\xff'
      >>> reader = GzipFile(None, 'rb', 9, StringIO(sio.getvalue()))
      >>> reader.read()[:20]
      'Mary had a little la'
      >>> reader.seek(30)
      >>> reader.read()
      'ece as white as snow\n'

  One thing this example shows is that the underlying compressed
  string is more or less gibberish. Although the file-like API
  hides the details from an application programmer, the
  decompression process is also stateful in its dependence on a
  symbol table built from the byte sequence in the compressed text.
  You cannot expect to make sense of a few bytes in the middle of
  the compressed text without a knowledge of the prior context.
  这个例子展示的一件事是压缩字符串或多或少是无用数据。尽管
  类似于文件的API将细节对于程序编程员隐藏起来，解压过程依赖于
  符号表也是分状态的，后者是从压缩后文本的字节序列中建立的。
  你不能指望不知道前面的语境，就搞清楚压缩后文本中间的一些字节的意义。
  

  A different approach to compression can have significant
  advantages in operating on natural-language textual sources. A
  group of researchers in Brazil and Chile have examined techniques
  for "word-based Huffman compression." The general strategy of
  these researchers is to treat whole words as the symbol set for a
  Huffman table, rather than merely naive byte values. In natural
  languages, a limited number of (various length, multibyte) words
  occur with a high frequency, and savings result if such words are
  represented with shorter byte sequences. In general, such reduced
  representation is common to all compression techniques, but
  word-based Huffman takes the additional step of retaining byte
  boundaries (and uses fixed symbol mapping, as with other Huffman
  variants).
  一个不同的压缩方法能在操作自然语言的文本源时拥有重要的优点。
  巴西和智利的一组学者已经检查了“基于单词的霍夫曼压缩”技术。
  这些学者的大体策略是将整个单词作为霍夫曼表中的符号集对待，而
  不仅仅是更短的字节序列。通常来说，这样的缩小的表示形式对于所有压缩技术
  来说，是常见的，但是基于单词的霍夫曼还有额外的一步，就是保留
  字节边界（并使用固定的符号匹配，就如其他霍夫曼变种一般）。

  A special quality of word-based Huffman compressed text is that
  it need not undergo decompression to be searched. This quality
  makes it convenient to store textual documents in compressed
  form, without incurring the requirement to decompress them before
  they are useful. Instead, if one is searching for words directly
  contained in the symbol table, one can merely precompress the
  search terms, then use standard searching algorithms. Such a
  search can be either against an in-memory string or against a
  file-like source; in general a search against a precompressed
  target will be -faster- than one against an uncompressed text. In
  code, one would use snippets similar to:
  使用基于单词的霍夫曼编码压缩过的文本的一个特殊品质就是，
  搜索它不需要经过解压过程。这个品质可以很方便地将原文档以
  压缩形式存储，而不需要在使用它们之前解压。更替的做法是，如果有人
  要搜索符号表中直接包含的单词，只要预先压缩搜索的术语，
  然后使用标准的搜索算法即可。这样的算法可以用于内存中的字符串，
  也可用于文件类似的源；通常搜索一个预压缩的目标会比未压缩的
  -更快-。你可以使用和下面类似的的代码片段：

      #*------------- Word-based Huffman compression ----------#
      #*------------- 基于单词的霍夫曼编码 ----------#
      small_text = word_Huffman_compress(big_text)
      search_term = "Foobar"
      coded_term = word_Huffman_compress(search_term)
      offset = small_text.find(coded_term)
      coded_context = small_text[offset-10:offset+10+len(search_term)]
      plain_context = word_Huffman_expand(coded_context)

  A sophisticated implementation of word-based Huffman compression
  can obtain better compression sizes than does `zlib`. For
  simplicity, the module below sacrifices optimal compression to
  the goal of clarity and brevity of code. A fleshed-out
  implementation could add a number of features.
  一个基于单词的霍夫曼压缩的精致实现能获得比`zlib`更佳的压缩尺寸。
  为了简单起见，下面的模块牺牲了最佳压缩，以便代码变得清晰简短。

  The presented module [word_huffman] uses a fixed number of bytes
  to encode each word in the symbol table. This number of bytes can
  be selected to be 1, 2, or 3 (thereby limiting the table to a
  generous 2 million entries). The module also separates the
  generation of a symbol table from the actual
  compression/decompression. The module can be used in a context
  where various documents get encoded using the same symbol
  table--the table presumably generated based on a set of canonical
  documents. In this situation, the computational requirement of
  symbol table generation can happen just once, and the symbol
  table itself need not be transmitted along with each compressed
  document. Of course, nothing prevents you from treating the
  document being processed currently as said canonical statistical
  word source (thereby somewhat improving compression).
  介绍的模块[word_huffman]使用一个固定数目的字节来对符号表中的
  每个单词编码。直接的数目可以是1，2，3（这个将表限制为大至两百万条目）。
  此模块还可用于某种情境，即若干个文档使用同一个符号表来进行编码--假设
  此表基于一套规范文档产生。在此情况中，对于产生符号表而需要的计算
  只要一次，而符号表本身不需要跟随每个压缩的文档传递。当然，没有
  什么阻止你把当前处理的文档作为规范统计的单词源（因此某种程度上改进了
  压缩）。

  In the algorithm utilized by [word_huffman], only high-bit
  bytes are utilized in the symbol table.  The lower 128 ASCII
  characters represent themselves as literals.  Any ASCII
  character sequence that is not in the symbol table is
  represented as itself--including any short words that would not
  benefit from encoding.  Any high-bit characters that occur in the
  original source text are escaped by being preceded by
  an 0xFF byte.  As a result, high-bit characters are encoded
  using two bytes; this technique is clearly only useful for
  encoding (mostly) textual files, not binary files.  Moreover,
  only character values 0x80-0xFE are used by the symbol table
  (0xFF -always- signals a literal high-bit character in the
  encoding).
  在[word_huffman]使用的算法中，只有高比特的字节才在符号表中使用。
  低一些的128个ASCII就如字面上那样，代表自己。任何不在符号表中的
  ASCII字符序列代表其自己--包括任何短单词，它们不会从编程中获利。
  任何高比特的字符，如果它们出现在初始源文本中，就通过在前面放置
  一个0xFF字节来转义。作为结果，高比特字符使用两个字节编码；这项
  技术显然只对（绝大部分）文本文件有用，而非二进制文件。而且，
  只有字符值在0x80-0xFE之间才在符号表中使用（0xFF-总是-表明
  编码中的一个字面高比特字符）。

  The [word_huffman] algorithm is not entirely stateless in the
  sense that not every subsequence in a compressed text can be
  expanded without additional context. But very little context is
  required. Any low-bit character always literally represents
  itself. A high-bit character, however, might be either an escaped
  literal, a first byte of a symbol table entry, or a non-first
  byte of a symbol table entry. In the worst case, where a 3-byte
  symbol table is used, it is necessary to look back two bytes from
  an arbitrary position in the text to determine the full context.
  Normally, only one byte lookback is necessary. In any case, words
  in the symbol table are separated from each other in the
  uncompressed text by nonalpha low-bit characters (usually
  whitespace), so parsing compressed entries is straightforward.
  并非压缩过的文本中所有子序列都能扩展，而不需要额外的上下文。
  在这种意义上，[word_huffman]算法并非完全无状态的。
  但是很少有上下文是必要的。所有低比特字符总是表示它们自己。
  但是一个高比特字符，要么是个转义的字面字符，某个符号表条目的
  第一个字节，要么是某个符号表条目的非第一个字节。在最坏的情况中，
  使用了一个三字节的符号表，为了确定完整的上下文，有必要从文本
  中的任意位置回顾两个字节。通常来说，只需要回顾一个字节即可。
  在所有情况中，符号表中的单词在解压后的文本中被互相隔开，
  使用的是非字母低比特字符（通常是空白），所以解析解压条目
  是显而易见的。

      #---------- word_huffman.py ----------#
      wordchars = '-_ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz'

      def normalize_text(txt):
          "Convert non-word characters to spaces"
          trans = [' '] * 256
          for c in wordchars: trans[ord(c)] = c
          return txt.translate(''.join(trans))

      def build_histogram(txt, hist={}):
          "Incrementally build a histogram table from text source(s)"
          for word in txt.split():
              hist[word] = hist.get(word, 0)+1
          return hist

      def optimal_Nbyte(hist, entrylen=2):
          "Build optimal word list for nominal symbol table byte-length"
          slots = 127**entrylen
          words = []
          for word, count in hist.items():
              gain = count * (len(word)-entrylen)
              if gain > 0: words.append((gain, word))
          words.sort()
          words.reverse()
          return [w[1] for w in words[:slots]]

      def tables_from_words(words):
          "Create symbol tables for compression and expansion"
          # Determine ACTUAL best symbol table byte length
          if len(words) < 128: entrylen = 1
          elif len(words) <= 16129: entrylen = 2
          else: entrylen = 3 # assume < ~2M distinct words
          comp_table = {}
          # Escape hibit characters
          for hibit_char in map(chr, range(128,256)):
              comp_table[hibit_char] = chr(255)+hibit_char
          # Literal low-bit characters
          for lowbit_char in map(chr, range(128)):
              comp_table[lowbit_char] = lowbit_char
          # Add word entries
          for word, index in zip(words, range(len(words))):
              comp_table[word] = symbol(index, entrylen)
          # Reverse dictionary for expansion table
          exp_table = {}
          for key, val in comp_table.items():
              exp_table[val] = key
          return (comp_table, exp_table, entrylen)

      def symbol(index, entrylen):
          "Determine actual symbol from word sequence and symbol length"
          if entrylen == 1:
              return chr(128+index)
          if entrylen == 2:
              byte1, byte2 = divmod(index, 128)
              return chr(128+byte1)+chr(128+byte2)
          if entrylen == 3:
              byte1, rem = divmod(index, 16129)
              byte2, byte3 = divmod(rem, 128)
              return chr(128+byte1)+chr(128+byte2)+chr(128+byte3)
          raise ValueError, "symbol byte len must be 1 <= S <=3: "+`entrylen`

      def word_Huffman_compress(text, comp_table):
          "Compress text based on word-to-symbol table"
          comp_text = []
          maybe_entry = []
          for c in text+chr(0):   # force flush of final word
              if c in wordchars:
                  maybe_entry.append(c)
              else:
                  word = ''.join(maybe_entry)
                  comp_text.append(comp_table.get(word, word))
                  maybe_entry = []
                  comp_text.append(comp_table[c])
          return ''.join(comp_text[:-1])

      def word_Huffman_expand(text, exp_table, entrylen):
          "Expand text based on symbol-to-word table"
          exp_text = []
          offset = 0
          end = len(text)
          while offset < end:
              c = text[offset]
              if ord(c) == 255:   # escaped highbit character
                  exp_text.append(text[offset+1])
                  offset += 2
              elif ord(c) >= 128: # symbol table entry
                  symbol = text[offset:offset+entrylen]
                  exp_text.append(exp_table[symbol])
                  offset += entrylen
              else:
                  exp_text.append(c)
                  offset += 1
          return ''.join(exp_text)

      def Huffman_find(pat, comp_text, comp_table):
          "Find a (plaintext) substring in compressed text"
          comp_pat = word_Huffman_compress(pat, comp_table)
          return comp_text.find(comp_pat)

      if __name__=='__main__':
          import sys, glob
          big_text = []
          for fpat in sys.argv[1:]:
              for fname in glob.glob(fpat):
                  big_text.append(open(fname).read())
          big_text = ''.join(big_text)
          hist = build_histogram(normalize_text(big_text))
          for entrylen in (1, 2, 3):
              comp_words = optimal_Nbyte(hist, entrylen)
              comp_table, exp_table, entrylen_ = tables_from_words(comp_words)
              comp_text = word_Huffman_compress(big_text, comp_table)
              exp_text = word_Huffman_expand(comp_text, exp_table, entrylen_)
              print "Nominal/actual symbol length (entries): %i/%i (%i)" % \
                    (entrylen, entrylen_, len(comp_words))
              print "Compression ratio: %i%%" % \
                    ((100*len(comp_text))/len(big_text))
              if big_text == exp_text:
                  print "*** Compression/expansion cycle successful!\n"
              else:
                  print "*** Failure in compression/expansion cycle!\n"
              # Just for fun, here's a search against compressed text
              pos = Huffman_find('Foobar', comp_text, comp_table)

  The [word_huffman] module, while simple and fairly short, is
  still likely to be useful--and it lays the basis for a
  fleshed-out variant. The compression obtained by the algorithm
  above is a comparatively modest 50-60 percent of the size of the
  original text (in informal tests). But given that locality of
  decompression of subsegments is both possible and cheap, there is
  nearly no disadvantage to this transformation for stored
  documents. Word searches become quicker basically in direct
  proportion to the length reduction.
  [word_huffman]模块，比较简单也比较短，也应该是有用的--它为
  充实的变量打下基础。上述算法获得的压缩是比较适度的，（在不正式测试中）
  是原始文本尺寸的百分之50-60。但是考虑到子节的定位和解压都是可能的，
  而且便宜，对于存储的文档来说，这种转换几乎没有优势可言。单词搜索
  变快直接和长度的减少相关。

  One likely improvement would be to add run-length compression of
  whitespace (or generally of nonalpha characters); doing so would
  lose none of the direct searchability that this algorithm is
  designed around, and in typical electronic natural-language texts
  would result in significant additional compression. Moreover, a
  pleasant side effect of the [word_huffman] transformation is that
  transformed documents become -more- compressible under
  Lempel-Ziv-based techniques (i.e., cumulatively). In other words,
  there is benefit in precompressing documents with [word-huffman]
  if you intend to later compress them with 'gzip', 'zip', or
  similar tools.
  一个可能的改进是，增加针对空白（或者通常是非字母字符）的游程压缩；
  这样做会丧失直接搜索的可能性，后者是此算法设计的目的之一，
  而对于典型的电子版的自然语言文本，可以得到有效的额外压缩。
  而且，[word_huffman]转换的一个让人高兴的负效应是，转换后的文档
  在基于Lempel-Ziv的技术下（也就是，累积地）变得-更-具可压缩性。
  换句话说，如果你想要在后面使用'gzip', 'zip',或其他类似工具来压缩，
  使用[word_huffman]进行预压缩可以获得好处。

  More aggressive improvements might be obtained by allowing
  variable byte-length symbol table entries and/or by claiming some
  additional low-bit control codes for the symbol table (and
  escaping literals in the original text). You can experiment with
  such variations, and your results might vary somewhat depending
  upon the details of application-specific canonical texts.
  要获取更多主动的改进，可以通过允许可变字节长度的符号表条目，
  和/或为符号表宣称一些额外的低比特控制代码（并转移原始文本中的字面字符）。
  你可以对这些变种进行试验，你的结果可能根据与程序相关的规范文本
  而有所不同。

  Search capabilities might also be generalized--but this would
  require considerably greater effort. In the referenced research
  article below, the authors show how to generalize to direct
  regular-expression searching against word-based Huffman encoded
  texts. The [word_huffman] implementation allows certain
  straightforward transformations of regular expressions (where
  literal words occur within them) for searching against compressed
  documents, but a number of caveats and restrictions apply.
  Overcoming most such limitations would involve digging into
  Python's underlying regular expression engine, but it is possible
  in principle.
  也可以产生搜索能力--但是这个会需要相当可观的更多的努力。
  在下面参考文章里面，作者们展示了如何产生一个直接正则表达式搜索，
  它是针对基于单词的霍夫曼编码文本的。[word_huffman]的实现允许
  特定的直截了当的正则表达式（其中出现了字面单词）的转换，以搜索
  压缩过的文档，但是也有许多的警告和限制。要克服这样的限制，需要
  深入到Python的底层正则表达式引擎，但是在理论上是可能的。


SECTION -- References
节 -- 参考文献
-------------------------------------------------------------------

  A good place to turn for additional theoretical and practical
  information on compression is at the '<comp.compression>' FAQ:
  一个好地方，可用于寻找额外的关于压缩的理论和实践信息，它就是
  '<comp.compression>' FAQ:

    <http://www.cis.ohio-state.edu/hypertext/faq/usenet/compression-faq/>.

  A research article on word-based Huffman encoding inspired my
  simple example of word-based compression.  The article "Fast
  and Flexible Word Searching on Compressed Text," by Edleno
  Silva de Moura, Gonzalo Navarro, Nivio Ziviani, and Ricardo
  Baeza-Yates, can be found at:
  一个基于单词的霍夫曼编码的研究文章，从我的那个简单的基于单词
  的压缩例子处获得灵感。这篇文章叫“Fast and Flexible Word 
  Searching on Compressed Text”，作者是y Edleno Silva de Moura, 
  Gonzalo Navarro, Nivio Ziviani, and Ricardo Baeza-Yates。可以
  在此处找到：

    <http://citeseer.nj.nec.com/silvademoura00fast.html>.

